{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import gc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"../data/Train.csv\")\n",
    "#test=pd.read_csv(\"./data/Test.csv\")\n",
    "meta=pd.read_csv(\"../data/airqo_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>location</th>\n",
       "      <th>temp</th>\n",
       "      <th>precip</th>\n",
       "      <th>rel_humidity</th>\n",
       "      <th>wind_dir</th>\n",
       "      <th>wind_spd</th>\n",
       "      <th>atmos_press</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_train_0</td>\n",
       "      <td>C</td>\n",
       "      <td>nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,na...</td>\n",
       "      <td>nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,na...</td>\n",
       "      <td>nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,na...</td>\n",
       "      <td>nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,na...</td>\n",
       "      <td>nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,na...</td>\n",
       "      <td>nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,na...</td>\n",
       "      <td>45.126304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_train_1</td>\n",
       "      <td>D</td>\n",
       "      <td>22.53333333,21.71666667,20.83333333,20.9833333...</td>\n",
       "      <td>0.102,0.0,0.0,0.0,0.0,0.0,0.0,0.034,0.017,0.01...</td>\n",
       "      <td>0.744583333,0.808083333,0.911166667,0.91633333...</td>\n",
       "      <td>281.6643101,89.15629262,81.96853891,291.018632...</td>\n",
       "      <td>2.3775,1.126666667,0.700833333,0.3416666670000...</td>\n",
       "      <td>90.32,90.3775,90.44083333,90.4725,90.45416667,...</td>\n",
       "      <td>79.131702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_train_10</td>\n",
       "      <td>A</td>\n",
       "      <td>28.975,27.95,29.6,26.425,22.09166667,21.775,22...</td>\n",
       "      <td>0.0,0.0,0.0,0.102,0.136,0.0,0.0,2.16,1.276,0.0...</td>\n",
       "      <td>0.573333333,0.597166667,0.5668333329999999,0.6...</td>\n",
       "      <td>nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,na...</td>\n",
       "      <td>nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,na...</td>\n",
       "      <td>88.55166667,88.46416667,88.31916667,88.24,88.2...</td>\n",
       "      <td>32.661304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_train_100</td>\n",
       "      <td>A</td>\n",
       "      <td>22.96666667,24.26666667,25.275,25.625,25.86666...</td>\n",
       "      <td>0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,7.77,3.012,1.0...</td>\n",
       "      <td>0.8430833329999999,0.79025,0.7375,0.728,0.7049...</td>\n",
       "      <td>300.0850574,293.6769595,294.5174647,301.921416...</td>\n",
       "      <td>1.446666667,1.1925,1.324166667,1.5441666669999...</td>\n",
       "      <td>88.615,88.53083333,88.4,88.27166667,88.2075,88...</td>\n",
       "      <td>53.850238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_train_1000</td>\n",
       "      <td>A</td>\n",
       "      <td>21.875,21.575,21.525,21.43333333,20.50833333,1...</td>\n",
       "      <td>0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0....</td>\n",
       "      <td>0.8564166670000001,0.874916667,0.879833333,0.8...</td>\n",
       "      <td>21.83997432,17.05405341,89.26406044,123.585424...</td>\n",
       "      <td>0.1975,0.244166667,0.411666667,0.56,0.5775,0.4...</td>\n",
       "      <td>88.55666667,88.64083333,88.65833333,88.6475,88...</td>\n",
       "      <td>177.418750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID location                                               temp  \\\n",
       "0     ID_train_0        C  nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,na...   \n",
       "1     ID_train_1        D  22.53333333,21.71666667,20.83333333,20.9833333...   \n",
       "2    ID_train_10        A  28.975,27.95,29.6,26.425,22.09166667,21.775,22...   \n",
       "3   ID_train_100        A  22.96666667,24.26666667,25.275,25.625,25.86666...   \n",
       "4  ID_train_1000        A  21.875,21.575,21.525,21.43333333,20.50833333,1...   \n",
       "\n",
       "                                              precip  \\\n",
       "0  nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,na...   \n",
       "1  0.102,0.0,0.0,0.0,0.0,0.0,0.0,0.034,0.017,0.01...   \n",
       "2  0.0,0.0,0.0,0.102,0.136,0.0,0.0,2.16,1.276,0.0...   \n",
       "3  0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,7.77,3.012,1.0...   \n",
       "4  0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0....   \n",
       "\n",
       "                                        rel_humidity  \\\n",
       "0  nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,na...   \n",
       "1  0.744583333,0.808083333,0.911166667,0.91633333...   \n",
       "2  0.573333333,0.597166667,0.5668333329999999,0.6...   \n",
       "3  0.8430833329999999,0.79025,0.7375,0.728,0.7049...   \n",
       "4  0.8564166670000001,0.874916667,0.879833333,0.8...   \n",
       "\n",
       "                                            wind_dir  \\\n",
       "0  nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,na...   \n",
       "1  281.6643101,89.15629262,81.96853891,291.018632...   \n",
       "2  nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,na...   \n",
       "3  300.0850574,293.6769595,294.5174647,301.921416...   \n",
       "4  21.83997432,17.05405341,89.26406044,123.585424...   \n",
       "\n",
       "                                            wind_spd  \\\n",
       "0  nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,na...   \n",
       "1  2.3775,1.126666667,0.700833333,0.3416666670000...   \n",
       "2  nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,na...   \n",
       "3  1.446666667,1.1925,1.324166667,1.5441666669999...   \n",
       "4  0.1975,0.244166667,0.411666667,0.56,0.5775,0.4...   \n",
       "\n",
       "                                         atmos_press      target  \n",
       "0  nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,nan,na...   45.126304  \n",
       "1  90.32,90.3775,90.44083333,90.4725,90.45416667,...   79.131702  \n",
       "2  88.55166667,88.46416667,88.31916667,88.24,88.2...   32.661304  \n",
       "3  88.615,88.53083333,88.4,88.27166667,88.2075,88...   53.850238  \n",
       "4  88.55666667,88.64083333,88.65833333,88.6475,88...  177.418750  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covert features  from string to List of values \n",
    "def replace_nan(x):\n",
    "    if x==\" \":\n",
    "        return np.nan\n",
    "    else :\n",
    "        return float(x)\n",
    "        \n",
    "features=[\"temp\",\"precip\",\"rel_humidity\",\"wind_dir\",\"wind_spd\",\"atmos_press\"]\n",
    "for feature in features : \n",
    "    train[feature]=train[feature].apply(lambda x: [ replace_nan(X) for X in x.replace(\"nan\",\" \").split(\",\")])\n",
    "    #test[feature]=test[feature].apply(lambda x: [ replace_nan(X)  for X in x.replace(\"nan\",\" \").split(\",\")])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15539, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_columns = ['temp', 'precip', 'rel_humidity', 'wind_dir', 'wind_spd', 'atmos_press']\n",
    "def count_nan(row):\n",
    "    counter = 0\n",
    "    if isinstance(row, type(list())):\n",
    "        for x in row:\n",
    "            if np.isnan(x):\n",
    "                counter += 1\n",
    "        return counter \n",
    "    else: return np.nan\n",
    "    \n",
    "for col in list_columns:\n",
    "    train['nan_' + str(col)] = round(train[col].apply(count_nan))\n",
    "data_nan = train.drop(list_columns, axis=1)\n",
    "\n",
    "data_nan['sum'] = round(data_nan.sum(axis=1, numeric_only=True)/726*100,2)\n",
    "data_nan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14536, 16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_wo_outlier = train.join(data_nan['sum'])[(train.join(data_nan['sum'])['sum']<50)]\n",
    "data_wo_outlier.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>location</th>\n",
       "      <th>temp</th>\n",
       "      <th>precip</th>\n",
       "      <th>rel_humidity</th>\n",
       "      <th>wind_dir</th>\n",
       "      <th>wind_spd</th>\n",
       "      <th>atmos_press</th>\n",
       "      <th>target</th>\n",
       "      <th>nan_temp</th>\n",
       "      <th>nan_precip</th>\n",
       "      <th>nan_rel_humidity</th>\n",
       "      <th>nan_wind_dir</th>\n",
       "      <th>nan_wind_spd</th>\n",
       "      <th>nan_atmos_press</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_train_1</td>\n",
       "      <td>D</td>\n",
       "      <td>[22.53333333, 21.71666667, 20.83333333, 20.983...</td>\n",
       "      <td>[0.102, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.034, 0...</td>\n",
       "      <td>[0.744583333, 0.808083333, 0.911166667, 0.9163...</td>\n",
       "      <td>[281.6643101, 89.15629262, 81.96853891, 291.01...</td>\n",
       "      <td>[2.3775, 1.126666667, 0.700833333, 0.341666667...</td>\n",
       "      <td>[90.32, 90.3775, 90.44083333, 90.4725, 90.4541...</td>\n",
       "      <td>79.131702</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_train_10</td>\n",
       "      <td>A</td>\n",
       "      <td>[28.975, 27.95, 29.6, 26.425, 22.09166667, 21....</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.102, 0.136, 0.0, 0.0, 2.16, ...</td>\n",
       "      <td>[0.573333333, 0.597166667, 0.5668333329999999,...</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>[88.55166667, 88.46416667, 88.31916667, 88.24,...</td>\n",
       "      <td>32.661304</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>17.72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID location                                               temp  \\\n",
       "1   ID_train_1        D  [22.53333333, 21.71666667, 20.83333333, 20.983...   \n",
       "2  ID_train_10        A  [28.975, 27.95, 29.6, 26.425, 22.09166667, 21....   \n",
       "\n",
       "                                              precip  \\\n",
       "1  [0.102, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.034, 0...   \n",
       "2  [0.0, 0.0, 0.0, 0.102, 0.136, 0.0, 0.0, 2.16, ...   \n",
       "\n",
       "                                        rel_humidity  \\\n",
       "1  [0.744583333, 0.808083333, 0.911166667, 0.9163...   \n",
       "2  [0.573333333, 0.597166667, 0.5668333329999999,...   \n",
       "\n",
       "                                            wind_dir  \\\n",
       "1  [281.6643101, 89.15629262, 81.96853891, 291.01...   \n",
       "2  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
       "\n",
       "                                            wind_spd  \\\n",
       "1  [2.3775, 1.126666667, 0.700833333, 0.341666667...   \n",
       "2  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
       "\n",
       "                                         atmos_press     target  nan_temp  \\\n",
       "1  [90.32, 90.3775, 90.44083333, 90.4725, 90.4541...  79.131702         0   \n",
       "2  [88.55166667, 88.46416667, 88.31916667, 88.24,...  32.661304         1   \n",
       "\n",
       "   nan_precip  nan_rel_humidity  nan_wind_dir  nan_wind_spd  nan_atmos_press  \\\n",
       "1           0                 0             0             0                0   \n",
       "2           1                 1            46            46                1   \n",
       "\n",
       "     sum  \n",
       "1  10.90  \n",
       "2  17.72  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_wo_outlier.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fill nan values with mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the nan values and show the remain values\n",
    "def mean_without_nan(row):\n",
    "    mean = np.nanmean(row)\n",
    "    new_row = []\n",
    "    for x in row: \n",
    "        if np.isnan(x): new_row.append(mean)\n",
    "        else: new_row.append(x)\n",
    "    return new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nan_temp',\n",
       " 'nan_precip',\n",
       " 'nan_rel_humidity',\n",
       " 'nan_wind_dir',\n",
       " 'nan_wind_spd',\n",
       " 'nan_atmos_press']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = data_wo_outlier.columns[2:8]\n",
    "features\n",
    "nan_featuers = [\"nan_\"+ str(feature) for feature in features]\n",
    "nan_featuers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill nan\n",
    "for feat in features:\n",
    "    data_wo_outlier[\"new_\" + str(feat)] = data_wo_outlier[feat].apply(mean_without_nan)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_drop = features #.extend(nan_featuers,['sum'])\n",
    "data_process = data_wo_outlier.drop(features, axis=1)\n",
    "data_process.drop(nan_featuers, axis=1, inplace=True)\n",
    "data_process.drop(['sum'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>location</th>\n",
       "      <th>target</th>\n",
       "      <th>new_temp</th>\n",
       "      <th>new_precip</th>\n",
       "      <th>new_rel_humidity</th>\n",
       "      <th>new_wind_dir</th>\n",
       "      <th>new_wind_spd</th>\n",
       "      <th>new_atmos_press</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_train_1</td>\n",
       "      <td>D</td>\n",
       "      <td>79.131702</td>\n",
       "      <td>[22.53333333, 21.71666667, 20.83333333, 20.983...</td>\n",
       "      <td>[0.102, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.034, 0...</td>\n",
       "      <td>[0.744583333, 0.808083333, 0.911166667, 0.9163...</td>\n",
       "      <td>[281.6643101, 89.15629262, 81.96853891, 291.01...</td>\n",
       "      <td>[2.3775, 1.126666667, 0.700833333, 0.341666667...</td>\n",
       "      <td>[90.32, 90.3775, 90.44083333, 90.4725, 90.4541...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_train_10</td>\n",
       "      <td>A</td>\n",
       "      <td>32.661304</td>\n",
       "      <td>[28.975, 27.95, 29.6, 26.425, 22.09166667, 21....</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.102, 0.136, 0.0, 0.0, 2.16, ...</td>\n",
       "      <td>[0.573333333, 0.597166667, 0.5668333329999999,...</td>\n",
       "      <td>[201.37321071165334, 201.37321071165334, 201.3...</td>\n",
       "      <td>[1.0386555556, 1.0386555556, 1.0386555556, 1.0...</td>\n",
       "      <td>[88.55166667, 88.46416667, 88.31916667, 88.24,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_train_100</td>\n",
       "      <td>A</td>\n",
       "      <td>53.850238</td>\n",
       "      <td>[22.96666667, 24.26666667, 25.275, 25.625, 25....</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7.77,...</td>\n",
       "      <td>[0.8430833329999999, 0.79025, 0.7375, 0.728, 0...</td>\n",
       "      <td>[300.0850574, 293.6769595, 294.5174647, 301.92...</td>\n",
       "      <td>[1.446666667, 1.1925, 1.324166667, 1.544166666...</td>\n",
       "      <td>[88.615, 88.53083333, 88.4, 88.27166667, 88.20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ID_train_10000</td>\n",
       "      <td>E</td>\n",
       "      <td>17.005000</td>\n",
       "      <td>[26.225, 26.25, 26.95833333, 27.925, 28.416666...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.7566666670000001, 0.7090833329999999, 0.632...</td>\n",
       "      <td>[168.29624559847116, 104.3737208, 161.28512859...</td>\n",
       "      <td>[1.1332211538846155, 1.4858333330000002, 1.090...</td>\n",
       "      <td>[88.56583333, 88.55, 88.50916667, 88.44083333,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ID_train_10001</td>\n",
       "      <td>C</td>\n",
       "      <td>53.100000</td>\n",
       "      <td>[21.86666667, 21.38333333, 20.75, 20.74166667,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.548333333, 0.571666667, 0.589166667, 0.57, ...</td>\n",
       "      <td>[334.0827161, 332.4037603, 328.58290530000005,...</td>\n",
       "      <td>[1.1525, 1.055, 1.11, 1.385833333, 1.2, 1.2508...</td>\n",
       "      <td>[87.73666667, 87.74416667, 87.7475, 87.78, 87....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID location     target  \\\n",
       "1      ID_train_1        D  79.131702   \n",
       "2     ID_train_10        A  32.661304   \n",
       "3    ID_train_100        A  53.850238   \n",
       "5  ID_train_10000        E  17.005000   \n",
       "6  ID_train_10001        C  53.100000   \n",
       "\n",
       "                                            new_temp  \\\n",
       "1  [22.53333333, 21.71666667, 20.83333333, 20.983...   \n",
       "2  [28.975, 27.95, 29.6, 26.425, 22.09166667, 21....   \n",
       "3  [22.96666667, 24.26666667, 25.275, 25.625, 25....   \n",
       "5  [26.225, 26.25, 26.95833333, 27.925, 28.416666...   \n",
       "6  [21.86666667, 21.38333333, 20.75, 20.74166667,...   \n",
       "\n",
       "                                          new_precip  \\\n",
       "1  [0.102, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.034, 0...   \n",
       "2  [0.0, 0.0, 0.0, 0.102, 0.136, 0.0, 0.0, 2.16, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7.77,...   \n",
       "5  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "6  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                    new_rel_humidity  \\\n",
       "1  [0.744583333, 0.808083333, 0.911166667, 0.9163...   \n",
       "2  [0.573333333, 0.597166667, 0.5668333329999999,...   \n",
       "3  [0.8430833329999999, 0.79025, 0.7375, 0.728, 0...   \n",
       "5  [0.7566666670000001, 0.7090833329999999, 0.632...   \n",
       "6  [0.548333333, 0.571666667, 0.589166667, 0.57, ...   \n",
       "\n",
       "                                        new_wind_dir  \\\n",
       "1  [281.6643101, 89.15629262, 81.96853891, 291.01...   \n",
       "2  [201.37321071165334, 201.37321071165334, 201.3...   \n",
       "3  [300.0850574, 293.6769595, 294.5174647, 301.92...   \n",
       "5  [168.29624559847116, 104.3737208, 161.28512859...   \n",
       "6  [334.0827161, 332.4037603, 328.58290530000005,...   \n",
       "\n",
       "                                        new_wind_spd  \\\n",
       "1  [2.3775, 1.126666667, 0.700833333, 0.341666667...   \n",
       "2  [1.0386555556, 1.0386555556, 1.0386555556, 1.0...   \n",
       "3  [1.446666667, 1.1925, 1.324166667, 1.544166666...   \n",
       "5  [1.1332211538846155, 1.4858333330000002, 1.090...   \n",
       "6  [1.1525, 1.055, 1.11, 1.385833333, 1.2, 1.2508...   \n",
       "\n",
       "                                     new_atmos_press  \n",
       "1  [90.32, 90.3775, 90.44083333, 90.4725, 90.4541...  \n",
       "2  [88.55166667, 88.46416667, 88.31916667, 88.24,...  \n",
       "3  [88.615, 88.53083333, 88.4, 88.27166667, 88.20...  \n",
       "5  [88.56583333, 88.55, 88.50916667, 88.44083333,...  \n",
       "6  [87.73666667, 87.74416667, 87.7475, 87.78, 87....  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_process\n",
    "data_process.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create new dataframes for each feature and observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['df_temp',\n",
       " 'df_precip',\n",
       " 'df_rel_humidity',\n",
       " 'df_wind_dir',\n",
       " 'df_wind_spd',\n",
       " 'df_atmos_press']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# name of dataframes:\n",
    "df_names = [\"df_\" + str(feat) for feat in features]\n",
    "df_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "features\n",
    "for f in features: \n",
    "    exec(f'df_{f} = pd.DataFrame(data_process[\"new_{f}\"])')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unpack all values: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new_atmos_press</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[90.32, 90.3775, 90.44083333, 90.4725, 90.4541...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[88.55166667, 88.46416667, 88.31916667, 88.24,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[88.615, 88.53083333, 88.4, 88.27166667, 88.20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[88.56583333, 88.55, 88.50916667, 88.44083333,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[87.73666667, 87.74416667, 87.7475, 87.78, 87....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15534</th>\n",
       "      <td>[88.27666667, 88.24333333, 88.18166667, 88.175...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15535</th>\n",
       "      <td>[88.43666667, 88.34583333, 88.26083333, 88.174...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15536</th>\n",
       "      <td>[90.61583333, 90.44916667, 90.32916667, 90.27,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15537</th>\n",
       "      <td>[90.31416667, 90.23916667, 90.20166667, 90.222...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15538</th>\n",
       "      <td>[88.0675, 88.11666667, 88.0925, 88.03666667, 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14536 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         new_atmos_press\n",
       "1      [90.32, 90.3775, 90.44083333, 90.4725, 90.4541...\n",
       "2      [88.55166667, 88.46416667, 88.31916667, 88.24,...\n",
       "3      [88.615, 88.53083333, 88.4, 88.27166667, 88.20...\n",
       "5      [88.56583333, 88.55, 88.50916667, 88.44083333,...\n",
       "6      [87.73666667, 87.74416667, 87.7475, 87.78, 87....\n",
       "...                                                  ...\n",
       "15534  [88.27666667, 88.24333333, 88.18166667, 88.175...\n",
       "15535  [88.43666667, 88.34583333, 88.26083333, 88.174...\n",
       "15536  [90.61583333, 90.44916667, 90.32916667, 90.27,...\n",
       "15537  [90.31416667, 90.23916667, 90.20166667, 90.222...\n",
       "15538  [88.0675, 88.11666667, 88.0925, 88.03666667, 8...\n",
       "\n",
       "[14536 rows x 1 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_atmos_press"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['df_temp',\n",
       " 'df_precip',\n",
       " 'df_rel_humidity',\n",
       " 'df_wind_dir',\n",
       " 'df_wind_spd',\n",
       " 'df_atmos_press']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n",
      "/var/folders/gz/t1kyprv95tzc2bf6b375z94h0000gn/T/ipykernel_8336/4261563240.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[name+ str(x)] = df[name].str[x]\n"
     ]
    }
   ],
   "source": [
    "for dframe in df_names:\n",
    "    df = locals()[dframe]\n",
    "    name = df.columns[0]\n",
    "    for x in range(121): # 121 messdaten pro punkt \n",
    "        df[name+ str(x)] = df[name].str[x]\n",
    "        #df.drop(name, axis=1, inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dframe in df_names:\n",
    "    df = locals()[dframe]\n",
    "    name = df.columns[0]\n",
    "    df.drop(name, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add meta information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>location</th>\n",
       "      <th>loc_altitude</th>\n",
       "      <th>km2</th>\n",
       "      <th>aspect</th>\n",
       "      <th>dist_motorway</th>\n",
       "      <th>dist_trunk</th>\n",
       "      <th>dist_primary</th>\n",
       "      <th>dist_secondary</th>\n",
       "      <th>dist_tertiary</th>\n",
       "      <th>dist_unclassified</th>\n",
       "      <th>dist_residential</th>\n",
       "      <th>popn</th>\n",
       "      <th>hh</th>\n",
       "      <th>hh_cook_charcoal</th>\n",
       "      <th>hh_cook_firewood</th>\n",
       "      <th>hh_burn_waste</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>1122.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>194.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.695789</td>\n",
       "      <td>343.595039</td>\n",
       "      <td>575.917422</td>\n",
       "      <td>330.609776</td>\n",
       "      <td>254.307415</td>\n",
       "      <td>4763</td>\n",
       "      <td>809</td>\n",
       "      <td>508</td>\n",
       "      <td>43</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>1155.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>219.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>528.078476</td>\n",
       "      <td>2172.680462</td>\n",
       "      <td>1144.376412</td>\n",
       "      <td>531.103271</td>\n",
       "      <td>65.142004</td>\n",
       "      <td>1.042809</td>\n",
       "      <td>22243</td>\n",
       "      <td>5735</td>\n",
       "      <td>116</td>\n",
       "      <td>1144</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>C</td>\n",
       "      <td>1178.3</td>\n",
       "      <td>8.5</td>\n",
       "      <td>168.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.885520</td>\n",
       "      <td>4794.704552</td>\n",
       "      <td>12.730489</td>\n",
       "      <td>72.459340</td>\n",
       "      <td>406.967815</td>\n",
       "      <td>13.931350</td>\n",
       "      <td>97895</td>\n",
       "      <td>26873</td>\n",
       "      <td>21316</td>\n",
       "      <td>751</td>\n",
       "      <td>9835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>D</td>\n",
       "      <td>980.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>90.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>265.896472</td>\n",
       "      <td>NaN</td>\n",
       "      <td>339.693908</td>\n",
       "      <td>76.024389</td>\n",
       "      <td>11.601442</td>\n",
       "      <td>3596</td>\n",
       "      <td>663</td>\n",
       "      <td>7</td>\n",
       "      <td>651</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>E</td>\n",
       "      <td>1186.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>121.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>850.423131</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1738.872942</td>\n",
       "      <td>137.722087</td>\n",
       "      <td>449.591885</td>\n",
       "      <td>3.836589</td>\n",
       "      <td>6064</td>\n",
       "      <td>1297</td>\n",
       "      <td>985</td>\n",
       "      <td>26</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 location  loc_altitude  km2  aspect  dist_motorway  dist_trunk  \\\n",
       "0           0        A        1122.4  1.9   194.0            NaN         NaN   \n",
       "1           1        B        1155.4  5.4   219.8            NaN  528.078476   \n",
       "2           2        C        1178.3  8.5   168.7            NaN   32.885520   \n",
       "3           3        D         980.8  0.8    90.0            NaN         NaN   \n",
       "4           4        E        1186.5  1.6   121.0            NaN  850.423131   \n",
       "\n",
       "   dist_primary  dist_secondary  dist_tertiary  dist_unclassified  \\\n",
       "0     14.695789      343.595039     575.917422         330.609776   \n",
       "1   2172.680462     1144.376412     531.103271          65.142004   \n",
       "2   4794.704552       12.730489      72.459340         406.967815   \n",
       "3    265.896472             NaN     339.693908          76.024389   \n",
       "4           NaN     1738.872942     137.722087         449.591885   \n",
       "\n",
       "   dist_residential   popn     hh  hh_cook_charcoal  hh_cook_firewood  \\\n",
       "0        254.307415   4763    809               508                43   \n",
       "1          1.042809  22243   5735               116              1144   \n",
       "2         13.931350  97895  26873             21316               751   \n",
       "3         11.601442   3596    663                 7               651   \n",
       "4          3.836589   6064   1297               985                26   \n",
       "\n",
       "   hh_burn_waste  \n",
       "0            142  \n",
       "1            239  \n",
       "2           9835  \n",
       "3             99  \n",
       "4             43  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input nan dist_trunk, dist_primary and dist_secondary with knn imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "meta_imp = imputer.fit_transform(meta.drop(['location'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>loc_altitude</th>\n",
       "      <th>km2</th>\n",
       "      <th>aspect</th>\n",
       "      <th>dist_trunk</th>\n",
       "      <th>dist_primary</th>\n",
       "      <th>dist_secondary</th>\n",
       "      <th>dist_tertiary</th>\n",
       "      <th>dist_unclassified</th>\n",
       "      <th>dist_residential</th>\n",
       "      <th>popn</th>\n",
       "      <th>hh</th>\n",
       "      <th>hh_cook_charcoal</th>\n",
       "      <th>hh_cook_firewood</th>\n",
       "      <th>hh_burn_waste</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1122.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>194.0</td>\n",
       "      <td>689.250804</td>\n",
       "      <td>14.695789</td>\n",
       "      <td>343.595039</td>\n",
       "      <td>575.917422</td>\n",
       "      <td>330.609776</td>\n",
       "      <td>254.307415</td>\n",
       "      <td>4763.0</td>\n",
       "      <td>809.0</td>\n",
       "      <td>508.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>142.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1155.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>219.8</td>\n",
       "      <td>528.078476</td>\n",
       "      <td>2172.680462</td>\n",
       "      <td>1144.376412</td>\n",
       "      <td>531.103271</td>\n",
       "      <td>65.142004</td>\n",
       "      <td>1.042809</td>\n",
       "      <td>22243.0</td>\n",
       "      <td>5735.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>1144.0</td>\n",
       "      <td>239.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1178.3</td>\n",
       "      <td>8.5</td>\n",
       "      <td>168.7</td>\n",
       "      <td>32.885520</td>\n",
       "      <td>4794.704552</td>\n",
       "      <td>12.730489</td>\n",
       "      <td>72.459340</td>\n",
       "      <td>406.967815</td>\n",
       "      <td>13.931350</td>\n",
       "      <td>97895.0</td>\n",
       "      <td>26873.0</td>\n",
       "      <td>21316.0</td>\n",
       "      <td>751.0</td>\n",
       "      <td>9835.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>980.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>90.0</td>\n",
       "      <td>689.250804</td>\n",
       "      <td>265.896472</td>\n",
       "      <td>1041.233990</td>\n",
       "      <td>339.693908</td>\n",
       "      <td>76.024389</td>\n",
       "      <td>11.601442</td>\n",
       "      <td>3596.0</td>\n",
       "      <td>663.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>651.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1186.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>121.0</td>\n",
       "      <td>850.423131</td>\n",
       "      <td>140.296131</td>\n",
       "      <td>1738.872942</td>\n",
       "      <td>137.722087</td>\n",
       "      <td>449.591885</td>\n",
       "      <td>3.836589</td>\n",
       "      <td>6064.0</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>985.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   location  loc_altitude  km2  aspect  dist_trunk  dist_primary  \\\n",
       "0       0.0        1122.4  1.9   194.0  689.250804     14.695789   \n",
       "1       1.0        1155.4  5.4   219.8  528.078476   2172.680462   \n",
       "2       2.0        1178.3  8.5   168.7   32.885520   4794.704552   \n",
       "3       3.0         980.8  0.8    90.0  689.250804    265.896472   \n",
       "4       4.0        1186.5  1.6   121.0  850.423131    140.296131   \n",
       "\n",
       "   dist_secondary  dist_tertiary  dist_unclassified  dist_residential  \\\n",
       "0      343.595039     575.917422         330.609776        254.307415   \n",
       "1     1144.376412     531.103271          65.142004          1.042809   \n",
       "2       12.730489      72.459340         406.967815         13.931350   \n",
       "3     1041.233990     339.693908          76.024389         11.601442   \n",
       "4     1738.872942     137.722087         449.591885          3.836589   \n",
       "\n",
       "      popn       hh  hh_cook_charcoal  hh_cook_firewood  hh_burn_waste  \n",
       "0   4763.0    809.0             508.0              43.0          142.0  \n",
       "1  22243.0   5735.0             116.0            1144.0          239.0  \n",
       "2  97895.0  26873.0           21316.0             751.0         9835.0  \n",
       "3   3596.0    663.0               7.0             651.0           99.0  \n",
       "4   6064.0   1297.0             985.0              26.0           43.0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_imp = pd.DataFrame(meta_imp)\n",
    "meta_imp.columns = [col for col in meta.columns[1:] if col != 'dist_motorway']\n",
    "meta_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_imp['location'] = meta.location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>loc_altitude</th>\n",
       "      <th>km2</th>\n",
       "      <th>aspect</th>\n",
       "      <th>dist_trunk</th>\n",
       "      <th>dist_primary</th>\n",
       "      <th>dist_secondary</th>\n",
       "      <th>dist_tertiary</th>\n",
       "      <th>dist_unclassified</th>\n",
       "      <th>dist_residential</th>\n",
       "      <th>popn</th>\n",
       "      <th>hh</th>\n",
       "      <th>hh_cook_charcoal</th>\n",
       "      <th>hh_cook_firewood</th>\n",
       "      <th>hh_burn_waste</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>1122.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>194.0</td>\n",
       "      <td>689.250804</td>\n",
       "      <td>14.695789</td>\n",
       "      <td>343.595039</td>\n",
       "      <td>575.917422</td>\n",
       "      <td>330.609776</td>\n",
       "      <td>254.307415</td>\n",
       "      <td>4763.0</td>\n",
       "      <td>809.0</td>\n",
       "      <td>508.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>142.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>1155.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>219.8</td>\n",
       "      <td>528.078476</td>\n",
       "      <td>2172.680462</td>\n",
       "      <td>1144.376412</td>\n",
       "      <td>531.103271</td>\n",
       "      <td>65.142004</td>\n",
       "      <td>1.042809</td>\n",
       "      <td>22243.0</td>\n",
       "      <td>5735.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>1144.0</td>\n",
       "      <td>239.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C</td>\n",
       "      <td>1178.3</td>\n",
       "      <td>8.5</td>\n",
       "      <td>168.7</td>\n",
       "      <td>32.885520</td>\n",
       "      <td>4794.704552</td>\n",
       "      <td>12.730489</td>\n",
       "      <td>72.459340</td>\n",
       "      <td>406.967815</td>\n",
       "      <td>13.931350</td>\n",
       "      <td>97895.0</td>\n",
       "      <td>26873.0</td>\n",
       "      <td>21316.0</td>\n",
       "      <td>751.0</td>\n",
       "      <td>9835.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D</td>\n",
       "      <td>980.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>90.0</td>\n",
       "      <td>689.250804</td>\n",
       "      <td>265.896472</td>\n",
       "      <td>1041.233990</td>\n",
       "      <td>339.693908</td>\n",
       "      <td>76.024389</td>\n",
       "      <td>11.601442</td>\n",
       "      <td>3596.0</td>\n",
       "      <td>663.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>651.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E</td>\n",
       "      <td>1186.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>121.0</td>\n",
       "      <td>850.423131</td>\n",
       "      <td>140.296131</td>\n",
       "      <td>1738.872942</td>\n",
       "      <td>137.722087</td>\n",
       "      <td>449.591885</td>\n",
       "      <td>3.836589</td>\n",
       "      <td>6064.0</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>985.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  location  loc_altitude  km2  aspect  dist_trunk  dist_primary  \\\n",
       "0        A        1122.4  1.9   194.0  689.250804     14.695789   \n",
       "1        B        1155.4  5.4   219.8  528.078476   2172.680462   \n",
       "2        C        1178.3  8.5   168.7   32.885520   4794.704552   \n",
       "3        D         980.8  0.8    90.0  689.250804    265.896472   \n",
       "4        E        1186.5  1.6   121.0  850.423131    140.296131   \n",
       "\n",
       "   dist_secondary  dist_tertiary  dist_unclassified  dist_residential  \\\n",
       "0      343.595039     575.917422         330.609776        254.307415   \n",
       "1     1144.376412     531.103271          65.142004          1.042809   \n",
       "2       12.730489      72.459340         406.967815         13.931350   \n",
       "3     1041.233990     339.693908          76.024389         11.601442   \n",
       "4     1738.872942     137.722087         449.591885          3.836589   \n",
       "\n",
       "      popn       hh  hh_cook_charcoal  hh_cook_firewood  hh_burn_waste  \n",
       "0   4763.0    809.0             508.0              43.0          142.0  \n",
       "1  22243.0   5735.0             116.0            1144.0          239.0  \n",
       "2  97895.0  26873.0           21316.0             751.0         9835.0  \n",
       "3   3596.0    663.0               7.0             651.0           99.0  \n",
       "4   6064.0   1297.0             985.0              26.0           43.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add the meta data into each dataframe according to the location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- add meta daten to the df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dframe in df_names:\n",
    "    df = locals()[dframe]\n",
    "    \n",
    "    name = df.columns[0]\n",
    "    df_1 = df.join(data_process[\"location\"])\n",
    "    exec(f'f_{name} = pd.merge(df_1, meta_imp, how=\"left\", on=\"location\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_features = []\n",
    "for dframe in df_names:\n",
    "    df = locals()[dframe]\n",
    "    name = df.columns[0]\n",
    "    list_features.append(\"f_\"+str(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f_new_temp0',\n",
       " 'f_new_precip0',\n",
       " 'f_new_rel_humidity0',\n",
       " 'f_new_wind_dir0',\n",
       " 'f_new_wind_spd0',\n",
       " 'f_new_atmos_press0']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data_process.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14536, 121)\n",
      "(14536, 121)\n",
      "(14536, 121)\n",
      "(14536, 121)\n",
      "(14536, 121)\n",
      "(14536, 121)\n"
     ]
    }
   ],
   "source": [
    "list_features = []\n",
    "for dframe in df_names:\n",
    "    df = locals()[dframe]\n",
    "    print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14536, 9)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_process.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model forecast for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict(model, df, target):\n",
    "    # data split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df, target,random_state=44)\n",
    "    #model\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    # predict\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    rsme_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "    rsme_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "    test_results[str(model) + \"_\"+ df.columns[0]] = [rsme_train, rsme_test]\n",
    "\n",
    "    print(df.columns[0])\n",
    "    print (f'[{rsme_train:.2f}, {rsme_test:.2f}]')\n",
    "    return y_pred_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_temp0\n",
      "[38.04, 38.50]\n",
      "new_precip0\n",
      "[40.44, 41.09]\n",
      "new_rel_humidity0\n",
      "[38.57, 39.10]\n",
      "new_wind_dir0\n",
      "[39.63, 40.19]\n",
      "new_wind_spd0\n",
      "[38.95, 39.39]\n",
      "new_atmos_press0\n",
      "[38.58, 38.92]\n"
     ]
    }
   ],
   "source": [
    "test_results = {}\n",
    "linear = LinearRegression()\n",
    "for dframe in df_names:\n",
    "    df = locals()[dframe]\n",
    "    name = df.columns[0]\n",
    "\n",
    "    train_predict(linear, df, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LinearRegression()_new_temp0</th>\n",
       "      <td>38.044771</td>\n",
       "      <td>38.503181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearRegression()_new_precip0</th>\n",
       "      <td>40.441800</td>\n",
       "      <td>41.090318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearRegression()_new_rel_humidity0</th>\n",
       "      <td>38.566880</td>\n",
       "      <td>39.104745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearRegression()_new_wind_dir0</th>\n",
       "      <td>39.627375</td>\n",
       "      <td>40.188481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearRegression()_new_wind_spd0</th>\n",
       "      <td>38.951910</td>\n",
       "      <td>39.394749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearRegression()_new_atmos_press0</th>\n",
       "      <td>38.578760</td>\n",
       "      <td>38.918730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              0          1\n",
       "LinearRegression()_new_temp0          38.044771  38.503181\n",
       "LinearRegression()_new_precip0        40.441800  41.090318\n",
       "LinearRegression()_new_rel_humidity0  38.566880  39.104745\n",
       "LinearRegression()_new_wind_dir0      39.627375  40.188481\n",
       "LinearRegression()_new_wind_spd0      38.951910  39.394749\n",
       "LinearRegression()_new_atmos_press0   38.578760  38.918730"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(test_results).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "temperatur has the best fit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_compile_and_fit(df, target, max_epochs=30):\n",
    "    # Get optimizer\n",
    "    #optimizer=tf.keras.optimizers.Adam()\n",
    "    # data split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df, target,random_state=44)\n",
    "\n",
    "    # model \n",
    "    with tf.device('/cpu:0'):\n",
    "      model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(32,kernel_initializer = 'uniform', activation='relu', input_dim = 121),\n",
    "            #tf.keras.layers.Dense(121,kernel_initializer = 'uniform', activation='relu', input_dim = 121),\n",
    "            tf.keras.layers.Dense(1,kernel_initializer = 'uniform')\n",
    "            ])\n",
    "\n",
    "    N_VAL =  len(X_test)\n",
    "    N_TRAIN = len(X_train)\n",
    "    BATCH_SIZE = 96\n",
    "    STEPS_PER_EPOCH = N_TRAIN // BATCH_SIZE\n",
    "    EPOCHS = 500\n",
    "\n",
    "    # model.compile\n",
    "    model.compile(optimizer='Adam',\n",
    "                metrics='mse', # [tf.keras.metrics.RootMeanSquaredError()]\n",
    "                loss='mae')\n",
    "    # model.fit\n",
    "    with tf.device('/cpu:0'):\n",
    "      train_history = model.fit(X_train, \n",
    "                        y_train,\n",
    "                        validation_split=0.2,\n",
    "                        verbose=1,\n",
    "                        steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                        epochs=EPOCHS, \n",
    "                        )\n",
    "   \n",
    "\n",
    "    # Predict values for test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "    print(df.columns[0])\n",
    "    print (f'[{rmse_train:.2f}, {rmse_test:.2f}]')\n",
    "\n",
    "    test_results[str(model) + \"_\"+ df.columns[0]] = [rmse_train, rmse_test]\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 20:21:37.349401: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-04-14 20:21:37.349704: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2022-04-14 20:21:37.457311: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 98/113 [=========================>....] - ETA: 0s - loss: 30.0935 - mse: 2237.5078"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 20:21:37.776751: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 1s 3ms/step - loss: 29.4827 - mse: 2173.6689 - val_loss: 26.8012 - val_mse: 1987.3652\n",
      "Epoch 2/500\n",
      " 36/113 [========>.....................] - ETA: 0s - loss: 25.7583 - mse: 1869.2821"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 20:21:38.045282: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 0s 2ms/step - loss: 25.7868 - mse: 1806.2052 - val_loss: 27.2775 - val_mse: 2136.6946\n",
      "Epoch 3/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.6375 - mse: 1791.6670 - val_loss: 26.7566 - val_mse: 1935.1958\n",
      "Epoch 4/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.7316 - mse: 1809.9119 - val_loss: 26.6512 - val_mse: 1964.0857\n",
      "Epoch 5/500\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 25.5134 - mse: 1781.7421 - val_loss: 26.5963 - val_mse: 1956.8031\n",
      "Epoch 6/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.6418 - mse: 1786.1873 - val_loss: 26.6304 - val_mse: 1907.3884\n",
      "Epoch 7/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.4779 - mse: 1793.6963 - val_loss: 26.6264 - val_mse: 1887.9357\n",
      "Epoch 8/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3116 - mse: 1764.6058 - val_loss: 26.4142 - val_mse: 1927.0101\n",
      "Epoch 9/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.2234 - mse: 1732.0486 - val_loss: 26.3602 - val_mse: 1962.9274\n",
      "Epoch 10/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3389 - mse: 1771.9250 - val_loss: 26.2704 - val_mse: 1918.1556\n",
      "Epoch 11/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3988 - mse: 1779.9900 - val_loss: 26.2134 - val_mse: 1940.1259\n",
      "Epoch 12/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1579 - mse: 1753.2169 - val_loss: 26.5665 - val_mse: 1817.7972\n",
      "Epoch 13/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.0638 - mse: 1736.4016 - val_loss: 26.1176 - val_mse: 1938.3464\n",
      "Epoch 14/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.5067 - mse: 1636.0776 - val_loss: 26.0682 - val_mse: 1929.3295\n",
      "Epoch 15/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.0938 - mse: 1771.2941 - val_loss: 26.0019 - val_mse: 1904.7825\n",
      "Epoch 16/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.0491 - mse: 1690.0687 - val_loss: 26.0146 - val_mse: 1848.2550\n",
      "Epoch 17/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.6687 - mse: 1687.6921 - val_loss: 25.9816 - val_mse: 1908.9572\n",
      "Epoch 18/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.5035 - mse: 1669.9514 - val_loss: 25.9714 - val_mse: 1911.7158\n",
      "Epoch 19/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1332 - mse: 1749.9600 - val_loss: 25.9983 - val_mse: 1926.0350\n",
      "Epoch 20/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.7226 - mse: 1672.7339 - val_loss: 25.9169 - val_mse: 1870.8524\n",
      "Epoch 21/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.7602 - mse: 1695.8228 - val_loss: 26.1465 - val_mse: 1790.8165\n",
      "Epoch 22/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.9660 - mse: 1719.9503 - val_loss: 25.9149 - val_mse: 1848.0771\n",
      "Epoch 23/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.5399 - mse: 1638.0465 - val_loss: 25.8992 - val_mse: 1847.7507\n",
      "Epoch 24/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4113 - mse: 1646.2551 - val_loss: 26.0697 - val_mse: 1793.7715\n",
      "Epoch 25/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.9688 - mse: 1722.3961 - val_loss: 26.0280 - val_mse: 1927.9990\n",
      "Epoch 26/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.6289 - mse: 1661.1798 - val_loss: 26.0734 - val_mse: 1786.6646\n",
      "Epoch 27/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.2904 - mse: 1611.5608 - val_loss: 25.8826 - val_mse: 1853.6855\n",
      "Epoch 28/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.9542 - mse: 1731.9902 - val_loss: 26.4664 - val_mse: 1756.4874\n",
      "Epoch 29/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.5748 - mse: 1661.7256 - val_loss: 26.0214 - val_mse: 1913.7350\n",
      "Epoch 30/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.5866 - mse: 1630.2218 - val_loss: 26.1131 - val_mse: 1776.5616\n",
      "Epoch 31/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.9387 - mse: 1738.1145 - val_loss: 25.9556 - val_mse: 1903.5892\n",
      "Epoch 32/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.4887 - mse: 1624.7852 - val_loss: 25.8739 - val_mse: 1836.3767\n",
      "Epoch 33/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.6855 - mse: 1700.6411 - val_loss: 25.9017 - val_mse: 1825.3822\n",
      "Epoch 34/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.5726 - mse: 1631.3950 - val_loss: 25.9302 - val_mse: 1807.8336\n",
      "Epoch 35/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.7995 - mse: 1719.8940 - val_loss: 25.8714 - val_mse: 1831.1071\n",
      "Epoch 36/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.5300 - mse: 1668.5166 - val_loss: 25.8985 - val_mse: 1814.6896\n",
      "Epoch 37/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3189 - mse: 1604.9200 - val_loss: 26.0082 - val_mse: 1929.1802\n",
      "Epoch 38/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.6929 - mse: 1660.3279 - val_loss: 25.8833 - val_mse: 1818.9209\n",
      "Epoch 39/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.6445 - mse: 1693.1790 - val_loss: 25.8830 - val_mse: 1867.5487\n",
      "Epoch 40/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.6898 - mse: 1695.7771 - val_loss: 25.9490 - val_mse: 1901.2169\n",
      "Epoch 41/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2709 - mse: 1632.4869 - val_loss: 25.8779 - val_mse: 1806.6418\n",
      "Epoch 42/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.7878 - mse: 1716.3900 - val_loss: 25.8842 - val_mse: 1801.3506\n",
      "Epoch 43/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1543 - mse: 1560.3217 - val_loss: 25.8146 - val_mse: 1838.2319\n",
      "Epoch 44/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.4916 - mse: 1709.3693 - val_loss: 25.8377 - val_mse: 1816.9086\n",
      "Epoch 45/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.8543 - mse: 1692.2765 - val_loss: 25.8404 - val_mse: 1821.0558\n",
      "Epoch 46/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.3372 - mse: 1629.6390 - val_loss: 25.8194 - val_mse: 1828.9661\n",
      "Epoch 47/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.7550 - mse: 1679.6302 - val_loss: 25.8480 - val_mse: 1832.5494\n",
      "Epoch 48/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3816 - mse: 1653.7213 - val_loss: 25.8429 - val_mse: 1881.5120\n",
      "Epoch 49/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.5138 - mse: 1673.6028 - val_loss: 25.7983 - val_mse: 1845.3168\n",
      "Epoch 50/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4762 - mse: 1663.2898 - val_loss: 25.8124 - val_mse: 1873.6542\n",
      "Epoch 51/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2988 - mse: 1620.0128 - val_loss: 25.8765 - val_mse: 1786.6016\n",
      "Epoch 52/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.6492 - mse: 1679.6011 - val_loss: 25.7714 - val_mse: 1823.5739\n",
      "Epoch 53/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3448 - mse: 1616.4259 - val_loss: 25.7749 - val_mse: 1839.3005\n",
      "Epoch 54/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.6196 - mse: 1691.8715 - val_loss: 25.7926 - val_mse: 1871.3097\n",
      "Epoch 55/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3643 - mse: 1643.4493 - val_loss: 25.7825 - val_mse: 1836.0339\n",
      "Epoch 56/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.4159 - mse: 1626.8751 - val_loss: 25.9298 - val_mse: 1774.8722\n",
      "Epoch 57/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.6508 - mse: 1711.5144 - val_loss: 25.7842 - val_mse: 1823.1007\n",
      "Epoch 58/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4826 - mse: 1623.0203 - val_loss: 25.8476 - val_mse: 1788.4370\n",
      "Epoch 59/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3170 - mse: 1620.9951 - val_loss: 25.7833 - val_mse: 1876.3234\n",
      "Epoch 60/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.4482 - mse: 1687.9680 - val_loss: 25.7355 - val_mse: 1830.1552\n",
      "Epoch 61/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.0848 - mse: 1627.6600 - val_loss: 25.7578 - val_mse: 1812.3541\n",
      "Epoch 62/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.7435 - mse: 1671.8693 - val_loss: 25.8116 - val_mse: 1803.6766\n",
      "Epoch 63/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.4709 - mse: 1627.8148 - val_loss: 26.0083 - val_mse: 1932.5039\n",
      "Epoch 64/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.7925 - mse: 1707.8694 - val_loss: 25.7661 - val_mse: 1869.4249\n",
      "Epoch 65/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0847 - mse: 1609.3480 - val_loss: 25.7867 - val_mse: 1800.7625\n",
      "Epoch 66/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4472 - mse: 1652.2762 - val_loss: 25.8277 - val_mse: 1786.4003\n",
      "Epoch 67/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.6526 - mse: 1674.1615 - val_loss: 25.7288 - val_mse: 1817.7814\n",
      "Epoch 68/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2589 - mse: 1624.1353 - val_loss: 25.7410 - val_mse: 1827.0439\n",
      "Epoch 69/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.8534 - mse: 1745.8882 - val_loss: 25.7361 - val_mse: 1840.2031\n",
      "Epoch 70/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0047 - mse: 1595.3781 - val_loss: 25.7697 - val_mse: 1799.0895\n",
      "Epoch 71/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3976 - mse: 1638.7206 - val_loss: 25.8147 - val_mse: 1783.0330\n",
      "Epoch 72/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.6407 - mse: 1676.6298 - val_loss: 25.7355 - val_mse: 1861.1329\n",
      "Epoch 73/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1969 - mse: 1638.0448 - val_loss: 25.7329 - val_mse: 1845.7946\n",
      "Epoch 74/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.5302 - mse: 1636.6708 - val_loss: 25.7589 - val_mse: 1848.3862\n",
      "Epoch 75/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4346 - mse: 1693.2227 - val_loss: 25.7428 - val_mse: 1809.4950\n",
      "Epoch 76/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1768 - mse: 1605.3635 - val_loss: 25.7320 - val_mse: 1804.3767\n",
      "Epoch 77/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4453 - mse: 1649.0836 - val_loss: 25.7226 - val_mse: 1806.9364\n",
      "Epoch 78/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.4439 - mse: 1646.5886 - val_loss: 25.7181 - val_mse: 1811.0217\n",
      "Epoch 79/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4675 - mse: 1680.0782 - val_loss: 25.7493 - val_mse: 1870.3638\n",
      "Epoch 80/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.5443 - mse: 1681.9438 - val_loss: 26.3130 - val_mse: 1735.9641\n",
      "Epoch 81/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2330 - mse: 1593.6747 - val_loss: 25.7412 - val_mse: 1870.3007\n",
      "Epoch 82/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.5979 - mse: 1714.1403 - val_loss: 25.6931 - val_mse: 1819.8838\n",
      "Epoch 83/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.3458 - mse: 1606.7732 - val_loss: 25.7227 - val_mse: 1850.0133\n",
      "Epoch 84/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.2564 - mse: 1631.7189 - val_loss: 25.6999 - val_mse: 1838.4750\n",
      "Epoch 85/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.3570 - mse: 1642.0099 - val_loss: 25.8582 - val_mse: 1779.9872\n",
      "Epoch 86/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0136 - mse: 1631.1257 - val_loss: 25.6957 - val_mse: 1817.3865\n",
      "Epoch 87/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.7703 - mse: 1682.1857 - val_loss: 25.6892 - val_mse: 1822.8757\n",
      "Epoch 88/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.9790 - mse: 1587.1655 - val_loss: 25.7083 - val_mse: 1843.3468\n",
      "Epoch 89/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.5283 - mse: 1659.6329 - val_loss: 25.8327 - val_mse: 1899.8910\n",
      "Epoch 90/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2246 - mse: 1636.2086 - val_loss: 25.7343 - val_mse: 1822.9802\n",
      "Epoch 91/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.6008 - mse: 1715.7727 - val_loss: 25.6855 - val_mse: 1825.7574\n",
      "Epoch 92/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.9837 - mse: 1574.9564 - val_loss: 25.7755 - val_mse: 1866.5093\n",
      "Epoch 93/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.7659 - mse: 1701.5908 - val_loss: 25.6856 - val_mse: 1821.1392\n",
      "Epoch 94/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2275 - mse: 1624.2220 - val_loss: 25.6989 - val_mse: 1832.9910\n",
      "Epoch 95/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.3965 - mse: 1635.8639 - val_loss: 25.7466 - val_mse: 1873.5587\n",
      "Epoch 96/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.4007 - mse: 1656.4949 - val_loss: 25.6807 - val_mse: 1837.6069\n",
      "Epoch 97/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3114 - mse: 1661.4084 - val_loss: 25.7735 - val_mse: 1882.0356\n",
      "Epoch 98/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.3552 - mse: 1621.2686 - val_loss: 25.7524 - val_mse: 1870.3866\n",
      "Epoch 99/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3525 - mse: 1635.5499 - val_loss: 25.9008 - val_mse: 1766.7332\n",
      "Epoch 100/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2139 - mse: 1641.5598 - val_loss: 26.0525 - val_mse: 1748.3258\n",
      "Epoch 101/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.5404 - mse: 1672.3264 - val_loss: 25.6880 - val_mse: 1805.0015\n",
      "Epoch 102/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.4540 - mse: 1650.4930 - val_loss: 25.9087 - val_mse: 1920.6809\n",
      "Epoch 103/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1819 - mse: 1625.5897 - val_loss: 25.6733 - val_mse: 1814.2330\n",
      "Epoch 104/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4508 - mse: 1644.3604 - val_loss: 25.6963 - val_mse: 1797.9308\n",
      "Epoch 105/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3083 - mse: 1643.0922 - val_loss: 25.7986 - val_mse: 1891.0085\n",
      "Epoch 106/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4581 - mse: 1650.8055 - val_loss: 25.8052 - val_mse: 1892.8218\n",
      "Epoch 107/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.5381 - mse: 1676.7640 - val_loss: 25.7616 - val_mse: 1780.6429\n",
      "Epoch 108/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1593 - mse: 1595.2201 - val_loss: 25.7731 - val_mse: 1777.3031\n",
      "Epoch 109/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4535 - mse: 1655.5349 - val_loss: 25.7149 - val_mse: 1868.5792\n",
      "Epoch 110/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3615 - mse: 1654.7751 - val_loss: 25.9102 - val_mse: 1761.8678\n",
      "Epoch 111/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3965 - mse: 1658.2712 - val_loss: 26.0359 - val_mse: 1754.0133\n",
      "Epoch 112/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3701 - mse: 1644.9568 - val_loss: 25.9670 - val_mse: 1760.2290\n",
      "Epoch 113/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3240 - mse: 1644.0613 - val_loss: 25.7965 - val_mse: 1772.0249\n",
      "Epoch 114/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3734 - mse: 1639.8842 - val_loss: 25.6946 - val_mse: 1858.8113\n",
      "Epoch 115/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4459 - mse: 1667.8683 - val_loss: 25.6722 - val_mse: 1825.5238\n",
      "Epoch 116/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1124 - mse: 1607.7731 - val_loss: 25.7085 - val_mse: 1794.2678\n",
      "Epoch 117/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4080 - mse: 1661.6162 - val_loss: 25.7168 - val_mse: 1787.4966\n",
      "Epoch 118/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3072 - mse: 1640.9023 - val_loss: 25.6919 - val_mse: 1845.4247\n",
      "Epoch 119/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1813 - mse: 1616.9852 - val_loss: 25.6773 - val_mse: 1840.4845\n",
      "Epoch 120/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4784 - mse: 1663.5641 - val_loss: 25.6693 - val_mse: 1814.0020\n",
      "Epoch 121/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3553 - mse: 1652.5667 - val_loss: 25.7037 - val_mse: 1855.3242\n",
      "Epoch 122/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1885 - mse: 1619.7506 - val_loss: 25.7947 - val_mse: 1880.7766\n",
      "Epoch 123/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.4512 - mse: 1675.9745 - val_loss: 25.6638 - val_mse: 1819.6460\n",
      "Epoch 124/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.2001 - mse: 1603.1432 - val_loss: 25.7103 - val_mse: 1863.2201\n",
      "Epoch 125/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.4931 - mse: 1657.6161 - val_loss: 25.6630 - val_mse: 1805.2073\n",
      "Epoch 126/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3801 - mse: 1657.1495 - val_loss: 25.7698 - val_mse: 1878.1504\n",
      "Epoch 127/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3933 - mse: 1660.2964 - val_loss: 25.7861 - val_mse: 1889.3743\n",
      "Epoch 128/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1083 - mse: 1592.1733 - val_loss: 25.7299 - val_mse: 1857.2141\n",
      "Epoch 129/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4782 - mse: 1659.3083 - val_loss: 25.6874 - val_mse: 1853.3860\n",
      "Epoch 130/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.2445 - mse: 1631.9884 - val_loss: 25.7704 - val_mse: 1814.1040\n",
      "Epoch 131/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.5666 - mse: 1689.3777 - val_loss: 25.7597 - val_mse: 1781.9943\n",
      "Epoch 132/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0065 - mse: 1603.5293 - val_loss: 25.7026 - val_mse: 1813.1093\n",
      "Epoch 133/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3044 - mse: 1639.0780 - val_loss: 25.6781 - val_mse: 1832.0791\n",
      "Epoch 134/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4212 - mse: 1669.1589 - val_loss: 25.6665 - val_mse: 1816.9200\n",
      "Epoch 135/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1918 - mse: 1636.7499 - val_loss: 25.6682 - val_mse: 1825.3719\n",
      "Epoch 136/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.6486 - mse: 1684.4786 - val_loss: 25.7120 - val_mse: 1817.5245\n",
      "Epoch 137/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0256 - mse: 1590.9797 - val_loss: 25.6772 - val_mse: 1793.7025\n",
      "Epoch 138/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0244 - mse: 1584.6853 - val_loss: 25.6534 - val_mse: 1820.1714\n",
      "Epoch 139/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.5255 - mse: 1670.8746 - val_loss: 25.7761 - val_mse: 1786.8993\n",
      "Epoch 140/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9522 - mse: 1595.5281 - val_loss: 25.8898 - val_mse: 1913.6512\n",
      "Epoch 141/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2017 - mse: 1619.8933 - val_loss: 25.9241 - val_mse: 1923.0015\n",
      "Epoch 142/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.9496 - mse: 1751.9473 - val_loss: 25.6536 - val_mse: 1836.7772\n",
      "Epoch 143/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3466 - mse: 1624.6627 - val_loss: 25.6501 - val_mse: 1811.5129\n",
      "Epoch 144/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8999 - mse: 1581.6617 - val_loss: 25.6798 - val_mse: 1841.5886\n",
      "Epoch 145/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4486 - mse: 1663.2883 - val_loss: 26.0652 - val_mse: 1943.0759\n",
      "Epoch 146/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1836 - mse: 1617.6334 - val_loss: 25.6512 - val_mse: 1824.6064\n",
      "Epoch 147/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3279 - mse: 1658.9135 - val_loss: 25.6570 - val_mse: 1800.7452\n",
      "Epoch 148/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1376 - mse: 1597.9308 - val_loss: 25.6541 - val_mse: 1832.9419\n",
      "Epoch 149/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1045 - mse: 1644.4321 - val_loss: 25.7188 - val_mse: 1821.9966\n",
      "Epoch 150/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.7952 - mse: 1679.4050 - val_loss: 25.8574 - val_mse: 1757.8127\n",
      "Epoch 151/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1516 - mse: 1631.3407 - val_loss: 25.6602 - val_mse: 1825.4097\n",
      "Epoch 152/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2777 - mse: 1631.3240 - val_loss: 25.9544 - val_mse: 1753.5968\n",
      "Epoch 153/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.5342 - mse: 1677.0642 - val_loss: 25.6606 - val_mse: 1797.6864\n",
      "Epoch 154/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0962 - mse: 1625.7999 - val_loss: 25.8715 - val_mse: 1838.8875\n",
      "Epoch 155/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4130 - mse: 1647.3844 - val_loss: 25.8279 - val_mse: 1808.0046\n",
      "Epoch 156/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8735 - mse: 1570.6550 - val_loss: 25.6826 - val_mse: 1787.8700\n",
      "Epoch 157/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1721 - mse: 1632.8145 - val_loss: 25.6384 - val_mse: 1804.6205\n",
      "Epoch 158/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.7124 - mse: 1697.6667 - val_loss: 25.8116 - val_mse: 1877.3235\n",
      "Epoch 159/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3396 - mse: 1630.2546 - val_loss: 25.7052 - val_mse: 1802.9222\n",
      "Epoch 160/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3329 - mse: 1635.2363 - val_loss: 25.6303 - val_mse: 1808.7109\n",
      "Epoch 161/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3511 - mse: 1659.2936 - val_loss: 25.6624 - val_mse: 1849.9319\n",
      "Epoch 162/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.2694 - mse: 1614.0769 - val_loss: 25.6807 - val_mse: 1790.3259\n",
      "Epoch 163/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.8896 - mse: 1615.8196 - val_loss: 25.6876 - val_mse: 1793.4744\n",
      "Epoch 164/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.6221 - mse: 1651.5122 - val_loss: 25.6474 - val_mse: 1843.9738\n",
      "Epoch 165/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.2558 - mse: 1659.6218 - val_loss: 25.6310 - val_mse: 1837.3488\n",
      "Epoch 166/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7463 - mse: 1555.7959 - val_loss: 25.6740 - val_mse: 1785.2391\n",
      "Epoch 167/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3235 - mse: 1603.8145 - val_loss: 25.6165 - val_mse: 1815.4343\n",
      "Epoch 168/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.7698 - mse: 1739.3280 - val_loss: 25.6180 - val_mse: 1821.5643\n",
      "Epoch 169/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8280 - mse: 1567.7140 - val_loss: 25.6581 - val_mse: 1785.1906\n",
      "Epoch 170/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3255 - mse: 1635.0020 - val_loss: 25.8505 - val_mse: 1912.5120\n",
      "Epoch 171/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4983 - mse: 1679.2715 - val_loss: 25.9136 - val_mse: 1923.0372\n",
      "Epoch 172/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2596 - mse: 1603.9049 - val_loss: 25.6599 - val_mse: 1789.5333\n",
      "Epoch 173/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1186 - mse: 1645.4169 - val_loss: 25.6400 - val_mse: 1849.6550\n",
      "Epoch 174/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2006 - mse: 1619.5878 - val_loss: 25.6822 - val_mse: 1843.4968\n",
      "Epoch 175/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2430 - mse: 1644.4781 - val_loss: 25.6305 - val_mse: 1825.9087\n",
      "Epoch 176/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.5082 - mse: 1686.5159 - val_loss: 25.6410 - val_mse: 1850.6399\n",
      "Epoch 177/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1755 - mse: 1594.7808 - val_loss: 25.8374 - val_mse: 1912.3882\n",
      "Epoch 178/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.2266 - mse: 1643.1090 - val_loss: 25.8036 - val_mse: 1901.3992\n",
      "Epoch 179/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.1403 - mse: 1597.8092 - val_loss: 25.5976 - val_mse: 1832.2700\n",
      "Epoch 180/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.4729 - mse: 1668.3461 - val_loss: 25.7727 - val_mse: 1789.8452\n",
      "Epoch 181/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.1285 - mse: 1614.7147 - val_loss: 25.6876 - val_mse: 1806.8445\n",
      "Epoch 182/500\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 24.2728 - mse: 1666.7728 - val_loss: 25.6412 - val_mse: 1820.6232\n",
      "Epoch 183/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9746 - mse: 1581.5546 - val_loss: 25.6410 - val_mse: 1856.2715\n",
      "Epoch 184/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3895 - mse: 1654.4441 - val_loss: 25.6128 - val_mse: 1818.8501\n",
      "Epoch 185/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.0969 - mse: 1614.8398 - val_loss: 25.7353 - val_mse: 1880.6801\n",
      "Epoch 186/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7966 - mse: 1569.6697 - val_loss: 25.6102 - val_mse: 1800.8822\n",
      "Epoch 187/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.6418 - mse: 1677.9056 - val_loss: 25.8148 - val_mse: 1751.2556\n",
      "Epoch 188/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.5055 - mse: 1673.8727 - val_loss: 25.6277 - val_mse: 1836.7241\n",
      "Epoch 189/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1332 - mse: 1619.3763 - val_loss: 25.6161 - val_mse: 1816.4589\n",
      "Epoch 190/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2848 - mse: 1649.2130 - val_loss: 25.7062 - val_mse: 1775.2922\n",
      "Epoch 191/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9240 - mse: 1576.5298 - val_loss: 25.5869 - val_mse: 1805.6738\n",
      "Epoch 192/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4651 - mse: 1634.8966 - val_loss: 25.5784 - val_mse: 1816.0093\n",
      "Epoch 193/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3005 - mse: 1666.0505 - val_loss: 25.6474 - val_mse: 1789.2174\n",
      "Epoch 194/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6918 - mse: 1560.0948 - val_loss: 25.6553 - val_mse: 1800.7885\n",
      "Epoch 195/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.8995 - mse: 1748.4663 - val_loss: 25.6121 - val_mse: 1857.9175\n",
      "Epoch 196/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7937 - mse: 1548.8707 - val_loss: 25.6107 - val_mse: 1837.3074\n",
      "Epoch 197/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3412 - mse: 1622.1288 - val_loss: 25.9999 - val_mse: 1741.6157\n",
      "Epoch 198/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3984 - mse: 1682.0579 - val_loss: 25.6595 - val_mse: 1771.1737\n",
      "Epoch 199/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8267 - mse: 1582.5900 - val_loss: 25.5980 - val_mse: 1810.2621\n",
      "Epoch 200/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.6916 - mse: 1694.8054 - val_loss: 25.6444 - val_mse: 1810.8337\n",
      "Epoch 201/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1751 - mse: 1624.6407 - val_loss: 25.7733 - val_mse: 1900.0891\n",
      "Epoch 202/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9462 - mse: 1568.4742 - val_loss: 25.5705 - val_mse: 1830.3538\n",
      "Epoch 203/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2743 - mse: 1659.9507 - val_loss: 25.6969 - val_mse: 1866.7544\n",
      "Epoch 204/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3274 - mse: 1647.9944 - val_loss: 25.6008 - val_mse: 1817.0393\n",
      "Epoch 205/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0329 - mse: 1600.9106 - val_loss: 25.6290 - val_mse: 1852.2030\n",
      "Epoch 206/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2513 - mse: 1599.2991 - val_loss: 25.6595 - val_mse: 1865.9021\n",
      "Epoch 207/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3631 - mse: 1704.2882 - val_loss: 25.5843 - val_mse: 1797.0211\n",
      "Epoch 208/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0105 - mse: 1597.4619 - val_loss: 25.6729 - val_mse: 1843.8956\n",
      "Epoch 209/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2463 - mse: 1614.3116 - val_loss: 25.6865 - val_mse: 1872.2527\n",
      "Epoch 210/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3957 - mse: 1668.9380 - val_loss: 25.6317 - val_mse: 1831.3655\n",
      "Epoch 211/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1299 - mse: 1618.4432 - val_loss: 25.7357 - val_mse: 1766.9723\n",
      "Epoch 212/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9210 - mse: 1564.7085 - val_loss: 25.6379 - val_mse: 1788.5475\n",
      "Epoch 213/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.5893 - mse: 1692.9058 - val_loss: 25.7331 - val_mse: 1761.1449\n",
      "Epoch 214/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.1633 - mse: 1606.1981 - val_loss: 25.5613 - val_mse: 1806.9835\n",
      "Epoch 215/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1482 - mse: 1618.3566 - val_loss: 25.6509 - val_mse: 1848.3093\n",
      "Epoch 216/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2780 - mse: 1647.8768 - val_loss: 25.5786 - val_mse: 1822.0056\n",
      "Epoch 217/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0630 - mse: 1599.5646 - val_loss: 25.7515 - val_mse: 1893.2241\n",
      "Epoch 218/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0923 - mse: 1628.1785 - val_loss: 25.5824 - val_mse: 1800.9076\n",
      "Epoch 219/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4965 - mse: 1656.4779 - val_loss: 25.6463 - val_mse: 1849.2587\n",
      "Epoch 220/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.0279 - mse: 1589.7416 - val_loss: 25.7721 - val_mse: 1753.7382\n",
      "Epoch 221/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2369 - mse: 1639.7188 - val_loss: 25.6175 - val_mse: 1789.9860\n",
      "Epoch 222/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1348 - mse: 1629.4326 - val_loss: 25.5940 - val_mse: 1843.2904\n",
      "Epoch 223/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2636 - mse: 1628.8063 - val_loss: 25.5676 - val_mse: 1801.1107\n",
      "Epoch 224/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2380 - mse: 1621.3503 - val_loss: 25.6460 - val_mse: 1853.4066\n",
      "Epoch 225/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1774 - mse: 1629.3987 - val_loss: 25.5417 - val_mse: 1818.0789\n",
      "Epoch 226/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1358 - mse: 1616.2626 - val_loss: 25.6595 - val_mse: 1768.3494\n",
      "Epoch 227/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1397 - mse: 1616.5378 - val_loss: 25.5771 - val_mse: 1800.8138\n",
      "Epoch 228/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1682 - mse: 1629.0177 - val_loss: 25.5417 - val_mse: 1827.8982\n",
      "Epoch 229/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2469 - mse: 1635.2356 - val_loss: 25.5995 - val_mse: 1863.4446\n",
      "Epoch 230/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9822 - mse: 1583.4745 - val_loss: 25.7173 - val_mse: 1894.5685\n",
      "Epoch 231/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3010 - mse: 1630.8361 - val_loss: 25.6944 - val_mse: 1881.8512\n",
      "Epoch 232/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3797 - mse: 1650.9347 - val_loss: 25.5454 - val_mse: 1820.7740\n",
      "Epoch 233/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1554 - mse: 1628.2986 - val_loss: 25.6546 - val_mse: 1859.4866\n",
      "Epoch 234/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0869 - mse: 1625.2273 - val_loss: 25.7122 - val_mse: 1898.5295\n",
      "Epoch 235/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2352 - mse: 1633.7808 - val_loss: 25.6033 - val_mse: 1827.4131\n",
      "Epoch 236/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2184 - mse: 1599.3293 - val_loss: 25.8725 - val_mse: 1731.1489\n",
      "Epoch 237/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0851 - mse: 1622.6494 - val_loss: 25.7554 - val_mse: 1755.2582\n",
      "Epoch 238/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2336 - mse: 1615.9570 - val_loss: 25.7276 - val_mse: 1898.5000\n",
      "Epoch 239/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0327 - mse: 1615.3519 - val_loss: 25.6416 - val_mse: 1768.0148\n",
      "Epoch 240/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0887 - mse: 1620.2163 - val_loss: 25.6430 - val_mse: 1761.9692\n",
      "Epoch 241/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2558 - mse: 1641.9956 - val_loss: 25.6091 - val_mse: 1845.5267\n",
      "Epoch 242/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3080 - mse: 1647.3124 - val_loss: 25.5644 - val_mse: 1806.7203\n",
      "Epoch 243/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0946 - mse: 1596.6487 - val_loss: 25.6954 - val_mse: 1894.6354\n",
      "Epoch 244/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3314 - mse: 1647.9902 - val_loss: 25.6091 - val_mse: 1867.3250\n",
      "Epoch 245/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9190 - mse: 1607.0066 - val_loss: 25.5387 - val_mse: 1807.9535\n",
      "Epoch 246/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4929 - mse: 1643.3456 - val_loss: 25.5612 - val_mse: 1848.0682\n",
      "Epoch 247/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8473 - mse: 1594.4109 - val_loss: 25.5255 - val_mse: 1815.4404\n",
      "Epoch 248/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2243 - mse: 1624.4094 - val_loss: 25.6117 - val_mse: 1774.8719\n",
      "Epoch 249/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2456 - mse: 1664.4817 - val_loss: 25.7539 - val_mse: 1767.0857\n",
      "Epoch 250/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1772 - mse: 1606.8031 - val_loss: 25.5732 - val_mse: 1810.1741\n",
      "Epoch 251/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1349 - mse: 1616.1042 - val_loss: 25.5425 - val_mse: 1839.0537\n",
      "Epoch 252/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0570 - mse: 1622.0438 - val_loss: 25.7102 - val_mse: 1765.7505\n",
      "Epoch 253/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.0540 - mse: 1583.9478 - val_loss: 25.6079 - val_mse: 1830.5574\n",
      "Epoch 254/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3690 - mse: 1660.9244 - val_loss: 25.6620 - val_mse: 1878.8391\n",
      "Epoch 255/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7716 - mse: 1574.2897 - val_loss: 25.6818 - val_mse: 1887.6224\n",
      "Epoch 256/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3774 - mse: 1697.1787 - val_loss: 25.5870 - val_mse: 1809.2604\n",
      "Epoch 257/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9226 - mse: 1575.7771 - val_loss: 25.6761 - val_mse: 1777.4475\n",
      "Epoch 258/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1162 - mse: 1592.5425 - val_loss: 25.6814 - val_mse: 1805.0427\n",
      "Epoch 259/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3160 - mse: 1641.2715 - val_loss: 25.5712 - val_mse: 1839.2684\n",
      "Epoch 260/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0009 - mse: 1615.8589 - val_loss: 25.5864 - val_mse: 1788.8035\n",
      "Epoch 261/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2299 - mse: 1607.4242 - val_loss: 25.6174 - val_mse: 1761.2532\n",
      "Epoch 262/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1116 - mse: 1617.6052 - val_loss: 25.5742 - val_mse: 1787.9045\n",
      "Epoch 263/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1792 - mse: 1638.2390 - val_loss: 25.5364 - val_mse: 1798.4465\n",
      "Epoch 264/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1238 - mse: 1627.3353 - val_loss: 25.6897 - val_mse: 1872.9768\n",
      "Epoch 265/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2002 - mse: 1616.5157 - val_loss: 25.5497 - val_mse: 1815.7240\n",
      "Epoch 266/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9806 - mse: 1586.6681 - val_loss: 25.5772 - val_mse: 1846.0433\n",
      "Epoch 267/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3537 - mse: 1644.2936 - val_loss: 25.6164 - val_mse: 1761.0354\n",
      "Epoch 268/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8278 - mse: 1589.8097 - val_loss: 25.5076 - val_mse: 1824.3875\n",
      "Epoch 269/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.4710 - mse: 1669.0796 - val_loss: 25.5500 - val_mse: 1777.4690\n",
      "Epoch 270/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7052 - mse: 1550.5585 - val_loss: 25.5700 - val_mse: 1773.5153\n",
      "Epoch 271/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3212 - mse: 1658.0280 - val_loss: 25.6281 - val_mse: 1869.1042\n",
      "Epoch 272/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1094 - mse: 1609.9358 - val_loss: 25.5400 - val_mse: 1799.3663\n",
      "Epoch 273/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9236 - mse: 1590.1917 - val_loss: 25.5285 - val_mse: 1838.4037\n",
      "Epoch 274/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.5715 - mse: 1696.6008 - val_loss: 25.6199 - val_mse: 1873.5424\n",
      "Epoch 275/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0304 - mse: 1609.4648 - val_loss: 25.5847 - val_mse: 1781.3999\n",
      "Epoch 276/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0121 - mse: 1595.1127 - val_loss: 25.5951 - val_mse: 1806.0381\n",
      "Epoch 277/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1204 - mse: 1626.8345 - val_loss: 25.5553 - val_mse: 1788.6396\n",
      "Epoch 278/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0299 - mse: 1595.4458 - val_loss: 25.6056 - val_mse: 1767.4727\n",
      "Epoch 279/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2726 - mse: 1650.1975 - val_loss: 25.5553 - val_mse: 1853.3149\n",
      "Epoch 280/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2008 - mse: 1635.8350 - val_loss: 25.6559 - val_mse: 1867.2467\n",
      "Epoch 281/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1185 - mse: 1606.0228 - val_loss: 25.6283 - val_mse: 1771.2266\n",
      "Epoch 282/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.2925 - mse: 1642.1897 - val_loss: 25.5493 - val_mse: 1800.6367\n",
      "Epoch 283/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8445 - mse: 1595.1387 - val_loss: 25.7000 - val_mse: 1893.7546\n",
      "Epoch 284/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3568 - mse: 1657.1638 - val_loss: 25.6246 - val_mse: 1767.0321\n",
      "Epoch 285/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7825 - mse: 1560.4860 - val_loss: 25.6371 - val_mse: 1857.8668\n",
      "Epoch 286/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3317 - mse: 1657.5782 - val_loss: 25.4921 - val_mse: 1808.1464\n",
      "Epoch 287/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9533 - mse: 1595.4291 - val_loss: 25.5377 - val_mse: 1780.0406\n",
      "Epoch 288/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0341 - mse: 1601.4720 - val_loss: 25.5250 - val_mse: 1780.3407\n",
      "Epoch 289/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2545 - mse: 1626.3660 - val_loss: 25.5265 - val_mse: 1840.3987\n",
      "Epoch 290/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1141 - mse: 1635.7236 - val_loss: 25.5684 - val_mse: 1847.8208\n",
      "Epoch 291/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1733 - mse: 1608.4407 - val_loss: 25.5622 - val_mse: 1861.3824\n",
      "Epoch 292/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0778 - mse: 1621.1311 - val_loss: 25.5486 - val_mse: 1789.5183\n",
      "Epoch 293/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0653 - mse: 1610.7201 - val_loss: 25.5373 - val_mse: 1793.5673\n",
      "Epoch 294/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1435 - mse: 1629.3335 - val_loss: 25.5156 - val_mse: 1780.1952\n",
      "Epoch 295/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2147 - mse: 1669.6796 - val_loss: 25.5802 - val_mse: 1783.0229\n",
      "Epoch 296/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6933 - mse: 1542.4346 - val_loss: 25.6587 - val_mse: 1757.0106\n",
      "Epoch 297/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.5308 - mse: 1665.2579 - val_loss: 25.5258 - val_mse: 1786.2197\n",
      "Epoch 298/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9358 - mse: 1603.0503 - val_loss: 25.5666 - val_mse: 1820.9586\n",
      "Epoch 299/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1713 - mse: 1624.4526 - val_loss: 25.5331 - val_mse: 1789.5251\n",
      "Epoch 300/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2122 - mse: 1640.2512 - val_loss: 25.6674 - val_mse: 1775.3787\n",
      "Epoch 301/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0465 - mse: 1599.7915 - val_loss: 25.5559 - val_mse: 1818.6582\n",
      "Epoch 302/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0538 - mse: 1599.6993 - val_loss: 25.4959 - val_mse: 1807.7805\n",
      "Epoch 303/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2499 - mse: 1663.3551 - val_loss: 25.5326 - val_mse: 1815.3745\n",
      "Epoch 304/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9991 - mse: 1600.2631 - val_loss: 25.5852 - val_mse: 1764.7933\n",
      "Epoch 305/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9067 - mse: 1560.0521 - val_loss: 25.4978 - val_mse: 1827.3169\n",
      "Epoch 306/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3883 - mse: 1682.8518 - val_loss: 25.6262 - val_mse: 1786.9307\n",
      "Epoch 307/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9591 - mse: 1584.6262 - val_loss: 25.8458 - val_mse: 1741.7405\n",
      "Epoch 308/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0348 - mse: 1598.5719 - val_loss: 25.6557 - val_mse: 1809.3998\n",
      "Epoch 309/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.3406 - mse: 1659.8641 - val_loss: 25.4892 - val_mse: 1831.6250\n",
      "Epoch 310/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6902 - mse: 1566.6974 - val_loss: 25.4995 - val_mse: 1834.7565\n",
      "Epoch 311/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2679 - mse: 1646.5083 - val_loss: 25.4964 - val_mse: 1824.6628\n",
      "Epoch 312/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0850 - mse: 1582.6198 - val_loss: 25.6098 - val_mse: 1829.2821\n",
      "Epoch 313/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8143 - mse: 1599.2852 - val_loss: 25.5806 - val_mse: 1805.5210\n",
      "Epoch 314/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1173 - mse: 1616.0364 - val_loss: 25.4891 - val_mse: 1801.1864\n",
      "Epoch 315/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0600 - mse: 1609.5597 - val_loss: 25.7409 - val_mse: 1745.8450\n",
      "Epoch 316/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2198 - mse: 1635.4614 - val_loss: 25.5436 - val_mse: 1787.6968\n",
      "Epoch 317/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2570 - mse: 1650.6664 - val_loss: 25.7738 - val_mse: 1735.0481\n",
      "Epoch 318/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0979 - mse: 1621.5461 - val_loss: 25.4891 - val_mse: 1814.8179\n",
      "Epoch 319/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0290 - mse: 1608.3082 - val_loss: 25.5527 - val_mse: 1845.1702\n",
      "Epoch 320/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1212 - mse: 1591.8361 - val_loss: 25.5156 - val_mse: 1837.9413\n",
      "Epoch 321/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8625 - mse: 1575.4822 - val_loss: 25.5861 - val_mse: 1848.7239\n",
      "Epoch 322/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3096 - mse: 1658.2820 - val_loss: 25.6545 - val_mse: 1784.6597\n",
      "Epoch 323/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9014 - mse: 1582.2954 - val_loss: 25.5025 - val_mse: 1825.4427\n",
      "Epoch 324/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0277 - mse: 1598.5658 - val_loss: 25.8848 - val_mse: 1732.7319\n",
      "Epoch 325/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1670 - mse: 1625.6022 - val_loss: 25.5743 - val_mse: 1769.6038\n",
      "Epoch 326/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0850 - mse: 1635.6925 - val_loss: 25.6250 - val_mse: 1879.5647\n",
      "Epoch 327/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.0335 - mse: 1593.2974 - val_loss: 25.7113 - val_mse: 1904.0651\n",
      "Epoch 328/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1681 - mse: 1627.9501 - val_loss: 25.4785 - val_mse: 1821.3220\n",
      "Epoch 329/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9990 - mse: 1606.1587 - val_loss: 25.5353 - val_mse: 1841.9927\n",
      "Epoch 330/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9874 - mse: 1613.5632 - val_loss: 25.5788 - val_mse: 1760.9062\n",
      "Epoch 331/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2090 - mse: 1626.6063 - val_loss: 25.7605 - val_mse: 1737.5658\n",
      "Epoch 332/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0104 - mse: 1602.1233 - val_loss: 25.5492 - val_mse: 1805.3685\n",
      "Epoch 333/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0405 - mse: 1616.0834 - val_loss: 25.5424 - val_mse: 1799.4541\n",
      "Epoch 334/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0384 - mse: 1607.8135 - val_loss: 25.5897 - val_mse: 1769.6194\n",
      "Epoch 335/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0386 - mse: 1604.8553 - val_loss: 25.7493 - val_mse: 1913.3535\n",
      "Epoch 336/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0444 - mse: 1609.4442 - val_loss: 25.4929 - val_mse: 1819.2639\n",
      "Epoch 337/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0548 - mse: 1610.1873 - val_loss: 25.4836 - val_mse: 1818.5408\n",
      "Epoch 338/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.0928 - mse: 1622.9650 - val_loss: 25.8293 - val_mse: 1723.1023\n",
      "Epoch 339/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.1218 - mse: 1625.6775 - val_loss: 25.6572 - val_mse: 1776.5940\n",
      "Epoch 340/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0272 - mse: 1595.2354 - val_loss: 25.6318 - val_mse: 1841.2913\n",
      "Epoch 341/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2085 - mse: 1637.5723 - val_loss: 25.6119 - val_mse: 1755.2465\n",
      "Epoch 342/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9816 - mse: 1596.0780 - val_loss: 25.5849 - val_mse: 1770.9017\n",
      "Epoch 343/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0001 - mse: 1616.1985 - val_loss: 25.5727 - val_mse: 1769.1167\n",
      "Epoch 344/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0535 - mse: 1615.3199 - val_loss: 25.5798 - val_mse: 1856.7825\n",
      "Epoch 345/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0242 - mse: 1605.0646 - val_loss: 25.5611 - val_mse: 1777.0166\n",
      "Epoch 346/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0954 - mse: 1631.4835 - val_loss: 25.5124 - val_mse: 1829.8544\n",
      "Epoch 347/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1566 - mse: 1621.9695 - val_loss: 25.5628 - val_mse: 1844.0166\n",
      "Epoch 348/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0133 - mse: 1589.9587 - val_loss: 25.5884 - val_mse: 1788.8392\n",
      "Epoch 349/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0869 - mse: 1615.4232 - val_loss: 25.6631 - val_mse: 1788.9133\n",
      "Epoch 350/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0544 - mse: 1631.0939 - val_loss: 25.4930 - val_mse: 1838.3204\n",
      "Epoch 351/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.9265 - mse: 1594.5353 - val_loss: 25.5168 - val_mse: 1772.3698\n",
      "Epoch 352/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1041 - mse: 1633.7809 - val_loss: 25.6461 - val_mse: 1751.6727\n",
      "Epoch 353/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9232 - mse: 1580.5840 - val_loss: 25.5586 - val_mse: 1843.6627\n",
      "Epoch 354/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.0105 - mse: 1609.3145 - val_loss: 25.5615 - val_mse: 1757.0234\n",
      "Epoch 355/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3726 - mse: 1657.5138 - val_loss: 25.4595 - val_mse: 1787.9510\n",
      "Epoch 356/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9783 - mse: 1598.2909 - val_loss: 25.5350 - val_mse: 1801.8029\n",
      "Epoch 357/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1162 - mse: 1623.4789 - val_loss: 25.4777 - val_mse: 1817.5980\n",
      "Epoch 358/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8735 - mse: 1587.4457 - val_loss: 25.4860 - val_mse: 1804.9050\n",
      "Epoch 359/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3139 - mse: 1669.4437 - val_loss: 25.7518 - val_mse: 1729.8225\n",
      "Epoch 360/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5347 - mse: 1522.4320 - val_loss: 25.4750 - val_mse: 1783.9688\n",
      "Epoch 361/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2297 - mse: 1634.8789 - val_loss: 25.4710 - val_mse: 1809.3438\n",
      "Epoch 362/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9984 - mse: 1616.8024 - val_loss: 25.4736 - val_mse: 1786.8855\n",
      "Epoch 363/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1675 - mse: 1634.1274 - val_loss: 25.5208 - val_mse: 1839.1029\n",
      "Epoch 364/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9158 - mse: 1594.1727 - val_loss: 25.5668 - val_mse: 1800.7258\n",
      "Epoch 365/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2826 - mse: 1652.7415 - val_loss: 25.7729 - val_mse: 1753.2982\n",
      "Epoch 366/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1790 - mse: 1580.7268 - val_loss: 25.4503 - val_mse: 1809.5171\n",
      "Epoch 367/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7091 - mse: 1590.9122 - val_loss: 25.5706 - val_mse: 1763.4297\n",
      "Epoch 368/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0714 - mse: 1614.8794 - val_loss: 25.4886 - val_mse: 1779.5205\n",
      "Epoch 369/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2348 - mse: 1633.9125 - val_loss: 25.5588 - val_mse: 1783.6804\n",
      "Epoch 370/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0803 - mse: 1634.4095 - val_loss: 25.5957 - val_mse: 1820.7854\n",
      "Epoch 371/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9810 - mse: 1620.7358 - val_loss: 25.4900 - val_mse: 1812.7406\n",
      "Epoch 372/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0632 - mse: 1590.1593 - val_loss: 25.4769 - val_mse: 1825.3566\n",
      "Epoch 373/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2473 - mse: 1632.6454 - val_loss: 25.4515 - val_mse: 1802.9592\n",
      "Epoch 374/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.0799 - mse: 1632.4076 - val_loss: 25.5475 - val_mse: 1860.2590\n",
      "Epoch 375/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1204 - mse: 1619.9926 - val_loss: 25.5836 - val_mse: 1875.3054\n",
      "Epoch 376/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.5824 - mse: 1530.1796 - val_loss: 25.5898 - val_mse: 1852.2401\n",
      "Epoch 377/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.6152 - mse: 1709.7571 - val_loss: 25.5143 - val_mse: 1774.6357\n",
      "Epoch 378/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6596 - mse: 1568.1912 - val_loss: 25.5240 - val_mse: 1789.4030\n",
      "Epoch 379/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1988 - mse: 1635.5426 - val_loss: 25.5907 - val_mse: 1759.9912\n",
      "Epoch 380/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8474 - mse: 1574.5708 - val_loss: 25.4858 - val_mse: 1815.4175\n",
      "Epoch 381/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.2141 - mse: 1642.9550 - val_loss: 25.4795 - val_mse: 1777.8690\n",
      "Epoch 382/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7932 - mse: 1567.9325 - val_loss: 25.6349 - val_mse: 1881.2242\n",
      "Epoch 383/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.4209 - mse: 1637.1516 - val_loss: 25.4612 - val_mse: 1787.6965\n",
      "Epoch 384/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.4467 - mse: 1546.6189 - val_loss: 25.4792 - val_mse: 1797.6829\n",
      "Epoch 385/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1869 - mse: 1626.7810 - val_loss: 25.5764 - val_mse: 1815.3232\n",
      "Epoch 386/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.2801 - mse: 1698.5416 - val_loss: 25.4514 - val_mse: 1801.1503\n",
      "Epoch 387/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7855 - mse: 1551.1813 - val_loss: 25.5152 - val_mse: 1782.7550\n",
      "Epoch 388/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4062 - mse: 1675.4417 - val_loss: 25.4706 - val_mse: 1789.7305\n",
      "Epoch 389/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1506 - mse: 1600.6963 - val_loss: 25.9608 - val_mse: 1732.9718\n",
      "Epoch 390/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.7233 - mse: 1559.3191 - val_loss: 25.4929 - val_mse: 1836.2432\n",
      "Epoch 391/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0142 - mse: 1623.0607 - val_loss: 25.5253 - val_mse: 1850.8759\n",
      "Epoch 392/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3943 - mse: 1670.0725 - val_loss: 25.4921 - val_mse: 1837.0328\n",
      "Epoch 393/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5114 - mse: 1514.0044 - val_loss: 25.5259 - val_mse: 1772.3888\n",
      "Epoch 394/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7911 - mse: 1573.7369 - val_loss: 25.4883 - val_mse: 1823.6255\n",
      "Epoch 395/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2351 - mse: 1634.3738 - val_loss: 25.4658 - val_mse: 1779.4825\n",
      "Epoch 396/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3482 - mse: 1678.9823 - val_loss: 25.5049 - val_mse: 1810.8811\n",
      "Epoch 397/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5837 - mse: 1552.3435 - val_loss: 25.5191 - val_mse: 1852.3660\n",
      "Epoch 398/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8819 - mse: 1552.0140 - val_loss: 25.4908 - val_mse: 1815.7761\n",
      "Epoch 399/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3851 - mse: 1678.9379 - val_loss: 25.5845 - val_mse: 1877.4890\n",
      "Epoch 400/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8659 - mse: 1593.3248 - val_loss: 25.5033 - val_mse: 1771.5098\n",
      "Epoch 401/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9418 - mse: 1624.8381 - val_loss: 25.5800 - val_mse: 1758.1019\n",
      "Epoch 402/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0377 - mse: 1596.4280 - val_loss: 25.4656 - val_mse: 1793.4718\n",
      "Epoch 403/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9739 - mse: 1611.4741 - val_loss: 25.5099 - val_mse: 1797.5502\n",
      "Epoch 404/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0772 - mse: 1616.0745 - val_loss: 25.6852 - val_mse: 1780.0422\n",
      "Epoch 405/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2387 - mse: 1618.8630 - val_loss: 25.5303 - val_mse: 1812.1797\n",
      "Epoch 406/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8917 - mse: 1606.8265 - val_loss: 25.5273 - val_mse: 1828.7026\n",
      "Epoch 407/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7734 - mse: 1585.8779 - val_loss: 25.6246 - val_mse: 1742.1000\n",
      "Epoch 408/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1984 - mse: 1618.7374 - val_loss: 25.4466 - val_mse: 1821.6145\n",
      "Epoch 409/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8281 - mse: 1588.4982 - val_loss: 25.6726 - val_mse: 1805.9337\n",
      "Epoch 410/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9030 - mse: 1599.0166 - val_loss: 25.5754 - val_mse: 1770.4417\n",
      "Epoch 411/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1758 - mse: 1635.7415 - val_loss: 25.4596 - val_mse: 1823.4659\n",
      "Epoch 412/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7559 - mse: 1544.8082 - val_loss: 25.4723 - val_mse: 1826.5702\n",
      "Epoch 413/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2304 - mse: 1659.4042 - val_loss: 25.5123 - val_mse: 1835.5459\n",
      "Epoch 414/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2612 - mse: 1653.4906 - val_loss: 25.6591 - val_mse: 1875.6819\n",
      "Epoch 415/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.9303 - mse: 1593.3649 - val_loss: 25.5133 - val_mse: 1838.6357\n",
      "Epoch 416/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8670 - mse: 1564.3547 - val_loss: 25.4618 - val_mse: 1792.7620\n",
      "Epoch 417/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8500 - mse: 1578.5409 - val_loss: 25.6108 - val_mse: 1787.5260\n",
      "Epoch 418/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8488 - mse: 1618.8550 - val_loss: 25.4481 - val_mse: 1788.7349\n",
      "Epoch 419/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0450 - mse: 1600.4713 - val_loss: 25.7876 - val_mse: 1733.4933\n",
      "Epoch 420/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2085 - mse: 1628.0369 - val_loss: 25.4818 - val_mse: 1816.3473\n",
      "Epoch 421/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0712 - mse: 1630.3019 - val_loss: 25.5146 - val_mse: 1757.6881\n",
      "Epoch 422/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9266 - mse: 1608.3168 - val_loss: 25.6867 - val_mse: 1896.0194\n",
      "Epoch 423/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0916 - mse: 1603.9164 - val_loss: 25.5328 - val_mse: 1845.4539\n",
      "Epoch 424/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8400 - mse: 1555.2600 - val_loss: 25.4413 - val_mse: 1792.3844\n",
      "Epoch 425/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1180 - mse: 1669.3422 - val_loss: 25.4578 - val_mse: 1814.2737\n",
      "Epoch 426/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0484 - mse: 1610.4407 - val_loss: 25.7829 - val_mse: 1919.2198\n",
      "Epoch 427/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.8532 - mse: 1566.6782 - val_loss: 25.5875 - val_mse: 1750.3373\n",
      "Epoch 428/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2345 - mse: 1662.8500 - val_loss: 25.5511 - val_mse: 1781.1914\n",
      "Epoch 429/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6665 - mse: 1557.9169 - val_loss: 25.4351 - val_mse: 1808.0093\n",
      "Epoch 430/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1127 - mse: 1607.8855 - val_loss: 25.4798 - val_mse: 1831.6888\n",
      "Epoch 431/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7786 - mse: 1587.0575 - val_loss: 25.4836 - val_mse: 1831.4708\n",
      "Epoch 432/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1374 - mse: 1632.1965 - val_loss: 25.5300 - val_mse: 1826.6305\n",
      "Epoch 433/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0817 - mse: 1650.2643 - val_loss: 25.4629 - val_mse: 1808.3915\n",
      "Epoch 434/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.8846 - mse: 1584.8826 - val_loss: 25.5972 - val_mse: 1744.0172\n",
      "Epoch 435/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.9082 - mse: 1569.5869 - val_loss: 25.5437 - val_mse: 1866.5374\n",
      "Epoch 436/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.9076 - mse: 1601.8202 - val_loss: 25.5083 - val_mse: 1777.6754\n",
      "Epoch 437/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.1593 - mse: 1643.7820 - val_loss: 25.4691 - val_mse: 1822.0615\n",
      "Epoch 438/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.0130 - mse: 1610.6702 - val_loss: 25.4448 - val_mse: 1800.8448\n",
      "Epoch 439/500\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 23.8687 - mse: 1589.6608 - val_loss: 25.6287 - val_mse: 1816.0120\n",
      "Epoch 440/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.0345 - mse: 1605.7239 - val_loss: 25.5429 - val_mse: 1754.6370\n",
      "Epoch 441/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.0672 - mse: 1630.7074 - val_loss: 25.6474 - val_mse: 1869.7882\n",
      "Epoch 442/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0606 - mse: 1617.9576 - val_loss: 25.5536 - val_mse: 1871.5259\n",
      "Epoch 443/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9078 - mse: 1596.6094 - val_loss: 25.5209 - val_mse: 1836.7081\n",
      "Epoch 444/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8495 - mse: 1590.9528 - val_loss: 25.5061 - val_mse: 1832.2605\n",
      "Epoch 445/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.8554 - mse: 1579.9667 - val_loss: 25.4349 - val_mse: 1818.8369\n",
      "Epoch 446/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.2008 - mse: 1631.3162 - val_loss: 25.5079 - val_mse: 1772.2301\n",
      "Epoch 447/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.0255 - mse: 1617.2203 - val_loss: 25.5853 - val_mse: 1745.9934\n",
      "Epoch 448/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9009 - mse: 1603.5469 - val_loss: 25.5031 - val_mse: 1806.0659\n",
      "Epoch 449/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.9243 - mse: 1601.5455 - val_loss: 25.4624 - val_mse: 1790.2991\n",
      "Epoch 450/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.9693 - mse: 1606.4227 - val_loss: 25.4694 - val_mse: 1771.9752\n",
      "Epoch 451/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.9673 - mse: 1606.1555 - val_loss: 25.5741 - val_mse: 1782.5420\n",
      "Epoch 452/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0334 - mse: 1615.7037 - val_loss: 25.4928 - val_mse: 1768.6471\n",
      "Epoch 453/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0145 - mse: 1618.0748 - val_loss: 25.5250 - val_mse: 1789.4825\n",
      "Epoch 454/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.8432 - mse: 1582.2551 - val_loss: 25.5539 - val_mse: 1754.1401\n",
      "Epoch 455/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0937 - mse: 1628.5634 - val_loss: 25.4747 - val_mse: 1803.6062\n",
      "Epoch 456/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7041 - mse: 1552.5461 - val_loss: 25.6143 - val_mse: 1767.6307\n",
      "Epoch 457/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3244 - mse: 1666.1996 - val_loss: 25.4530 - val_mse: 1827.4166\n",
      "Epoch 458/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8749 - mse: 1597.6318 - val_loss: 25.4780 - val_mse: 1781.0604\n",
      "Epoch 459/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0439 - mse: 1602.4906 - val_loss: 25.6209 - val_mse: 1755.5829\n",
      "Epoch 460/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9057 - mse: 1594.9370 - val_loss: 25.5813 - val_mse: 1748.3176\n",
      "Epoch 461/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0439 - mse: 1600.7782 - val_loss: 25.7717 - val_mse: 1843.8669\n",
      "Epoch 462/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.1143 - mse: 1630.8593 - val_loss: 25.9496 - val_mse: 1712.6930\n",
      "Epoch 463/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7599 - mse: 1580.9325 - val_loss: 25.4543 - val_mse: 1834.1167\n",
      "Epoch 464/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1163 - mse: 1614.4674 - val_loss: 25.6085 - val_mse: 1885.9399\n",
      "Epoch 465/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0739 - mse: 1639.1057 - val_loss: 25.4550 - val_mse: 1814.7535\n",
      "Epoch 466/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7292 - mse: 1552.2247 - val_loss: 25.4442 - val_mse: 1806.5515\n",
      "Epoch 467/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2095 - mse: 1641.6581 - val_loss: 25.4617 - val_mse: 1816.5508\n",
      "Epoch 468/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8142 - mse: 1598.8525 - val_loss: 25.4629 - val_mse: 1777.4132\n",
      "Epoch 469/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0542 - mse: 1613.7926 - val_loss: 25.5669 - val_mse: 1755.7482\n",
      "Epoch 470/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8460 - mse: 1574.7701 - val_loss: 25.5840 - val_mse: 1854.1069\n",
      "Epoch 471/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.9720 - mse: 1602.5819 - val_loss: 25.4419 - val_mse: 1807.1182\n",
      "Epoch 472/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.0955 - mse: 1637.9631 - val_loss: 25.7567 - val_mse: 1736.3535\n",
      "Epoch 473/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.7895 - mse: 1572.9463 - val_loss: 25.4753 - val_mse: 1811.5942\n",
      "Epoch 474/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9821 - mse: 1612.8330 - val_loss: 25.4254 - val_mse: 1787.9590\n",
      "Epoch 475/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1147 - mse: 1633.2365 - val_loss: 25.6332 - val_mse: 1757.7888\n",
      "Epoch 476/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8228 - mse: 1594.2147 - val_loss: 25.5589 - val_mse: 1767.8348\n",
      "Epoch 477/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0727 - mse: 1641.3358 - val_loss: 25.5224 - val_mse: 1751.7593\n",
      "Epoch 478/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5991 - mse: 1539.3398 - val_loss: 25.4559 - val_mse: 1798.6940\n",
      "Epoch 479/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2874 - mse: 1651.1562 - val_loss: 25.4830 - val_mse: 1772.0457\n",
      "Epoch 480/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7587 - mse: 1571.9656 - val_loss: 25.5193 - val_mse: 1779.8398\n",
      "Epoch 481/500\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 24.1473 - mse: 1644.2338 - val_loss: 25.4586 - val_mse: 1829.1099\n",
      "Epoch 482/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3808 - mse: 1499.0619 - val_loss: 25.4447 - val_mse: 1817.6733\n",
      "Epoch 483/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3914 - mse: 1699.2170 - val_loss: 25.4683 - val_mse: 1771.0276\n",
      "Epoch 484/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6663 - mse: 1573.3970 - val_loss: 25.5159 - val_mse: 1832.8042\n",
      "Epoch 485/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9021 - mse: 1573.8722 - val_loss: 25.5193 - val_mse: 1772.4803\n",
      "Epoch 486/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.0137 - mse: 1609.4777 - val_loss: 25.4564 - val_mse: 1793.4294\n",
      "Epoch 487/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2543 - mse: 1661.6890 - val_loss: 25.6268 - val_mse: 1783.5648\n",
      "Epoch 488/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7646 - mse: 1564.3290 - val_loss: 25.5043 - val_mse: 1757.3982\n",
      "Epoch 489/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.8321 - mse: 1611.5000 - val_loss: 25.4398 - val_mse: 1801.2679\n",
      "Epoch 490/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.9315 - mse: 1609.9260 - val_loss: 25.5058 - val_mse: 1769.7058\n",
      "Epoch 491/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.5021 - mse: 1683.8124 - val_loss: 25.4376 - val_mse: 1774.5498\n",
      "Epoch 492/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3807 - mse: 1496.2628 - val_loss: 25.4594 - val_mse: 1796.2092\n",
      "Epoch 493/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8919 - mse: 1607.2524 - val_loss: 25.5217 - val_mse: 1854.4801\n",
      "Epoch 494/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.0430 - mse: 1629.6136 - val_loss: 25.5637 - val_mse: 1780.6302\n",
      "Epoch 495/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.9494 - mse: 1593.3734 - val_loss: 25.4429 - val_mse: 1777.0837\n",
      "Epoch 496/500\n",
      " 38/113 [=========>....................] - ETA: 0s - loss: 23.4826 - mse: 1529.4672WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 56500 batches). You may need to use the repeat() function when building your dataset.\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.7118 - mse: 1585.8916 - val_loss: 25.4798 - val_mse: 1781.5857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 20:23:50.191374: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_temp0\n",
      "[40.26, 40.22]\n",
      "Epoch 1/500\n",
      "107/113 [===========================>..] - ETA: 0s - loss: 54.0911 - mse: 4576.6211"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 20:23:51.059896: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 0s 3ms/step - loss: 53.9074 - mse: 4565.0161 - val_loss: 49.2114 - val_mse: 4132.3994\n",
      "Epoch 2/500\n",
      " 38/113 [=========>....................] - ETA: 0s - loss: 47.4719 - mse: 3943.4548"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 20:23:51.297833: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 0s 2ms/step - loss: 44.9679 - mse: 3563.6531 - val_loss: 43.8948 - val_mse: 3425.1711\n",
      "Epoch 3/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 43.0440 - mse: 3329.3931 - val_loss: 43.0145 - val_mse: 3316.8738\n",
      "Epoch 4/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 41.9301 - mse: 3200.1743 - val_loss: 42.1306 - val_mse: 3227.1846\n",
      "Epoch 5/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 40.5895 - mse: 3067.0771 - val_loss: 40.8316 - val_mse: 3099.0923\n",
      "Epoch 6/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 39.1828 - mse: 2956.1189 - val_loss: 39.1322 - val_mse: 2944.0996\n",
      "Epoch 7/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 36.8777 - mse: 2696.5647 - val_loss: 37.0249 - val_mse: 2762.4460\n",
      "Epoch 8/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 34.4484 - mse: 2519.5010 - val_loss: 34.4902 - val_mse: 2564.7925\n",
      "Epoch 9/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 31.9995 - mse: 2333.3555 - val_loss: 31.7533 - val_mse: 2348.7737\n",
      "Epoch 10/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 29.2572 - mse: 2123.1370 - val_loss: 29.4113 - val_mse: 2155.0164\n",
      "Epoch 11/500\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 26.6015 - mse: 1858.8956 - val_loss: 27.8635 - val_mse: 2053.6438\n",
      "Epoch 12/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 26.2302 - mse: 1890.4863 - val_loss: 27.0141 - val_mse: 2010.9120\n",
      "Epoch 13/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2835 - mse: 1769.1909 - val_loss: 26.7178 - val_mse: 1963.4423\n",
      "Epoch 14/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4489 - mse: 1825.8923 - val_loss: 26.6195 - val_mse: 1969.1268\n",
      "Epoch 15/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.7878 - mse: 1693.5928 - val_loss: 26.4912 - val_mse: 1954.0052\n",
      "Epoch 16/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.8956 - mse: 1736.2181 - val_loss: 26.4219 - val_mse: 1934.1268\n",
      "Epoch 17/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4086 - mse: 1832.5580 - val_loss: 26.4356 - val_mse: 1932.3643\n",
      "Epoch 18/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.9886 - mse: 1760.7407 - val_loss: 26.4683 - val_mse: 1965.7178\n",
      "Epoch 19/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.5472 - mse: 1664.3595 - val_loss: 26.3985 - val_mse: 1939.1353\n",
      "Epoch 20/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.9171 - mse: 1758.4033 - val_loss: 26.4736 - val_mse: 1941.6160\n",
      "Epoch 21/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.8084 - mse: 1713.5951 - val_loss: 26.4279 - val_mse: 1952.4039\n",
      "Epoch 22/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.6826 - mse: 1734.8699 - val_loss: 26.3966 - val_mse: 1920.8741\n",
      "Epoch 23/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.7891 - mse: 1726.5170 - val_loss: 26.4550 - val_mse: 1951.4583\n",
      "Epoch 24/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.7769 - mse: 1731.9199 - val_loss: 26.4259 - val_mse: 1939.4462\n",
      "Epoch 25/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4450 - mse: 1679.1976 - val_loss: 26.3746 - val_mse: 1944.6738\n",
      "Epoch 26/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.8494 - mse: 1781.9242 - val_loss: 26.4110 - val_mse: 1930.3107\n",
      "Epoch 27/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.3812 - mse: 1683.7986 - val_loss: 26.4029 - val_mse: 1942.8243\n",
      "Epoch 28/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.5464 - mse: 1695.6600 - val_loss: 26.3963 - val_mse: 1939.7460\n",
      "Epoch 29/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3898 - mse: 1697.1729 - val_loss: 26.3992 - val_mse: 1953.3037\n",
      "Epoch 30/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.8155 - mse: 1755.4448 - val_loss: 26.4304 - val_mse: 1961.5587\n",
      "Epoch 31/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.5023 - mse: 1731.3223 - val_loss: 26.4053 - val_mse: 1945.6293\n",
      "Epoch 32/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4951 - mse: 1690.3005 - val_loss: 26.3287 - val_mse: 1938.3368\n",
      "Epoch 33/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.5965 - mse: 1733.4209 - val_loss: 26.4512 - val_mse: 1927.9877\n",
      "Epoch 34/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2553 - mse: 1657.1838 - val_loss: 26.3628 - val_mse: 1938.4768\n",
      "Epoch 35/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3843 - mse: 1717.4530 - val_loss: 26.3782 - val_mse: 1937.2058\n",
      "Epoch 36/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.5194 - mse: 1728.3221 - val_loss: 26.3733 - val_mse: 1942.5940\n",
      "Epoch 37/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.6711 - mse: 1715.5891 - val_loss: 26.3700 - val_mse: 1929.9862\n",
      "Epoch 38/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.5127 - mse: 1735.1969 - val_loss: 26.3358 - val_mse: 1940.6389\n",
      "Epoch 39/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2900 - mse: 1719.7177 - val_loss: 26.3696 - val_mse: 1941.4637\n",
      "Epoch 40/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0489 - mse: 1646.3842 - val_loss: 26.3606 - val_mse: 1932.9371\n",
      "Epoch 41/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2312 - mse: 1672.4275 - val_loss: 26.4084 - val_mse: 1963.7368\n",
      "Epoch 42/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2265 - mse: 1672.4802 - val_loss: 26.3702 - val_mse: 1934.8861\n",
      "Epoch 43/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4685 - mse: 1739.8328 - val_loss: 26.3497 - val_mse: 1946.0376\n",
      "Epoch 44/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.9823 - mse: 1660.9786 - val_loss: 26.3627 - val_mse: 1949.6525\n",
      "Epoch 45/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.5627 - mse: 1693.5457 - val_loss: 26.3877 - val_mse: 1933.0587\n",
      "Epoch 46/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4202 - mse: 1746.8103 - val_loss: 26.4425 - val_mse: 1936.8376\n",
      "Epoch 47/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2967 - mse: 1710.9548 - val_loss: 26.3675 - val_mse: 1948.2256\n",
      "Epoch 48/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1666 - mse: 1682.4233 - val_loss: 26.3762 - val_mse: 1949.2065\n",
      "Epoch 49/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4647 - mse: 1554.5851 - val_loss: 26.4078 - val_mse: 1949.4984\n",
      "Epoch 50/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4769 - mse: 1734.9691 - val_loss: 26.4012 - val_mse: 1946.1963\n",
      "Epoch 51/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1594 - mse: 1702.4884 - val_loss: 26.3742 - val_mse: 1941.1520\n",
      "Epoch 52/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1413 - mse: 1675.6445 - val_loss: 26.4036 - val_mse: 1940.1914\n",
      "Epoch 53/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4487 - mse: 1736.6449 - val_loss: 26.3753 - val_mse: 1927.1279\n",
      "Epoch 54/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7232 - mse: 1610.7504 - val_loss: 26.4197 - val_mse: 1937.8749\n",
      "Epoch 55/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0851 - mse: 1640.0999 - val_loss: 26.4600 - val_mse: 1958.3315\n",
      "Epoch 56/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2133 - mse: 1748.9701 - val_loss: 26.4181 - val_mse: 1936.9028\n",
      "Epoch 57/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4973 - mse: 1717.5609 - val_loss: 26.4248 - val_mse: 1940.7891\n",
      "Epoch 58/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9774 - mse: 1651.4921 - val_loss: 26.4563 - val_mse: 1942.7872\n",
      "Epoch 59/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8778 - mse: 1681.8364 - val_loss: 26.4694 - val_mse: 1945.0995\n",
      "Epoch 60/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9245 - mse: 1619.1177 - val_loss: 26.3714 - val_mse: 1944.7861\n",
      "Epoch 61/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1372 - mse: 1689.6631 - val_loss: 26.4682 - val_mse: 1958.1167\n",
      "Epoch 62/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9834 - mse: 1647.5681 - val_loss: 26.4570 - val_mse: 1942.7261\n",
      "Epoch 63/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2956 - mse: 1748.4281 - val_loss: 26.4393 - val_mse: 1936.9209\n",
      "Epoch 64/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.7678 - mse: 1619.4658 - val_loss: 26.4424 - val_mse: 1936.3728\n",
      "Epoch 65/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.1625 - mse: 1690.8905 - val_loss: 26.4548 - val_mse: 1945.4126\n",
      "Epoch 66/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8364 - mse: 1646.1682 - val_loss: 26.5082 - val_mse: 1946.1002\n",
      "Epoch 67/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4858 - mse: 1763.7246 - val_loss: 26.4999 - val_mse: 1936.8081\n",
      "Epoch 68/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2611 - mse: 1517.0734 - val_loss: 26.4329 - val_mse: 1931.5844\n",
      "Epoch 69/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9336 - mse: 1666.5399 - val_loss: 26.4726 - val_mse: 1954.5498\n",
      "Epoch 70/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1771 - mse: 1713.6222 - val_loss: 26.4886 - val_mse: 1942.4595\n",
      "Epoch 71/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1566 - mse: 1670.9686 - val_loss: 26.4710 - val_mse: 1928.8016\n",
      "Epoch 72/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0459 - mse: 1663.3717 - val_loss: 26.5150 - val_mse: 1945.7063\n",
      "Epoch 73/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.8613 - mse: 1657.6508 - val_loss: 26.4609 - val_mse: 1943.8109\n",
      "Epoch 74/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9543 - mse: 1689.2632 - val_loss: 26.5429 - val_mse: 1950.9305\n",
      "Epoch 75/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1230 - mse: 1690.7048 - val_loss: 26.4869 - val_mse: 1938.1825\n",
      "Epoch 76/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7378 - mse: 1633.6884 - val_loss: 26.4440 - val_mse: 1952.0217\n",
      "Epoch 77/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9314 - mse: 1673.1633 - val_loss: 26.4742 - val_mse: 1934.1112\n",
      "Epoch 78/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8837 - mse: 1629.3861 - val_loss: 26.4807 - val_mse: 1927.1326\n",
      "Epoch 79/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8231 - mse: 1673.2665 - val_loss: 26.4492 - val_mse: 1944.8668\n",
      "Epoch 80/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2852 - mse: 1710.6196 - val_loss: 26.5005 - val_mse: 1934.8441\n",
      "Epoch 81/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5637 - mse: 1596.8990 - val_loss: 26.5075 - val_mse: 1954.6399\n",
      "Epoch 82/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.8304 - mse: 1640.0159 - val_loss: 26.5358 - val_mse: 1943.3219\n",
      "Epoch 83/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.0632 - mse: 1703.4331 - val_loss: 26.5518 - val_mse: 1955.8478\n",
      "Epoch 84/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.7041 - mse: 1629.3589 - val_loss: 26.5405 - val_mse: 1949.1823\n",
      "Epoch 85/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.9364 - mse: 1666.7986 - val_loss: 26.5292 - val_mse: 1951.8177\n",
      "Epoch 86/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.7809 - mse: 1631.8411 - val_loss: 26.5450 - val_mse: 1941.3279\n",
      "Epoch 87/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7789 - mse: 1645.1680 - val_loss: 26.5385 - val_mse: 1942.1031\n",
      "Epoch 88/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.0181 - mse: 1686.9352 - val_loss: 26.5886 - val_mse: 1951.5535\n",
      "Epoch 89/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7487 - mse: 1650.8423 - val_loss: 26.5483 - val_mse: 1943.5914\n",
      "Epoch 90/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9515 - mse: 1659.0791 - val_loss: 26.5523 - val_mse: 1934.1091\n",
      "Epoch 91/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.8540 - mse: 1660.8643 - val_loss: 26.5627 - val_mse: 1947.6697\n",
      "Epoch 92/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6342 - mse: 1614.1445 - val_loss: 26.5339 - val_mse: 1941.9585\n",
      "Epoch 93/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5012 - mse: 1588.8036 - val_loss: 26.5644 - val_mse: 1950.2321\n",
      "Epoch 94/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9585 - mse: 1667.3798 - val_loss: 26.5582 - val_mse: 1961.2499\n",
      "Epoch 95/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8995 - mse: 1677.6428 - val_loss: 26.5629 - val_mse: 1955.2084\n",
      "Epoch 96/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8176 - mse: 1662.6565 - val_loss: 26.5264 - val_mse: 1942.9799\n",
      "Epoch 97/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8186 - mse: 1652.8855 - val_loss: 26.5866 - val_mse: 1946.8865\n",
      "Epoch 98/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9622 - mse: 1679.7705 - val_loss: 26.5634 - val_mse: 1955.6141\n",
      "Epoch 99/500\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 23.5774 - mse: 1586.1951 - val_loss: 26.5812 - val_mse: 1942.3175\n",
      "Epoch 100/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9310 - mse: 1699.4011 - val_loss: 26.5750 - val_mse: 1960.8726\n",
      "Epoch 101/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8329 - mse: 1653.3934 - val_loss: 26.6114 - val_mse: 1958.4705\n",
      "Epoch 102/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7782 - mse: 1660.0793 - val_loss: 26.5912 - val_mse: 1955.1687\n",
      "Epoch 103/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8945 - mse: 1657.5010 - val_loss: 26.5834 - val_mse: 1943.2875\n",
      "Epoch 104/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6754 - mse: 1634.3524 - val_loss: 26.5848 - val_mse: 1952.4725\n",
      "Epoch 105/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5594 - mse: 1615.6580 - val_loss: 26.6070 - val_mse: 1943.4303\n",
      "Epoch 106/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9300 - mse: 1676.5841 - val_loss: 26.6078 - val_mse: 1941.5133\n",
      "Epoch 107/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7610 - mse: 1640.2294 - val_loss: 26.5533 - val_mse: 1944.8065\n",
      "Epoch 108/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6749 - mse: 1641.0748 - val_loss: 26.6136 - val_mse: 1949.6572\n",
      "Epoch 109/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8239 - mse: 1663.2090 - val_loss: 26.6048 - val_mse: 1942.1951\n",
      "Epoch 110/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.7271 - mse: 1640.7135 - val_loss: 26.5754 - val_mse: 1941.3086\n",
      "Epoch 111/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6788 - mse: 1637.0187 - val_loss: 26.5725 - val_mse: 1936.7313\n",
      "Epoch 112/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7392 - mse: 1641.7770 - val_loss: 26.5685 - val_mse: 1940.0919\n",
      "Epoch 113/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.7471 - mse: 1646.1992 - val_loss: 26.5950 - val_mse: 1950.5525\n",
      "Epoch 114/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7309 - mse: 1646.2133 - val_loss: 26.5983 - val_mse: 1942.0413\n",
      "Epoch 115/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7423 - mse: 1647.8158 - val_loss: 26.5984 - val_mse: 1948.0812\n",
      "Epoch 116/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6168 - mse: 1632.5321 - val_loss: 26.6045 - val_mse: 1934.2485\n",
      "Epoch 117/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7433 - mse: 1645.9233 - val_loss: 26.6177 - val_mse: 1948.8616\n",
      "Epoch 118/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7135 - mse: 1644.1538 - val_loss: 26.5767 - val_mse: 1944.8173\n",
      "Epoch 119/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8181 - mse: 1668.7734 - val_loss: 26.5843 - val_mse: 1940.3000\n",
      "Epoch 120/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6886 - mse: 1637.0719 - val_loss: 26.6103 - val_mse: 1940.5314\n",
      "Epoch 121/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6766 - mse: 1622.7897 - val_loss: 26.5961 - val_mse: 1949.4164\n",
      "Epoch 122/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7100 - mse: 1661.9556 - val_loss: 26.6160 - val_mse: 1950.9943\n",
      "Epoch 123/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4931 - mse: 1605.3330 - val_loss: 26.6056 - val_mse: 1947.1486\n",
      "Epoch 124/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8053 - mse: 1657.4503 - val_loss: 26.5789 - val_mse: 1942.0752\n",
      "Epoch 125/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8547 - mse: 1678.8882 - val_loss: 26.6327 - val_mse: 1951.2391\n",
      "Epoch 126/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4877 - mse: 1594.8875 - val_loss: 26.6363 - val_mse: 1951.3397\n",
      "Epoch 127/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6340 - mse: 1643.3796 - val_loss: 26.6027 - val_mse: 1947.8365\n",
      "Epoch 128/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.7618 - mse: 1652.4003 - val_loss: 26.6354 - val_mse: 1951.7075\n",
      "Epoch 129/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6059 - mse: 1641.4178 - val_loss: 26.6562 - val_mse: 1951.9663\n",
      "Epoch 130/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6926 - mse: 1642.5901 - val_loss: 26.6701 - val_mse: 1952.7948\n",
      "Epoch 131/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5638 - mse: 1603.5824 - val_loss: 26.6404 - val_mse: 1957.8242\n",
      "Epoch 132/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9282 - mse: 1693.2896 - val_loss: 26.6392 - val_mse: 1954.5417\n",
      "Epoch 133/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5229 - mse: 1624.5894 - val_loss: 26.6548 - val_mse: 1946.3600\n",
      "Epoch 134/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8546 - mse: 1688.0109 - val_loss: 26.6476 - val_mse: 1941.1293\n",
      "Epoch 135/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5873 - mse: 1609.5157 - val_loss: 26.6078 - val_mse: 1944.8204\n",
      "Epoch 136/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7469 - mse: 1643.1062 - val_loss: 26.6460 - val_mse: 1950.2010\n",
      "Epoch 137/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3732 - mse: 1627.6399 - val_loss: 26.6506 - val_mse: 1947.3118\n",
      "Epoch 138/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.7682 - mse: 1630.2893 - val_loss: 26.6558 - val_mse: 1938.8143\n",
      "Epoch 139/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6172 - mse: 1635.1542 - val_loss: 26.6219 - val_mse: 1946.4551\n",
      "Epoch 140/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8270 - mse: 1685.3289 - val_loss: 26.6828 - val_mse: 1954.3363\n",
      "Epoch 141/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.5051 - mse: 1610.3375 - val_loss: 26.6501 - val_mse: 1943.3877\n",
      "Epoch 142/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2844 - mse: 1560.2209 - val_loss: 26.6587 - val_mse: 1950.6658\n",
      "Epoch 143/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0757 - mse: 1711.5924 - val_loss: 26.6973 - val_mse: 1946.7864\n",
      "Epoch 144/500\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 23.6949 - mse: 1669.6112 - val_loss: 26.6374 - val_mse: 1945.2994\n",
      "Epoch 145/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.1776 - mse: 1566.0725 - val_loss: 26.6628 - val_mse: 1953.1481\n",
      "Epoch 146/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7965 - mse: 1655.5814 - val_loss: 26.6767 - val_mse: 1949.5217\n",
      "Epoch 147/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8494 - mse: 1669.7003 - val_loss: 26.6199 - val_mse: 1946.8865\n",
      "Epoch 148/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7648 - mse: 1643.4609 - val_loss: 26.6784 - val_mse: 1943.2034\n",
      "Epoch 149/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6225 - mse: 1651.7195 - val_loss: 26.6362 - val_mse: 1932.7120\n",
      "Epoch 150/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5335 - mse: 1614.3770 - val_loss: 26.6381 - val_mse: 1946.8040\n",
      "Epoch 151/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6385 - mse: 1656.1100 - val_loss: 26.6058 - val_mse: 1938.5676\n",
      "Epoch 152/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5826 - mse: 1630.6499 - val_loss: 26.6633 - val_mse: 1943.1476\n",
      "Epoch 153/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7834 - mse: 1673.5205 - val_loss: 26.6858 - val_mse: 1944.5204\n",
      "Epoch 154/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5922 - mse: 1614.2036 - val_loss: 26.6873 - val_mse: 1949.5151\n",
      "Epoch 155/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.0856 - mse: 1572.5134 - val_loss: 26.6586 - val_mse: 1939.2070\n",
      "Epoch 156/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7815 - mse: 1657.3362 - val_loss: 26.6238 - val_mse: 1944.4410\n",
      "Epoch 157/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.5513 - mse: 1653.7019 - val_loss: 26.6636 - val_mse: 1944.7269\n",
      "Epoch 158/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7804 - mse: 1670.0380 - val_loss: 26.6422 - val_mse: 1949.6373\n",
      "Epoch 159/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6938 - mse: 1645.3535 - val_loss: 26.7188 - val_mse: 1934.1497\n",
      "Epoch 160/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3677 - mse: 1575.0088 - val_loss: 26.7045 - val_mse: 1932.5742\n",
      "Epoch 161/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8198 - mse: 1701.1608 - val_loss: 26.6598 - val_mse: 1942.5342\n",
      "Epoch 162/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.9521 - mse: 1492.3909 - val_loss: 26.6989 - val_mse: 1946.1350\n",
      "Epoch 163/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0996 - mse: 1741.0714 - val_loss: 26.6566 - val_mse: 1947.8368\n",
      "Epoch 164/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5773 - mse: 1615.2051 - val_loss: 26.6853 - val_mse: 1938.9987\n",
      "Epoch 165/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6350 - mse: 1633.3895 - val_loss: 26.7049 - val_mse: 1955.1914\n",
      "Epoch 166/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4914 - mse: 1634.6758 - val_loss: 26.6726 - val_mse: 1950.8416\n",
      "Epoch 167/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0493 - mse: 1706.3965 - val_loss: 26.7147 - val_mse: 1942.8395\n",
      "Epoch 168/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.1130 - mse: 1576.3668 - val_loss: 26.7021 - val_mse: 1933.9065\n",
      "Epoch 169/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6749 - mse: 1643.5356 - val_loss: 26.6547 - val_mse: 1937.9370\n",
      "Epoch 170/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8170 - mse: 1675.3574 - val_loss: 26.7090 - val_mse: 1948.2609\n",
      "Epoch 171/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4273 - mse: 1610.7626 - val_loss: 26.6815 - val_mse: 1951.5084\n",
      "Epoch 172/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.1302 - mse: 1547.9944 - val_loss: 26.6775 - val_mse: 1953.4805\n",
      "Epoch 173/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6786 - mse: 1672.1532 - val_loss: 26.7168 - val_mse: 1940.9968\n",
      "Epoch 174/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7100 - mse: 1659.1212 - val_loss: 26.7067 - val_mse: 1947.4290\n",
      "Epoch 175/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7184 - mse: 1659.8876 - val_loss: 26.6957 - val_mse: 1944.5997\n",
      "Epoch 176/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.9640 - mse: 1513.7019 - val_loss: 26.6854 - val_mse: 1945.2297\n",
      "Epoch 177/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9328 - mse: 1710.8300 - val_loss: 26.7269 - val_mse: 1942.5564\n",
      "Epoch 178/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6183 - mse: 1668.4056 - val_loss: 26.7206 - val_mse: 1952.3647\n",
      "Epoch 179/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.6881 - mse: 1642.8191 - val_loss: 26.6910 - val_mse: 1944.1942\n",
      "Epoch 180/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4418 - mse: 1603.0724 - val_loss: 26.6830 - val_mse: 1945.2454\n",
      "Epoch 181/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.5760 - mse: 1643.0159 - val_loss: 26.7163 - val_mse: 1951.0715\n",
      "Epoch 182/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2084 - mse: 1561.3127 - val_loss: 26.6827 - val_mse: 1932.2849\n",
      "Epoch 183/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9690 - mse: 1735.8610 - val_loss: 26.7387 - val_mse: 1959.8407\n",
      "Epoch 184/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4848 - mse: 1591.3755 - val_loss: 26.7193 - val_mse: 1946.5867\n",
      "Epoch 185/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7198 - mse: 1663.8247 - val_loss: 26.6869 - val_mse: 1940.1373\n",
      "Epoch 186/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3753 - mse: 1607.2792 - val_loss: 26.7268 - val_mse: 1953.2645\n",
      "Epoch 187/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5331 - mse: 1615.6356 - val_loss: 26.6848 - val_mse: 1937.7559\n",
      "Epoch 188/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4924 - mse: 1632.9557 - val_loss: 26.6988 - val_mse: 1944.9668\n",
      "Epoch 189/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3688 - mse: 1577.1315 - val_loss: 26.7042 - val_mse: 1929.0762\n",
      "Epoch 190/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8304 - mse: 1716.8422 - val_loss: 26.7111 - val_mse: 1948.0282\n",
      "Epoch 191/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5720 - mse: 1602.1014 - val_loss: 26.7441 - val_mse: 1944.9741\n",
      "Epoch 192/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3612 - mse: 1630.1285 - val_loss: 26.7124 - val_mse: 1946.1266\n",
      "Epoch 193/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7271 - mse: 1648.5879 - val_loss: 26.7393 - val_mse: 1948.9264\n",
      "Epoch 194/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7004 - mse: 1671.9094 - val_loss: 26.7213 - val_mse: 1942.5885\n",
      "Epoch 195/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2426 - mse: 1581.6315 - val_loss: 26.7025 - val_mse: 1938.4686\n",
      "Epoch 196/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5941 - mse: 1618.0028 - val_loss: 26.7085 - val_mse: 1946.1671\n",
      "Epoch 197/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5902 - mse: 1663.5402 - val_loss: 26.7071 - val_mse: 1945.3585\n",
      "Epoch 198/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7617 - mse: 1656.4839 - val_loss: 26.7188 - val_mse: 1946.3877\n",
      "Epoch 199/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2526 - mse: 1615.9371 - val_loss: 26.6968 - val_mse: 1935.0211\n",
      "Epoch 200/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3791 - mse: 1571.5111 - val_loss: 26.7654 - val_mse: 1948.0750\n",
      "Epoch 201/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6647 - mse: 1669.9038 - val_loss: 26.7096 - val_mse: 1939.1877\n",
      "Epoch 202/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3969 - mse: 1621.8573 - val_loss: 26.7383 - val_mse: 1943.9071\n",
      "Epoch 203/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6938 - mse: 1644.7358 - val_loss: 26.7653 - val_mse: 1943.3911\n",
      "Epoch 204/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3355 - mse: 1611.8022 - val_loss: 26.7052 - val_mse: 1945.7148\n",
      "Epoch 205/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5602 - mse: 1624.8744 - val_loss: 26.7091 - val_mse: 1948.9182\n",
      "Epoch 206/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7758 - mse: 1666.3771 - val_loss: 26.7176 - val_mse: 1954.4611\n",
      "Epoch 207/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.1309 - mse: 1599.5461 - val_loss: 26.7223 - val_mse: 1947.8287\n",
      "Epoch 208/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6309 - mse: 1645.2651 - val_loss: 26.7251 - val_mse: 1943.2974\n",
      "Epoch 209/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4861 - mse: 1619.7654 - val_loss: 26.7037 - val_mse: 1946.5337\n",
      "Epoch 210/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4598 - mse: 1636.5404 - val_loss: 26.7142 - val_mse: 1944.2903\n",
      "Epoch 211/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8454 - mse: 1661.3372 - val_loss: 26.7377 - val_mse: 1930.5708\n",
      "Epoch 212/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2999 - mse: 1604.6333 - val_loss: 26.7046 - val_mse: 1945.0186\n",
      "Epoch 213/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5042 - mse: 1626.4893 - val_loss: 26.7109 - val_mse: 1950.0592\n",
      "Epoch 214/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4358 - mse: 1633.7982 - val_loss: 26.7091 - val_mse: 1941.7180\n",
      "Epoch 215/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5769 - mse: 1637.5703 - val_loss: 26.7344 - val_mse: 1947.8840\n",
      "Epoch 216/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4534 - mse: 1620.2748 - val_loss: 26.7391 - val_mse: 1944.2689\n",
      "Epoch 217/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4816 - mse: 1636.3163 - val_loss: 26.7319 - val_mse: 1949.0887\n",
      "Epoch 218/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5330 - mse: 1631.5863 - val_loss: 26.7366 - val_mse: 1945.1439\n",
      "Epoch 219/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4675 - mse: 1624.7665 - val_loss: 26.7499 - val_mse: 1945.1832\n",
      "Epoch 220/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5107 - mse: 1635.8574 - val_loss: 26.7480 - val_mse: 1943.4182\n",
      "Epoch 221/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5199 - mse: 1634.8601 - val_loss: 26.7312 - val_mse: 1945.4802\n",
      "Epoch 222/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4699 - mse: 1625.7416 - val_loss: 26.7536 - val_mse: 1946.9191\n",
      "Epoch 223/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4303 - mse: 1626.2644 - val_loss: 26.7140 - val_mse: 1944.4177\n",
      "Epoch 224/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4872 - mse: 1626.1691 - val_loss: 26.7059 - val_mse: 1943.4895\n",
      "Epoch 225/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4787 - mse: 1622.8551 - val_loss: 26.7359 - val_mse: 1947.5857\n",
      "Epoch 226/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5967 - mse: 1655.5370 - val_loss: 26.7530 - val_mse: 1941.9097\n",
      "Epoch 227/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3367 - mse: 1604.0800 - val_loss: 26.7123 - val_mse: 1943.8191\n",
      "Epoch 228/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5034 - mse: 1623.5400 - val_loss: 26.7527 - val_mse: 1935.2410\n",
      "Epoch 229/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6281 - mse: 1670.7876 - val_loss: 26.7365 - val_mse: 1944.1731\n",
      "Epoch 230/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2945 - mse: 1579.8447 - val_loss: 26.7547 - val_mse: 1945.6494\n",
      "Epoch 231/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4888 - mse: 1624.5986 - val_loss: 26.7652 - val_mse: 1945.8593\n",
      "Epoch 232/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3361 - mse: 1614.0413 - val_loss: 26.7485 - val_mse: 1936.8386\n",
      "Epoch 233/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5357 - mse: 1628.0643 - val_loss: 26.7479 - val_mse: 1944.6176\n",
      "Epoch 234/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6912 - mse: 1651.4358 - val_loss: 26.7389 - val_mse: 1944.1063\n",
      "Epoch 235/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3153 - mse: 1611.8599 - val_loss: 26.7234 - val_mse: 1948.9666\n",
      "Epoch 236/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.3421 - mse: 1632.4299 - val_loss: 26.7888 - val_mse: 1946.7148\n",
      "Epoch 237/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5768 - mse: 1621.1313 - val_loss: 26.7413 - val_mse: 1935.7350\n",
      "Epoch 238/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4768 - mse: 1645.8094 - val_loss: 26.7488 - val_mse: 1949.9705\n",
      "Epoch 239/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5182 - mse: 1644.0491 - val_loss: 26.7452 - val_mse: 1943.1222\n",
      "Epoch 240/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5694 - mse: 1633.0682 - val_loss: 26.7427 - val_mse: 1946.0640\n",
      "Epoch 241/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.1501 - mse: 1583.2555 - val_loss: 26.7426 - val_mse: 1942.5658\n",
      "Epoch 242/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8027 - mse: 1666.7092 - val_loss: 26.7391 - val_mse: 1946.1099\n",
      "Epoch 243/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3737 - mse: 1602.5505 - val_loss: 26.7446 - val_mse: 1948.4652\n",
      "Epoch 244/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.1781 - mse: 1590.7429 - val_loss: 26.7640 - val_mse: 1934.7679\n",
      "Epoch 245/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5365 - mse: 1629.5724 - val_loss: 26.7447 - val_mse: 1937.1525\n",
      "Epoch 246/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5503 - mse: 1679.4073 - val_loss: 26.7391 - val_mse: 1939.9446\n",
      "Epoch 247/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3970 - mse: 1594.1736 - val_loss: 26.7586 - val_mse: 1947.0553\n",
      "Epoch 248/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6624 - mse: 1650.0636 - val_loss: 26.7671 - val_mse: 1938.8026\n",
      "Epoch 249/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3645 - mse: 1634.1847 - val_loss: 26.7589 - val_mse: 1940.4796\n",
      "Epoch 250/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.0681 - mse: 1563.4574 - val_loss: 26.7785 - val_mse: 1946.9979\n",
      "Epoch 251/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.6603 - mse: 1669.0763 - val_loss: 26.7614 - val_mse: 1944.6483\n",
      "Epoch 252/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5639 - mse: 1617.3853 - val_loss: 26.7735 - val_mse: 1935.8558\n",
      "Epoch 253/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.2405 - mse: 1586.0375 - val_loss: 26.7609 - val_mse: 1941.0635\n",
      "Epoch 254/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6337 - mse: 1665.6772 - val_loss: 26.7757 - val_mse: 1942.4899\n",
      "Epoch 255/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3043 - mse: 1611.1700 - val_loss: 26.7561 - val_mse: 1942.7831\n",
      "Epoch 256/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6449 - mse: 1654.4178 - val_loss: 26.7987 - val_mse: 1942.8446\n",
      "Epoch 257/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.5509 - mse: 1648.3331 - val_loss: 26.7135 - val_mse: 1937.9264\n",
      "Epoch 258/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.1529 - mse: 1617.3824 - val_loss: 26.8096 - val_mse: 1945.2574\n",
      "Epoch 259/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2914 - mse: 1600.4478 - val_loss: 26.7415 - val_mse: 1954.6703\n",
      "Epoch 260/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6507 - mse: 1630.7069 - val_loss: 26.7389 - val_mse: 1937.8616\n",
      "Epoch 261/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4306 - mse: 1600.4659 - val_loss: 26.7803 - val_mse: 1948.4392\n",
      "Epoch 262/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.1526 - mse: 1622.4224 - val_loss: 26.7881 - val_mse: 1948.4624\n",
      "Epoch 263/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0727 - mse: 1663.8398 - val_loss: 26.7848 - val_mse: 1947.7969\n",
      "Epoch 264/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2629 - mse: 1627.9586 - val_loss: 26.7592 - val_mse: 1944.6272\n",
      "Epoch 265/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.4468 - mse: 1645.3824 - val_loss: 26.7532 - val_mse: 1935.1703\n",
      "Epoch 266/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.3707 - mse: 1589.6682 - val_loss: 26.7591 - val_mse: 1943.5809\n",
      "Epoch 267/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.0272 - mse: 1562.6516 - val_loss: 26.7995 - val_mse: 1929.0065\n",
      "Epoch 268/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.5575 - mse: 1641.3511 - val_loss: 26.7846 - val_mse: 1941.0781\n",
      "Epoch 269/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.6699 - mse: 1672.0585 - val_loss: 26.7932 - val_mse: 1941.0483\n",
      "Epoch 270/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3891 - mse: 1610.0272 - val_loss: 26.8054 - val_mse: 1944.3430\n",
      "Epoch 271/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.1362 - mse: 1566.2332 - val_loss: 26.7431 - val_mse: 1944.9592\n",
      "Epoch 272/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5863 - mse: 1655.0941 - val_loss: 26.8076 - val_mse: 1949.0890\n",
      "Epoch 273/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.5060 - mse: 1662.8958 - val_loss: 26.8004 - val_mse: 1942.6167\n",
      "Epoch 274/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5417 - mse: 1633.8113 - val_loss: 26.8303 - val_mse: 1939.5410\n",
      "Epoch 275/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4099 - mse: 1620.2041 - val_loss: 26.7599 - val_mse: 1943.6451\n",
      "Epoch 276/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3776 - mse: 1631.2672 - val_loss: 26.7989 - val_mse: 1947.6898\n",
      "Epoch 277/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2278 - mse: 1586.6530 - val_loss: 26.7737 - val_mse: 1931.6268\n",
      "Epoch 278/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3971 - mse: 1597.8693 - val_loss: 26.7924 - val_mse: 1947.7063\n",
      "Epoch 279/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3170 - mse: 1616.0874 - val_loss: 26.7936 - val_mse: 1949.5220\n",
      "Epoch 280/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5452 - mse: 1628.2327 - val_loss: 26.7951 - val_mse: 1951.0564\n",
      "Epoch 281/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.1723 - mse: 1597.3450 - val_loss: 26.8021 - val_mse: 1944.7321\n",
      "Epoch 282/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3673 - mse: 1634.6628 - val_loss: 26.8005 - val_mse: 1948.0577\n",
      "Epoch 283/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7285 - mse: 1651.3438 - val_loss: 26.7600 - val_mse: 1943.9128\n",
      "Epoch 284/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2659 - mse: 1603.2367 - val_loss: 26.7943 - val_mse: 1946.9086\n",
      "Epoch 285/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2835 - mse: 1600.0419 - val_loss: 26.7884 - val_mse: 1948.8994\n",
      "Epoch 286/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6728 - mse: 1663.5548 - val_loss: 26.8028 - val_mse: 1951.9248\n",
      "Epoch 287/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3909 - mse: 1622.4445 - val_loss: 26.8029 - val_mse: 1945.7068\n",
      "Epoch 288/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3597 - mse: 1637.3275 - val_loss: 26.8222 - val_mse: 1953.6545\n",
      "Epoch 289/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4538 - mse: 1595.6907 - val_loss: 26.8030 - val_mse: 1953.3242\n",
      "Epoch 290/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.0607 - mse: 1601.9814 - val_loss: 26.7987 - val_mse: 1944.3258\n",
      "Epoch 291/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6493 - mse: 1632.8379 - val_loss: 26.7756 - val_mse: 1944.8232\n",
      "Epoch 292/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4096 - mse: 1663.1455 - val_loss: 26.7855 - val_mse: 1946.3580\n",
      "Epoch 293/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.1430 - mse: 1578.8499 - val_loss: 26.7937 - val_mse: 1942.0525\n",
      "Epoch 294/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7991 - mse: 1670.1660 - val_loss: 26.8078 - val_mse: 1946.5757\n",
      "Epoch 295/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.0183 - mse: 1541.8573 - val_loss: 26.8090 - val_mse: 1943.1439\n",
      "Epoch 296/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7094 - mse: 1699.9596 - val_loss: 26.8118 - val_mse: 1948.7104\n",
      "Epoch 297/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.1830 - mse: 1583.1733 - val_loss: 26.7989 - val_mse: 1951.1075\n",
      "Epoch 298/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3101 - mse: 1586.2114 - val_loss: 26.7882 - val_mse: 1945.3389\n",
      "Epoch 299/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5214 - mse: 1609.9132 - val_loss: 26.7997 - val_mse: 1949.2856\n",
      "Epoch 300/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7899 - mse: 1716.5773 - val_loss: 26.8073 - val_mse: 1941.7146\n",
      "Epoch 301/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.7955 - mse: 1542.8563 - val_loss: 26.8116 - val_mse: 1946.0690\n",
      "Epoch 302/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7788 - mse: 1694.2244 - val_loss: 26.7976 - val_mse: 1948.3542\n",
      "Epoch 303/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2526 - mse: 1591.9697 - val_loss: 26.8010 - val_mse: 1946.0405\n",
      "Epoch 304/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.9516 - mse: 1544.7606 - val_loss: 26.7996 - val_mse: 1946.1770\n",
      "Epoch 305/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7803 - mse: 1691.3850 - val_loss: 26.8118 - val_mse: 1955.4431\n",
      "Epoch 306/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5344 - mse: 1624.9860 - val_loss: 26.8426 - val_mse: 1951.0186\n",
      "Epoch 307/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.0628 - mse: 1572.2012 - val_loss: 26.8297 - val_mse: 1946.9635\n",
      "Epoch 308/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5667 - mse: 1662.6337 - val_loss: 26.8228 - val_mse: 1945.4791\n",
      "Epoch 309/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3749 - mse: 1601.0214 - val_loss: 26.8224 - val_mse: 1959.1288\n",
      "Epoch 310/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5697 - mse: 1662.7643 - val_loss: 26.7807 - val_mse: 1951.5890\n",
      "Epoch 311/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.1974 - mse: 1589.7854 - val_loss: 26.7761 - val_mse: 1942.3601\n",
      "Epoch 312/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.3607 - mse: 1635.2134 - val_loss: 26.8175 - val_mse: 1949.3682\n",
      "Epoch 313/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.2589 - mse: 1573.6714 - val_loss: 26.8000 - val_mse: 1948.8955\n",
      "Epoch 314/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4955 - mse: 1639.8950 - val_loss: 26.8040 - val_mse: 1943.2524\n",
      "Epoch 315/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2675 - mse: 1595.5370 - val_loss: 26.8127 - val_mse: 1951.9473\n",
      "Epoch 316/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.3076 - mse: 1632.2593 - val_loss: 26.7921 - val_mse: 1944.6392\n",
      "Epoch 317/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4163 - mse: 1626.7880 - val_loss: 26.7955 - val_mse: 1950.5050\n",
      "Epoch 318/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.3645 - mse: 1594.5492 - val_loss: 26.7858 - val_mse: 1940.8494\n",
      "Epoch 319/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.0953 - mse: 1576.4374 - val_loss: 26.7982 - val_mse: 1947.2139\n",
      "Epoch 320/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4627 - mse: 1630.3975 - val_loss: 26.8095 - val_mse: 1950.9186\n",
      "Epoch 321/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4730 - mse: 1641.2784 - val_loss: 26.8212 - val_mse: 1953.0807\n",
      "Epoch 322/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5335 - mse: 1660.0101 - val_loss: 26.7988 - val_mse: 1946.9611\n",
      "Epoch 323/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2453 - mse: 1587.1580 - val_loss: 26.7840 - val_mse: 1941.2107\n",
      "Epoch 324/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.4439 - mse: 1623.3988 - val_loss: 26.8008 - val_mse: 1929.1758\n",
      "Epoch 325/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4099 - mse: 1644.2496 - val_loss: 26.8040 - val_mse: 1948.6593\n",
      "Epoch 326/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.1841 - mse: 1585.4169 - val_loss: 26.8089 - val_mse: 1944.7040\n",
      "Epoch 327/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4474 - mse: 1627.7169 - val_loss: 26.8011 - val_mse: 1947.3207\n",
      "Epoch 328/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4868 - mse: 1640.0049 - val_loss: 26.7790 - val_mse: 1951.0220\n",
      "Epoch 329/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.0018 - mse: 1562.1099 - val_loss: 26.8112 - val_mse: 1939.6313\n",
      "Epoch 330/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7761 - mse: 1677.6743 - val_loss: 26.8285 - val_mse: 1942.0559\n",
      "Epoch 331/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2375 - mse: 1616.6213 - val_loss: 26.8048 - val_mse: 1943.0648\n",
      "Epoch 332/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3077 - mse: 1597.7147 - val_loss: 26.8125 - val_mse: 1957.7778\n",
      "Epoch 333/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3762 - mse: 1622.0103 - val_loss: 26.8304 - val_mse: 1953.7909\n",
      "Epoch 334/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.4372 - mse: 1631.4132 - val_loss: 26.8067 - val_mse: 1940.8248\n",
      "Epoch 335/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2988 - mse: 1610.9736 - val_loss: 26.8260 - val_mse: 1947.5353\n",
      "Epoch 336/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3560 - mse: 1615.5892 - val_loss: 26.8375 - val_mse: 1952.7891\n",
      "Epoch 337/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4219 - mse: 1629.6855 - val_loss: 26.8198 - val_mse: 1947.0704\n",
      "Epoch 338/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.3176 - mse: 1610.8362 - val_loss: 26.8031 - val_mse: 1940.7712\n",
      "Epoch 339/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4016 - mse: 1622.3806 - val_loss: 26.7988 - val_mse: 1941.4053\n",
      "Epoch 340/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2389 - mse: 1602.4858 - val_loss: 26.8469 - val_mse: 1949.2781\n",
      "Epoch 341/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3808 - mse: 1620.5388 - val_loss: 26.8177 - val_mse: 1953.2927\n",
      "Epoch 342/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3433 - mse: 1620.6904 - val_loss: 26.8333 - val_mse: 1950.2524\n",
      "Epoch 343/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.4407 - mse: 1636.5966 - val_loss: 26.8148 - val_mse: 1940.9658\n",
      "Epoch 344/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.3051 - mse: 1593.8630 - val_loss: 26.8164 - val_mse: 1946.1102\n",
      "Epoch 345/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2618 - mse: 1606.3983 - val_loss: 26.8311 - val_mse: 1942.1449\n",
      "Epoch 346/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.3130 - mse: 1614.6978 - val_loss: 26.8370 - val_mse: 1949.2249\n",
      "Epoch 347/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4397 - mse: 1630.5094 - val_loss: 26.8198 - val_mse: 1937.4799\n",
      "Epoch 348/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5177 - mse: 1645.7474 - val_loss: 26.8230 - val_mse: 1942.2909\n",
      "Epoch 349/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.0964 - mse: 1562.3414 - val_loss: 26.8516 - val_mse: 1942.5326\n",
      "Epoch 350/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2762 - mse: 1621.4318 - val_loss: 26.8451 - val_mse: 1952.1588\n",
      "Epoch 351/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5194 - mse: 1634.2058 - val_loss: 26.8349 - val_mse: 1951.9078\n",
      "Epoch 352/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.1910 - mse: 1580.3060 - val_loss: 26.8264 - val_mse: 1932.0044\n",
      "Epoch 353/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8395 - mse: 1706.0182 - val_loss: 26.8191 - val_mse: 1947.2566\n",
      "Epoch 354/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 22.8217 - mse: 1534.7550 - val_loss: 26.8205 - val_mse: 1951.5026\n",
      "Epoch 355/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5115 - mse: 1664.3660 - val_loss: 26.8318 - val_mse: 1951.7886\n",
      "Epoch 356/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2556 - mse: 1574.2173 - val_loss: 26.8383 - val_mse: 1943.2737\n",
      "Epoch 357/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5069 - mse: 1655.5215 - val_loss: 26.8221 - val_mse: 1948.5201\n",
      "Epoch 358/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.1517 - mse: 1580.0458 - val_loss: 26.8270 - val_mse: 1949.7006\n",
      "Epoch 359/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4919 - mse: 1625.6544 - val_loss: 26.8577 - val_mse: 1946.1128\n",
      "Epoch 360/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.9601 - mse: 1578.6216 - val_loss: 26.8270 - val_mse: 1949.6830\n",
      "Epoch 361/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6801 - mse: 1654.9927 - val_loss: 26.8387 - val_mse: 1950.1023\n",
      "Epoch 362/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4675 - mse: 1661.0348 - val_loss: 26.8222 - val_mse: 1942.6843\n",
      "Epoch 363/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.0967 - mse: 1559.0969 - val_loss: 26.8379 - val_mse: 1938.8373\n",
      "Epoch 364/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4563 - mse: 1642.4194 - val_loss: 26.8380 - val_mse: 1941.4778\n",
      "Epoch 365/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.1146 - mse: 1584.2485 - val_loss: 26.8560 - val_mse: 1954.5612\n",
      "Epoch 366/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6830 - mse: 1676.4182 - val_loss: 26.8492 - val_mse: 1948.9081\n",
      "Epoch 367/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.1291 - mse: 1590.8588 - val_loss: 26.8392 - val_mse: 1941.8260\n",
      "Epoch 368/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.1268 - mse: 1566.7855 - val_loss: 26.8118 - val_mse: 1947.9316\n",
      "Epoch 369/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7253 - mse: 1689.5861 - val_loss: 26.8564 - val_mse: 1931.2990\n",
      "Epoch 370/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3068 - mse: 1609.4746 - val_loss: 26.8606 - val_mse: 1940.5590\n",
      "Epoch 371/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.0103 - mse: 1561.8445 - val_loss: 26.8817 - val_mse: 1953.8600\n",
      "Epoch 372/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.5221 - mse: 1641.3749 - val_loss: 26.7971 - val_mse: 1936.2726\n",
      "Epoch 373/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.1351 - mse: 1604.1514 - val_loss: 26.8708 - val_mse: 1948.0068\n",
      "Epoch 374/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3406 - mse: 1593.0067 - val_loss: 26.7891 - val_mse: 1938.5927\n",
      "Epoch 375/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3700 - mse: 1624.3633 - val_loss: 26.8137 - val_mse: 1947.2185\n",
      "Epoch 376/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.1110 - mse: 1568.3082 - val_loss: 26.8732 - val_mse: 1962.3209\n",
      "Epoch 377/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4591 - mse: 1652.0093 - val_loss: 26.8356 - val_mse: 1949.8741\n",
      "Epoch 378/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.1249 - mse: 1588.5637 - val_loss: 26.8056 - val_mse: 1945.6626\n",
      "Epoch 379/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.5924 - mse: 1674.0656 - val_loss: 26.8575 - val_mse: 1944.9337\n",
      "Epoch 380/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3119 - mse: 1597.0636 - val_loss: 26.8207 - val_mse: 1945.7430\n",
      "Epoch 381/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4444 - mse: 1630.7311 - val_loss: 26.8484 - val_mse: 1956.1199\n",
      "Epoch 382/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.1946 - mse: 1594.1790 - val_loss: 26.8107 - val_mse: 1949.7786\n",
      "Epoch 383/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4049 - mse: 1622.3142 - val_loss: 26.8278 - val_mse: 1947.6768\n",
      "Epoch 384/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.7482 - mse: 1537.9160 - val_loss: 26.8176 - val_mse: 1950.3835\n",
      "Epoch 385/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7065 - mse: 1634.8905 - val_loss: 26.8651 - val_mse: 1946.6036\n",
      "Epoch 386/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2236 - mse: 1647.8595 - val_loss: 26.8464 - val_mse: 1947.2321\n",
      "Epoch 387/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5005 - mse: 1613.4594 - val_loss: 26.8565 - val_mse: 1950.2107\n",
      "Epoch 388/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4165 - mse: 1646.3187 - val_loss: 26.8579 - val_mse: 1953.4225\n",
      "Epoch 389/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.4311 - mse: 1629.4490 - val_loss: 26.8135 - val_mse: 1940.8629\n",
      "Epoch 390/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.2567 - mse: 1616.3646 - val_loss: 26.8350 - val_mse: 1943.9822\n",
      "Epoch 391/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.1028 - mse: 1559.7434 - val_loss: 26.8436 - val_mse: 1947.9543\n",
      "Epoch 392/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3854 - mse: 1633.0337 - val_loss: 26.8526 - val_mse: 1944.2391\n",
      "Epoch 393/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4005 - mse: 1640.9817 - val_loss: 26.8431 - val_mse: 1950.7694\n",
      "Epoch 394/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4730 - mse: 1648.8632 - val_loss: 26.8401 - val_mse: 1947.4175\n",
      "Epoch 395/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2273 - mse: 1576.6573 - val_loss: 26.8802 - val_mse: 1949.8909\n",
      "Epoch 396/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3213 - mse: 1614.5294 - val_loss: 26.8816 - val_mse: 1940.2794\n",
      "Epoch 397/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.8416 - mse: 1539.5073 - val_loss: 26.8246 - val_mse: 1949.0660\n",
      "Epoch 398/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6820 - mse: 1662.6785 - val_loss: 26.8188 - val_mse: 1944.2504\n",
      "Epoch 399/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.1695 - mse: 1614.4200 - val_loss: 26.8464 - val_mse: 1940.9434\n",
      "Epoch 400/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6264 - mse: 1650.7893 - val_loss: 26.8455 - val_mse: 1948.9899\n",
      "Epoch 401/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.9943 - mse: 1572.6104 - val_loss: 26.8172 - val_mse: 1949.2715\n",
      "Epoch 402/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.1652 - mse: 1599.5297 - val_loss: 26.8583 - val_mse: 1944.3157\n",
      "Epoch 403/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5565 - mse: 1592.1366 - val_loss: 26.8298 - val_mse: 1942.0292\n",
      "Epoch 404/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.1419 - mse: 1634.2122 - val_loss: 26.8592 - val_mse: 1943.3271\n",
      "Epoch 405/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2422 - mse: 1583.4265 - val_loss: 26.8211 - val_mse: 1948.0770\n",
      "Epoch 406/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3505 - mse: 1634.5193 - val_loss: 26.8361 - val_mse: 1949.9982\n",
      "Epoch 407/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2132 - mse: 1610.4207 - val_loss: 26.8128 - val_mse: 1953.1814\n",
      "Epoch 408/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2598 - mse: 1600.0117 - val_loss: 26.8469 - val_mse: 1949.4556\n",
      "Epoch 409/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5333 - mse: 1667.9094 - val_loss: 26.8393 - val_mse: 1939.8152\n",
      "Epoch 410/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3809 - mse: 1579.6969 - val_loss: 26.8681 - val_mse: 1955.2488\n",
      "Epoch 411/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2117 - mse: 1631.4437 - val_loss: 26.8154 - val_mse: 1943.5984\n",
      "Epoch 412/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4002 - mse: 1650.7815 - val_loss: 26.8322 - val_mse: 1952.7274\n",
      "Epoch 413/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.1933 - mse: 1553.7628 - val_loss: 26.8542 - val_mse: 1948.8402\n",
      "Epoch 414/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2254 - mse: 1620.9777 - val_loss: 26.8382 - val_mse: 1945.0096\n",
      "Epoch 415/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2014 - mse: 1570.4832 - val_loss: 26.8282 - val_mse: 1949.1006\n",
      "Epoch 416/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.1710 - mse: 1609.6892 - val_loss: 26.8311 - val_mse: 1952.1754\n",
      "Epoch 417/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3579 - mse: 1617.0999 - val_loss: 26.8677 - val_mse: 1943.0498\n",
      "Epoch 418/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3643 - mse: 1641.2369 - val_loss: 26.8596 - val_mse: 1955.3276\n",
      "Epoch 419/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.0791 - mse: 1575.0022 - val_loss: 26.8346 - val_mse: 1952.1174\n",
      "Epoch 420/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5061 - mse: 1639.1328 - val_loss: 26.8697 - val_mse: 1951.2379\n",
      "Epoch 421/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2332 - mse: 1604.1921 - val_loss: 26.8379 - val_mse: 1949.9670\n",
      "Epoch 422/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4220 - mse: 1645.0022 - val_loss: 26.8398 - val_mse: 1947.2157\n",
      "Epoch 423/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3912 - mse: 1627.3149 - val_loss: 26.8774 - val_mse: 1944.6284\n",
      "Epoch 424/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2897 - mse: 1615.7328 - val_loss: 26.8463 - val_mse: 1942.3694\n",
      "Epoch 425/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.0035 - mse: 1560.5461 - val_loss: 26.8285 - val_mse: 1947.3115\n",
      "Epoch 426/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4499 - mse: 1641.3982 - val_loss: 26.8601 - val_mse: 1956.5344\n",
      "Epoch 427/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.0848 - mse: 1577.4008 - val_loss: 26.8306 - val_mse: 1947.2698\n",
      "Epoch 428/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4722 - mse: 1655.7490 - val_loss: 26.8522 - val_mse: 1950.2762\n",
      "Epoch 429/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2053 - mse: 1594.4935 - val_loss: 26.8495 - val_mse: 1950.5933\n",
      "Epoch 430/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.0781 - mse: 1551.8512 - val_loss: 26.8386 - val_mse: 1945.0536\n",
      "Epoch 431/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6249 - mse: 1678.8834 - val_loss: 26.8229 - val_mse: 1944.5162\n",
      "Epoch 432/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.0347 - mse: 1573.6135 - val_loss: 26.8291 - val_mse: 1949.3491\n",
      "Epoch 433/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.4620 - mse: 1645.4128 - val_loss: 26.8632 - val_mse: 1953.4982\n",
      "Epoch 434/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.0874 - mse: 1571.7217 - val_loss: 26.8271 - val_mse: 1947.0068\n",
      "Epoch 435/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3659 - mse: 1626.5824 - val_loss: 26.8307 - val_mse: 1948.0940\n",
      "Epoch 436/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4453 - mse: 1650.8384 - val_loss: 26.8315 - val_mse: 1947.3776\n",
      "Epoch 437/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.0999 - mse: 1586.7101 - val_loss: 26.8671 - val_mse: 1948.8732\n",
      "Epoch 438/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3141 - mse: 1604.2253 - val_loss: 26.8483 - val_mse: 1945.4725\n",
      "Epoch 439/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2967 - mse: 1618.6532 - val_loss: 26.8631 - val_mse: 1950.9512\n",
      "Epoch 440/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3242 - mse: 1604.5526 - val_loss: 26.8365 - val_mse: 1946.7765\n",
      "Epoch 441/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2386 - mse: 1619.0420 - val_loss: 26.8495 - val_mse: 1955.8083\n",
      "Epoch 442/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.1275 - mse: 1574.8850 - val_loss: 26.8350 - val_mse: 1943.2120\n",
      "Epoch 443/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2199 - mse: 1606.4078 - val_loss: 26.8429 - val_mse: 1944.3806\n",
      "Epoch 444/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3840 - mse: 1622.6704 - val_loss: 26.8401 - val_mse: 1949.4865\n",
      "Epoch 445/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2875 - mse: 1624.6116 - val_loss: 26.8381 - val_mse: 1951.0460\n",
      "Epoch 446/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2846 - mse: 1601.1014 - val_loss: 26.8538 - val_mse: 1950.6582\n",
      "Epoch 447/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2962 - mse: 1615.2838 - val_loss: 26.8498 - val_mse: 1953.6862\n",
      "Epoch 448/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2321 - mse: 1604.0522 - val_loss: 26.8622 - val_mse: 1956.3131\n",
      "Epoch 449/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2520 - mse: 1605.0238 - val_loss: 26.8324 - val_mse: 1951.4718\n",
      "Epoch 450/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2756 - mse: 1619.3525 - val_loss: 26.8245 - val_mse: 1951.1138\n",
      "Epoch 451/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.2354 - mse: 1597.7433 - val_loss: 26.8195 - val_mse: 1948.9556\n",
      "Epoch 452/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3253 - mse: 1621.3879 - val_loss: 26.8408 - val_mse: 1950.9702\n",
      "Epoch 453/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.2656 - mse: 1606.7085 - val_loss: 26.8529 - val_mse: 1953.7107\n",
      "Epoch 454/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.1921 - mse: 1598.9929 - val_loss: 26.8341 - val_mse: 1949.6382\n",
      "Epoch 455/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.3741 - mse: 1631.4890 - val_loss: 26.8418 - val_mse: 1954.9938\n",
      "Epoch 456/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.1646 - mse: 1591.0267 - val_loss: 26.8642 - val_mse: 1952.6812\n",
      "Epoch 457/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.2588 - mse: 1610.2532 - val_loss: 26.8371 - val_mse: 1945.5096\n",
      "Epoch 458/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4274 - mse: 1632.0114 - val_loss: 26.8651 - val_mse: 1942.2788\n",
      "Epoch 459/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.0666 - mse: 1585.6102 - val_loss: 26.8196 - val_mse: 1944.7384\n",
      "Epoch 460/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.1585 - mse: 1576.7271 - val_loss: 26.8347 - val_mse: 1946.0126\n",
      "Epoch 461/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4565 - mse: 1644.7241 - val_loss: 26.8664 - val_mse: 1950.0704\n",
      "Epoch 462/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.1273 - mse: 1586.2328 - val_loss: 26.8309 - val_mse: 1944.8606\n",
      "Epoch 463/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2459 - mse: 1613.1165 - val_loss: 26.8284 - val_mse: 1947.6942\n",
      "Epoch 464/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.2868 - mse: 1606.6560 - val_loss: 26.8498 - val_mse: 1949.6960\n",
      "Epoch 465/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3878 - mse: 1618.4769 - val_loss: 26.8237 - val_mse: 1945.7405\n",
      "Epoch 466/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.0664 - mse: 1596.3555 - val_loss: 26.8631 - val_mse: 1946.8452\n",
      "Epoch 467/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5805 - mse: 1650.5149 - val_loss: 26.8648 - val_mse: 1945.4562\n",
      "Epoch 468/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2751 - mse: 1635.9100 - val_loss: 26.8681 - val_mse: 1944.6378\n",
      "Epoch 469/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.9193 - mse: 1544.2961 - val_loss: 26.8427 - val_mse: 1946.6776\n",
      "Epoch 470/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.4069 - mse: 1629.6312 - val_loss: 26.8448 - val_mse: 1946.5310\n",
      "Epoch 471/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.1290 - mse: 1594.0884 - val_loss: 26.8528 - val_mse: 1946.4982\n",
      "Epoch 472/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.2270 - mse: 1596.5271 - val_loss: 26.8583 - val_mse: 1953.9878\n",
      "Epoch 473/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2008 - mse: 1613.5905 - val_loss: 26.8673 - val_mse: 1941.4663\n",
      "Epoch 474/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.0596 - mse: 1569.4734 - val_loss: 26.8217 - val_mse: 1946.9956\n",
      "Epoch 475/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6627 - mse: 1664.4316 - val_loss: 26.8590 - val_mse: 1951.6329\n",
      "Epoch 476/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2502 - mse: 1598.7184 - val_loss: 26.8765 - val_mse: 1951.4496\n",
      "Epoch 477/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.1956 - mse: 1606.9409 - val_loss: 26.8373 - val_mse: 1952.7253\n",
      "Epoch 478/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2919 - mse: 1632.0021 - val_loss: 26.8801 - val_mse: 1955.6522\n",
      "Epoch 479/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2563 - mse: 1622.2289 - val_loss: 26.8626 - val_mse: 1949.6969\n",
      "Epoch 480/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.0898 - mse: 1535.9349 - val_loss: 26.8205 - val_mse: 1939.9353\n",
      "Epoch 481/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6846 - mse: 1684.5085 - val_loss: 26.8652 - val_mse: 1941.7678\n",
      "Epoch 482/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.9238 - mse: 1557.9290 - val_loss: 26.8804 - val_mse: 1949.4778\n",
      "Epoch 483/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.7580 - mse: 1532.4165 - val_loss: 26.8580 - val_mse: 1948.3290\n",
      "Epoch 484/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8683 - mse: 1692.9447 - val_loss: 26.8586 - val_mse: 1944.0428\n",
      "Epoch 485/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.0743 - mse: 1599.4419 - val_loss: 26.8686 - val_mse: 1945.5762\n",
      "Epoch 486/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3434 - mse: 1629.3239 - val_loss: 26.8634 - val_mse: 1944.1985\n",
      "Epoch 487/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.1700 - mse: 1593.3201 - val_loss: 26.8781 - val_mse: 1952.6866\n",
      "Epoch 488/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.1831 - mse: 1580.8888 - val_loss: 26.8540 - val_mse: 1947.3773\n",
      "Epoch 489/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.1841 - mse: 1606.4692 - val_loss: 26.8749 - val_mse: 1951.1869\n",
      "Epoch 490/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3754 - mse: 1630.8456 - val_loss: 26.8577 - val_mse: 1953.4768\n",
      "Epoch 491/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.1313 - mse: 1574.1310 - val_loss: 26.8483 - val_mse: 1955.8271\n",
      "Epoch 492/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.0366 - mse: 1587.3865 - val_loss: 26.8685 - val_mse: 1944.3123\n",
      "Epoch 493/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4738 - mse: 1638.0795 - val_loss: 26.8556 - val_mse: 1945.5814\n",
      "Epoch 494/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.1017 - mse: 1595.4800 - val_loss: 26.8654 - val_mse: 1944.9423\n",
      "Epoch 495/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5013 - mse: 1618.4457 - val_loss: 26.8629 - val_mse: 1946.1073\n",
      "Epoch 496/500\n",
      " 40/113 [=========>....................] - ETA: 0s - loss: 22.7898 - mse: 1557.4855WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 56500 batches). You may need to use the repeat() function when building your dataset.\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.9561 - mse: 1612.3516 - val_loss: 26.8371 - val_mse: 1944.9402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 20:25:59.129082: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_precip0\n",
      "[40.85, 42.93]\n",
      "Epoch 1/500\n",
      "109/113 [===========================>..] - ETA: 0s - loss: 46.7719 - mse: 3860.8999"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 20:25:59.941569: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 0s 3ms/step - loss: 46.2232 - mse: 3826.5085 - val_loss: 29.0816 - val_mse: 2328.0132\n",
      "Epoch 2/500\n",
      " 76/113 [===================>..........] - ETA: 0s - loss: 25.6262 - mse: 1792.9514"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 20:26:00.167453: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 0s 2ms/step - loss: 25.6010 - mse: 1789.9901 - val_loss: 26.1963 - val_mse: 1893.0410\n",
      "Epoch 3/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2900 - mse: 1731.8597 - val_loss: 26.1837 - val_mse: 1893.2174\n",
      "Epoch 4/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4770 - mse: 1775.3827 - val_loss: 26.1574 - val_mse: 1859.5601\n",
      "Epoch 5/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2145 - mse: 1724.9998 - val_loss: 26.1419 - val_mse: 1882.9587\n",
      "Epoch 6/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.3997 - mse: 1769.3627 - val_loss: 26.1245 - val_mse: 1882.5824\n",
      "Epoch 7/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4804 - mse: 1774.7065 - val_loss: 26.1061 - val_mse: 1881.7960\n",
      "Epoch 8/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1090 - mse: 1703.5345 - val_loss: 26.0824 - val_mse: 1861.9963\n",
      "Epoch 9/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4522 - mse: 1780.6101 - val_loss: 26.0771 - val_mse: 1886.7589\n",
      "Epoch 10/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.9998 - mse: 1692.3026 - val_loss: 26.0524 - val_mse: 1880.9934\n",
      "Epoch 11/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3251 - mse: 1765.6854 - val_loss: 26.0226 - val_mse: 1861.6250\n",
      "Epoch 12/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1345 - mse: 1725.9307 - val_loss: 26.0036 - val_mse: 1853.5486\n",
      "Epoch 13/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2461 - mse: 1751.2306 - val_loss: 26.0161 - val_mse: 1891.3358\n",
      "Epoch 14/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1498 - mse: 1725.4679 - val_loss: 25.9676 - val_mse: 1871.6202\n",
      "Epoch 15/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3271 - mse: 1766.4856 - val_loss: 25.9521 - val_mse: 1835.8617\n",
      "Epoch 16/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.0159 - mse: 1720.5692 - val_loss: 25.9213 - val_mse: 1860.6224\n",
      "Epoch 17/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.0568 - mse: 1707.3893 - val_loss: 25.9473 - val_mse: 1890.6237\n",
      "Epoch 18/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.9870 - mse: 1728.3887 - val_loss: 25.8783 - val_mse: 1855.0393\n",
      "Epoch 19/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.0290 - mse: 1708.4059 - val_loss: 25.8599 - val_mse: 1858.7914\n",
      "Epoch 20/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3220 - mse: 1782.8905 - val_loss: 25.8377 - val_mse: 1836.1420\n",
      "Epoch 21/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.8423 - mse: 1693.4413 - val_loss: 25.8259 - val_mse: 1862.8613\n",
      "Epoch 22/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.8631 - mse: 1705.2385 - val_loss: 25.7935 - val_mse: 1848.9640\n",
      "Epoch 23/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.9236 - mse: 1730.2333 - val_loss: 25.7757 - val_mse: 1852.2006\n",
      "Epoch 24/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.0739 - mse: 1731.5712 - val_loss: 25.7836 - val_mse: 1870.4336\n",
      "Epoch 25/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.8775 - mse: 1687.8059 - val_loss: 25.7291 - val_mse: 1844.3090\n",
      "Epoch 26/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.8872 - mse: 1734.8969 - val_loss: 25.7096 - val_mse: 1845.9862\n",
      "Epoch 27/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.9406 - mse: 1682.3361 - val_loss: 25.6843 - val_mse: 1822.8438\n",
      "Epoch 28/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.7995 - mse: 1743.6475 - val_loss: 25.6664 - val_mse: 1813.3408\n",
      "Epoch 29/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3179 - mse: 1611.3273 - val_loss: 25.6796 - val_mse: 1861.6250\n",
      "Epoch 30/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3031 - mse: 1794.8669 - val_loss: 25.6691 - val_mse: 1864.2166\n",
      "Epoch 31/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.9049 - mse: 1691.6172 - val_loss: 25.6521 - val_mse: 1864.0862\n",
      "Epoch 32/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4792 - mse: 1670.7825 - val_loss: 25.6185 - val_mse: 1856.1931\n",
      "Epoch 33/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.8955 - mse: 1708.7301 - val_loss: 25.5512 - val_mse: 1807.0221\n",
      "Epoch 34/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.8693 - mse: 1728.5582 - val_loss: 25.5519 - val_mse: 1837.7091\n",
      "Epoch 35/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2916 - mse: 1631.0525 - val_loss: 25.5108 - val_mse: 1813.8806\n",
      "Epoch 36/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.8928 - mse: 1727.5463 - val_loss: 25.5022 - val_mse: 1826.3127\n",
      "Epoch 37/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.5363 - mse: 1655.3531 - val_loss: 25.4798 - val_mse: 1787.7026\n",
      "Epoch 38/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.5406 - mse: 1676.1195 - val_loss: 25.4632 - val_mse: 1821.5449\n",
      "Epoch 39/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.6580 - mse: 1695.0083 - val_loss: 25.4355 - val_mse: 1810.2137\n",
      "Epoch 40/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4152 - mse: 1661.5824 - val_loss: 25.4287 - val_mse: 1818.0724\n",
      "Epoch 41/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3079 - mse: 1633.0950 - val_loss: 25.4012 - val_mse: 1785.0719\n",
      "Epoch 42/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.8890 - mse: 1743.7695 - val_loss: 25.3822 - val_mse: 1787.0603\n",
      "Epoch 43/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3157 - mse: 1626.8207 - val_loss: 25.3686 - val_mse: 1798.2194\n",
      "Epoch 44/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2191 - mse: 1633.4183 - val_loss: 25.3536 - val_mse: 1783.3354\n",
      "Epoch 45/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.5158 - mse: 1683.4996 - val_loss: 25.3457 - val_mse: 1797.7729\n",
      "Epoch 46/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.6946 - mse: 1708.1315 - val_loss: 25.3261 - val_mse: 1786.9611\n",
      "Epoch 47/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0072 - mse: 1599.9623 - val_loss: 25.3276 - val_mse: 1798.3795\n",
      "Epoch 48/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.6483 - mse: 1694.2069 - val_loss: 25.3341 - val_mse: 1806.7837\n",
      "Epoch 49/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.5779 - mse: 1707.5133 - val_loss: 25.2995 - val_mse: 1757.6506\n",
      "Epoch 50/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1669 - mse: 1591.3749 - val_loss: 25.2906 - val_mse: 1784.8201\n",
      "Epoch 51/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1505 - mse: 1624.6827 - val_loss: 25.2821 - val_mse: 1782.5608\n",
      "Epoch 52/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3861 - mse: 1657.1816 - val_loss: 25.2721 - val_mse: 1778.0364\n",
      "Epoch 53/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1243 - mse: 1629.6558 - val_loss: 25.3430 - val_mse: 1816.9116\n",
      "Epoch 54/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4270 - mse: 1685.5892 - val_loss: 25.2573 - val_mse: 1773.3934\n",
      "Epoch 55/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1750 - mse: 1607.9304 - val_loss: 25.2932 - val_mse: 1797.2705\n",
      "Epoch 56/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4190 - mse: 1674.2780 - val_loss: 25.2627 - val_mse: 1781.8053\n",
      "Epoch 57/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2673 - mse: 1600.7191 - val_loss: 25.2343 - val_mse: 1749.2396\n",
      "Epoch 58/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.1035 - mse: 1620.3912 - val_loss: 25.2629 - val_mse: 1783.1702\n",
      "Epoch 59/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.6738 - mse: 1701.9255 - val_loss: 25.2456 - val_mse: 1732.3196\n",
      "Epoch 60/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8950 - mse: 1604.6445 - val_loss: 25.2497 - val_mse: 1777.0458\n",
      "Epoch 61/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3945 - mse: 1663.7698 - val_loss: 25.2448 - val_mse: 1727.0188\n",
      "Epoch 62/500\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 24.1299 - mse: 1602.2644 - val_loss: 25.2179 - val_mse: 1752.0433\n",
      "Epoch 63/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.1263 - mse: 1613.3209 - val_loss: 25.2292 - val_mse: 1731.2294\n",
      "Epoch 64/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3019 - mse: 1630.2877 - val_loss: 25.2306 - val_mse: 1766.6266\n",
      "Epoch 65/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0242 - mse: 1605.3325 - val_loss: 25.2302 - val_mse: 1726.4381\n",
      "Epoch 66/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2629 - mse: 1613.8704 - val_loss: 25.2676 - val_mse: 1783.4116\n",
      "Epoch 67/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1675 - mse: 1631.0907 - val_loss: 25.2444 - val_mse: 1772.4222\n",
      "Epoch 68/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0507 - mse: 1611.2766 - val_loss: 25.2176 - val_mse: 1754.2465\n",
      "Epoch 69/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0564 - mse: 1604.0750 - val_loss: 25.2632 - val_mse: 1779.6761\n",
      "Epoch 70/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.5776 - mse: 1684.0691 - val_loss: 25.3000 - val_mse: 1699.6456\n",
      "Epoch 71/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9701 - mse: 1573.9408 - val_loss: 25.2657 - val_mse: 1778.7954\n",
      "Epoch 72/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.1341 - mse: 1588.1813 - val_loss: 25.2480 - val_mse: 1713.7067\n",
      "Epoch 73/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2004 - mse: 1670.3905 - val_loss: 25.2350 - val_mse: 1761.6761\n",
      "Epoch 74/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1635 - mse: 1605.4495 - val_loss: 25.2178 - val_mse: 1748.4366\n",
      "Epoch 75/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1815 - mse: 1582.8093 - val_loss: 25.2212 - val_mse: 1751.4519\n",
      "Epoch 76/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3463 - mse: 1639.5631 - val_loss: 25.2127 - val_mse: 1736.3085\n",
      "Epoch 77/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8062 - mse: 1577.1165 - val_loss: 25.2283 - val_mse: 1755.0642\n",
      "Epoch 78/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2755 - mse: 1622.5707 - val_loss: 25.2505 - val_mse: 1767.0005\n",
      "Epoch 79/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4658 - mse: 1677.8923 - val_loss: 25.2182 - val_mse: 1744.4795\n",
      "Epoch 80/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8442 - mse: 1546.3771 - val_loss: 25.2500 - val_mse: 1765.6858\n",
      "Epoch 81/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1005 - mse: 1608.3553 - val_loss: 25.2214 - val_mse: 1747.1929\n",
      "Epoch 82/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.4930 - mse: 1657.4457 - val_loss: 25.2235 - val_mse: 1747.5570\n",
      "Epoch 83/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0202 - mse: 1583.1134 - val_loss: 25.2236 - val_mse: 1747.5753\n",
      "Epoch 84/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9889 - mse: 1603.4094 - val_loss: 25.2295 - val_mse: 1752.0956\n",
      "Epoch 85/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.5714 - mse: 1643.6587 - val_loss: 25.2220 - val_mse: 1744.5107\n",
      "Epoch 86/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6885 - mse: 1550.2031 - val_loss: 25.2197 - val_mse: 1725.8955\n",
      "Epoch 87/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2688 - mse: 1621.0771 - val_loss: 25.2985 - val_mse: 1693.8877\n",
      "Epoch 88/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2450 - mse: 1633.1924 - val_loss: 25.2163 - val_mse: 1733.2335\n",
      "Epoch 89/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9316 - mse: 1589.6013 - val_loss: 25.2287 - val_mse: 1749.7090\n",
      "Epoch 90/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4743 - mse: 1649.3630 - val_loss: 25.2248 - val_mse: 1746.2340\n",
      "Epoch 91/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3019 - mse: 1651.2219 - val_loss: 25.2317 - val_mse: 1751.1205\n",
      "Epoch 92/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8699 - mse: 1555.6942 - val_loss: 25.2196 - val_mse: 1737.3080\n",
      "Epoch 93/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1061 - mse: 1603.0538 - val_loss: 25.2311 - val_mse: 1717.2773\n",
      "Epoch 94/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2498 - mse: 1624.4556 - val_loss: 25.2449 - val_mse: 1709.3815\n",
      "Epoch 95/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9514 - mse: 1576.5797 - val_loss: 25.2190 - val_mse: 1733.0914\n",
      "Epoch 96/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2611 - mse: 1614.6188 - val_loss: 25.2265 - val_mse: 1720.8800\n",
      "Epoch 97/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0465 - mse: 1590.2681 - val_loss: 25.2559 - val_mse: 1764.5845\n",
      "Epoch 98/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1942 - mse: 1618.9265 - val_loss: 25.2248 - val_mse: 1722.0968\n",
      "Epoch 99/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2058 - mse: 1604.5909 - val_loss: 25.2369 - val_mse: 1712.8530\n",
      "Epoch 100/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0794 - mse: 1603.9755 - val_loss: 25.2756 - val_mse: 1772.2472\n",
      "Epoch 101/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.1339 - mse: 1633.6448 - val_loss: 25.2249 - val_mse: 1743.2998\n",
      "Epoch 102/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0025 - mse: 1570.7094 - val_loss: 25.2200 - val_mse: 1732.2397\n",
      "Epoch 103/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1960 - mse: 1633.8702 - val_loss: 25.2268 - val_mse: 1745.6482\n",
      "Epoch 104/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2774 - mse: 1621.4899 - val_loss: 25.2228 - val_mse: 1725.0212\n",
      "Epoch 105/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0127 - mse: 1588.1313 - val_loss: 25.2196 - val_mse: 1734.0283\n",
      "Epoch 106/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1160 - mse: 1614.4655 - val_loss: 25.2206 - val_mse: 1735.5735\n",
      "Epoch 107/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1675 - mse: 1604.1077 - val_loss: 25.2662 - val_mse: 1768.3938\n",
      "Epoch 108/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1241 - mse: 1600.6246 - val_loss: 25.2293 - val_mse: 1747.9354\n",
      "Epoch 109/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0286 - mse: 1595.8342 - val_loss: 25.2295 - val_mse: 1748.2842\n",
      "Epoch 110/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0718 - mse: 1599.3387 - val_loss: 25.2210 - val_mse: 1737.6196\n",
      "Epoch 111/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.1496 - mse: 1608.6145 - val_loss: 25.2327 - val_mse: 1716.1569\n",
      "Epoch 112/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.1673 - mse: 1605.6361 - val_loss: 25.2816 - val_mse: 1775.2347\n",
      "Epoch 113/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0888 - mse: 1600.8336 - val_loss: 25.2398 - val_mse: 1755.5374\n",
      "Epoch 114/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1023 - mse: 1603.7130 - val_loss: 25.2496 - val_mse: 1706.4860\n",
      "Epoch 115/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2396 - mse: 1624.8701 - val_loss: 25.2370 - val_mse: 1712.6836\n",
      "Epoch 116/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9585 - mse: 1568.2444 - val_loss: 25.2765 - val_mse: 1773.1820\n",
      "Epoch 117/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3945 - mse: 1651.6572 - val_loss: 25.2202 - val_mse: 1728.9761\n",
      "Epoch 118/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0353 - mse: 1597.0830 - val_loss: 25.2648 - val_mse: 1701.5630\n",
      "Epoch 119/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.9961 - mse: 1587.7021 - val_loss: 25.2474 - val_mse: 1760.3834\n",
      "Epoch 120/500\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 24.2114 - mse: 1619.5322 - val_loss: 25.2314 - val_mse: 1716.6986\n",
      "Epoch 121/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1699 - mse: 1621.9771 - val_loss: 25.2763 - val_mse: 1773.3247\n",
      "Epoch 122/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0012 - mse: 1570.5538 - val_loss: 25.2225 - val_mse: 1725.0198\n",
      "Epoch 123/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0195 - mse: 1605.5647 - val_loss: 25.2389 - val_mse: 1711.4655\n",
      "Epoch 124/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0862 - mse: 1596.0232 - val_loss: 25.2189 - val_mse: 1731.2814\n",
      "Epoch 125/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0238 - mse: 1593.4138 - val_loss: 25.2562 - val_mse: 1764.7041\n",
      "Epoch 126/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3514 - mse: 1639.9147 - val_loss: 25.2206 - val_mse: 1740.4192\n",
      "Epoch 127/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9911 - mse: 1608.8210 - val_loss: 25.2485 - val_mse: 1761.4376\n",
      "Epoch 128/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1289 - mse: 1611.2596 - val_loss: 25.2184 - val_mse: 1732.5034\n",
      "Epoch 129/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1714 - mse: 1598.6165 - val_loss: 25.2190 - val_mse: 1731.6202\n",
      "Epoch 130/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2799 - mse: 1627.5033 - val_loss: 25.2193 - val_mse: 1735.1481\n",
      "Epoch 131/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9554 - mse: 1588.4473 - val_loss: 25.2954 - val_mse: 1780.7687\n",
      "Epoch 132/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1150 - mse: 1603.7408 - val_loss: 25.2196 - val_mse: 1736.1555\n",
      "Epoch 133/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0002 - mse: 1586.5187 - val_loss: 25.2200 - val_mse: 1738.9857\n",
      "Epoch 134/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1595 - mse: 1604.1189 - val_loss: 25.2372 - val_mse: 1754.8838\n",
      "Epoch 135/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0876 - mse: 1586.1387 - val_loss: 25.2665 - val_mse: 1769.3336\n",
      "Epoch 136/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3871 - mse: 1663.1971 - val_loss: 25.2502 - val_mse: 1761.7888\n",
      "Epoch 137/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9046 - mse: 1584.7483 - val_loss: 25.2199 - val_mse: 1736.7152\n",
      "Epoch 138/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1800 - mse: 1606.5448 - val_loss: 25.3178 - val_mse: 1788.7727\n",
      "Epoch 139/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0471 - mse: 1603.1027 - val_loss: 25.2206 - val_mse: 1728.4352\n",
      "Epoch 140/500\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 24.2116 - mse: 1614.6290 - val_loss: 25.2341 - val_mse: 1752.9188\n",
      "Epoch 141/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.1518 - mse: 1605.8188 - val_loss: 25.2368 - val_mse: 1755.1481\n",
      "Epoch 142/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5909 - mse: 1540.9053 - val_loss: 25.2251 - val_mse: 1747.1024\n",
      "Epoch 143/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3983 - mse: 1631.9053 - val_loss: 25.2310 - val_mse: 1716.4148\n",
      "Epoch 144/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9584 - mse: 1599.1888 - val_loss: 25.2332 - val_mse: 1752.4410\n",
      "Epoch 145/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.4648 - mse: 1661.7554 - val_loss: 25.2494 - val_mse: 1762.1038\n",
      "Epoch 146/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.1815 - mse: 1615.3065 - val_loss: 25.2776 - val_mse: 1697.3733\n",
      "Epoch 147/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8614 - mse: 1551.2266 - val_loss: 25.2245 - val_mse: 1746.6908\n",
      "Epoch 148/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3469 - mse: 1646.1962 - val_loss: 25.2238 - val_mse: 1723.1270\n",
      "Epoch 149/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0928 - mse: 1611.2749 - val_loss: 25.2603 - val_mse: 1767.3916\n",
      "Epoch 150/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9912 - mse: 1566.7935 - val_loss: 25.2198 - val_mse: 1738.5775\n",
      "Epoch 151/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.9472 - mse: 1588.5906 - val_loss: 25.2182 - val_mse: 1736.8032\n",
      "Epoch 152/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.0688 - mse: 1618.6954 - val_loss: 25.2571 - val_mse: 1765.9282\n",
      "Epoch 153/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2308 - mse: 1625.6406 - val_loss: 25.2195 - val_mse: 1730.8369\n",
      "Epoch 154/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.2911 - mse: 1626.4662 - val_loss: 25.2450 - val_mse: 1760.4468\n",
      "Epoch 155/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.1118 - mse: 1596.6262 - val_loss: 25.2775 - val_mse: 1774.6796\n",
      "Epoch 156/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7817 - mse: 1559.9751 - val_loss: 25.2937 - val_mse: 1781.5461\n",
      "Epoch 157/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.5057 - mse: 1658.9607 - val_loss: 25.2664 - val_mse: 1770.2954\n",
      "Epoch 158/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7836 - mse: 1583.8898 - val_loss: 25.2928 - val_mse: 1781.0800\n",
      "Epoch 159/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1605 - mse: 1595.6541 - val_loss: 25.2557 - val_mse: 1704.3424\n",
      "Epoch 160/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9588 - mse: 1583.8568 - val_loss: 25.3540 - val_mse: 1800.7418\n",
      "Epoch 161/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2373 - mse: 1635.4020 - val_loss: 25.2561 - val_mse: 1765.4375\n",
      "Epoch 162/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2170 - mse: 1628.3453 - val_loss: 25.2192 - val_mse: 1737.7454\n",
      "Epoch 163/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1653 - mse: 1621.0677 - val_loss: 25.2239 - val_mse: 1722.5500\n",
      "Epoch 164/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0517 - mse: 1578.8661 - val_loss: 25.3207 - val_mse: 1790.5562\n",
      "Epoch 165/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7830 - mse: 1548.5802 - val_loss: 25.2258 - val_mse: 1749.2134\n",
      "Epoch 166/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2767 - mse: 1666.3320 - val_loss: 25.2255 - val_mse: 1721.4633\n",
      "Epoch 167/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2267 - mse: 1597.0322 - val_loss: 25.2291 - val_mse: 1718.6637\n",
      "Epoch 168/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5836 - mse: 1529.2375 - val_loss: 25.2215 - val_mse: 1727.8490\n",
      "Epoch 169/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3379 - mse: 1666.5365 - val_loss: 25.3088 - val_mse: 1786.9495\n",
      "Epoch 170/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2814 - mse: 1619.4602 - val_loss: 25.2498 - val_mse: 1762.1227\n",
      "Epoch 171/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9089 - mse: 1559.5056 - val_loss: 25.2657 - val_mse: 1769.9565\n",
      "Epoch 172/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3902 - mse: 1667.7612 - val_loss: 25.2198 - val_mse: 1738.4890\n",
      "Epoch 173/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.5878 - mse: 1522.0988 - val_loss: 25.2491 - val_mse: 1762.5048\n",
      "Epoch 174/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.3239 - mse: 1651.7695 - val_loss: 25.2539 - val_mse: 1705.4354\n",
      "Epoch 175/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8585 - mse: 1570.9464 - val_loss: 25.2189 - val_mse: 1732.8690\n",
      "Epoch 176/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2007 - mse: 1641.5588 - val_loss: 25.2918 - val_mse: 1780.2382\n",
      "Epoch 177/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.1651 - mse: 1602.2328 - val_loss: 25.2595 - val_mse: 1767.6715\n",
      "Epoch 178/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.1038 - mse: 1618.2406 - val_loss: 25.2251 - val_mse: 1749.1506\n",
      "Epoch 179/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9294 - mse: 1547.1024 - val_loss: 25.2190 - val_mse: 1735.4216\n",
      "Epoch 180/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2031 - mse: 1628.1937 - val_loss: 25.2471 - val_mse: 1762.0455\n",
      "Epoch 181/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1645 - mse: 1613.5330 - val_loss: 25.2196 - val_mse: 1738.0151\n",
      "Epoch 182/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0314 - mse: 1590.7524 - val_loss: 25.2275 - val_mse: 1750.7792\n",
      "Epoch 183/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9356 - mse: 1571.7230 - val_loss: 25.2752 - val_mse: 1774.7804\n",
      "Epoch 184/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2955 - mse: 1657.8882 - val_loss: 25.2366 - val_mse: 1756.4313\n",
      "Epoch 185/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.2623 - mse: 1623.2603 - val_loss: 25.2298 - val_mse: 1718.3291\n",
      "Epoch 186/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9627 - mse: 1567.7346 - val_loss: 25.2405 - val_mse: 1758.0776\n",
      "Epoch 187/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9271 - mse: 1597.5529 - val_loss: 25.2627 - val_mse: 1769.7750\n",
      "Epoch 188/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2875 - mse: 1637.1224 - val_loss: 25.2196 - val_mse: 1740.0439\n",
      "Epoch 189/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6654 - mse: 1548.8708 - val_loss: 25.2189 - val_mse: 1731.2689\n",
      "Epoch 190/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4196 - mse: 1663.1843 - val_loss: 25.2189 - val_mse: 1740.2853\n",
      "Epoch 191/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.1812 - mse: 1597.1575 - val_loss: 25.2211 - val_mse: 1744.8313\n",
      "Epoch 192/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0710 - mse: 1602.9979 - val_loss: 25.2680 - val_mse: 1772.1135\n",
      "Epoch 193/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0573 - mse: 1621.8806 - val_loss: 25.2179 - val_mse: 1734.2546\n",
      "Epoch 194/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9771 - mse: 1567.5148 - val_loss: 25.2341 - val_mse: 1755.5328\n",
      "Epoch 195/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.9747 - mse: 1631.8793 - val_loss: 25.3224 - val_mse: 1791.8862\n",
      "Epoch 196/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.0414 - mse: 1560.0157 - val_loss: 25.2611 - val_mse: 1770.1095\n",
      "Epoch 197/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8922 - mse: 1560.5159 - val_loss: 25.2205 - val_mse: 1727.9019\n",
      "Epoch 198/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4200 - mse: 1674.1703 - val_loss: 25.2522 - val_mse: 1706.6749\n",
      "Epoch 199/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.0291 - mse: 1605.1833 - val_loss: 25.2290 - val_mse: 1718.5922\n",
      "Epoch 200/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.1192 - mse: 1597.0767 - val_loss: 25.2682 - val_mse: 1700.6270\n",
      "Epoch 201/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.1251 - mse: 1627.4839 - val_loss: 25.2396 - val_mse: 1759.1708\n",
      "Epoch 202/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9029 - mse: 1572.4453 - val_loss: 25.2178 - val_mse: 1738.8815\n",
      "Epoch 203/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0800 - mse: 1593.8494 - val_loss: 25.2215 - val_mse: 1726.5764\n",
      "Epoch 204/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0725 - mse: 1596.2709 - val_loss: 25.3258 - val_mse: 1793.6115\n",
      "Epoch 205/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0568 - mse: 1616.8492 - val_loss: 25.2447 - val_mse: 1762.2725\n",
      "Epoch 206/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0673 - mse: 1599.6501 - val_loss: 25.2201 - val_mse: 1731.0134\n",
      "Epoch 207/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1155 - mse: 1611.9991 - val_loss: 25.2276 - val_mse: 1719.3688\n",
      "Epoch 208/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0260 - mse: 1590.6907 - val_loss: 25.2606 - val_mse: 1770.1100\n",
      "Epoch 209/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1214 - mse: 1589.8408 - val_loss: 25.2185 - val_mse: 1742.0443\n",
      "Epoch 210/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0397 - mse: 1616.6910 - val_loss: 25.3333 - val_mse: 1796.3037\n",
      "Epoch 211/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0783 - mse: 1599.2124 - val_loss: 25.2365 - val_mse: 1714.3601\n",
      "Epoch 212/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3054 - mse: 1638.5320 - val_loss: 25.2186 - val_mse: 1741.5365\n",
      "Epoch 213/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8554 - mse: 1584.5586 - val_loss: 25.2211 - val_mse: 1726.9185\n",
      "Epoch 214/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0296 - mse: 1591.2233 - val_loss: 25.2182 - val_mse: 1741.7343\n",
      "Epoch 215/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8867 - mse: 1574.9860 - val_loss: 25.2233 - val_mse: 1723.9683\n",
      "Epoch 216/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3328 - mse: 1649.0272 - val_loss: 25.2266 - val_mse: 1721.3967\n",
      "Epoch 217/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9126 - mse: 1584.1270 - val_loss: 25.2183 - val_mse: 1740.7697\n",
      "Epoch 218/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0814 - mse: 1588.8793 - val_loss: 25.2663 - val_mse: 1772.7343\n",
      "Epoch 219/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0318 - mse: 1617.8085 - val_loss: 25.2461 - val_mse: 1763.0771\n",
      "Epoch 220/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1511 - mse: 1614.0552 - val_loss: 25.2324 - val_mse: 1755.7491\n",
      "Epoch 221/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0720 - mse: 1604.4032 - val_loss: 25.2389 - val_mse: 1760.0299\n",
      "Epoch 222/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1092 - mse: 1615.3761 - val_loss: 25.2309 - val_mse: 1717.5220\n",
      "Epoch 223/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.0543 - mse: 1599.0490 - val_loss: 25.2265 - val_mse: 1752.0159\n",
      "Epoch 224/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.0591 - mse: 1604.5571 - val_loss: 25.2180 - val_mse: 1740.3656\n",
      "Epoch 225/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0282 - mse: 1599.4447 - val_loss: 25.2202 - val_mse: 1745.6908\n",
      "Epoch 226/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0068 - mse: 1594.5626 - val_loss: 25.2407 - val_mse: 1761.3726\n",
      "Epoch 227/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2081 - mse: 1628.2637 - val_loss: 25.2347 - val_mse: 1715.4152\n",
      "Epoch 228/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9918 - mse: 1587.5127 - val_loss: 25.2181 - val_mse: 1740.4622\n",
      "Epoch 229/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0340 - mse: 1600.6903 - val_loss: 25.2192 - val_mse: 1734.6141\n",
      "Epoch 230/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1122 - mse: 1610.5308 - val_loss: 25.2191 - val_mse: 1741.2247\n",
      "Epoch 231/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1435 - mse: 1613.8004 - val_loss: 25.2213 - val_mse: 1745.7258\n",
      "Epoch 232/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0387 - mse: 1614.0389 - val_loss: 25.2404 - val_mse: 1760.3433\n",
      "Epoch 233/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0236 - mse: 1593.0500 - val_loss: 25.2191 - val_mse: 1734.9788\n",
      "Epoch 234/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0522 - mse: 1589.7330 - val_loss: 25.2207 - val_mse: 1745.5972\n",
      "Epoch 235/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8636 - mse: 1564.7617 - val_loss: 25.3107 - val_mse: 1789.8132\n",
      "Epoch 236/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3094 - mse: 1644.7565 - val_loss: 25.2211 - val_mse: 1745.0709\n",
      "Epoch 237/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.9303 - mse: 1583.3308 - val_loss: 25.3939 - val_mse: 1813.8224\n",
      "Epoch 238/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1226 - mse: 1606.4753 - val_loss: 25.2366 - val_mse: 1758.9025\n",
      "Epoch 239/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1054 - mse: 1622.2450 - val_loss: 25.2231 - val_mse: 1747.6619\n",
      "Epoch 240/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9662 - mse: 1592.4940 - val_loss: 25.3175 - val_mse: 1791.9983\n",
      "Epoch 241/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1807 - mse: 1625.8569 - val_loss: 25.2589 - val_mse: 1769.7303\n",
      "Epoch 242/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0576 - mse: 1580.2125 - val_loss: 25.2196 - val_mse: 1735.8320\n",
      "Epoch 243/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9269 - mse: 1595.1758 - val_loss: 25.3212 - val_mse: 1793.0907\n",
      "Epoch 244/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3918 - mse: 1652.8480 - val_loss: 25.2256 - val_mse: 1723.6074\n",
      "Epoch 245/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3533 - mse: 1649.2062 - val_loss: 25.2526 - val_mse: 1706.7107\n",
      "Epoch 246/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.5152 - mse: 1532.5450 - val_loss: 25.2975 - val_mse: 1784.8094\n",
      "Epoch 247/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9534 - mse: 1568.0165 - val_loss: 25.2194 - val_mse: 1740.4861\n",
      "Epoch 248/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1313 - mse: 1640.4375 - val_loss: 25.2306 - val_mse: 1719.2560\n",
      "Epoch 249/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4124 - mse: 1649.7543 - val_loss: 25.2846 - val_mse: 1697.2628\n",
      "Epoch 250/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7612 - mse: 1548.3369 - val_loss: 25.2304 - val_mse: 1754.1327\n",
      "Epoch 251/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.8916 - mse: 1601.6307 - val_loss: 25.2211 - val_mse: 1731.7148\n",
      "Epoch 252/500\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 23.9827 - mse: 1564.8763 - val_loss: 25.2344 - val_mse: 1756.9647\n",
      "Epoch 253/500\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 24.3092 - mse: 1632.5461 - val_loss: 25.2342 - val_mse: 1717.0426\n",
      "Epoch 254/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.9908 - mse: 1635.3949 - val_loss: 25.2710 - val_mse: 1775.1852\n",
      "Epoch 255/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.2071 - mse: 1611.1014 - val_loss: 25.2396 - val_mse: 1713.6143\n",
      "Epoch 256/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.5683 - mse: 1537.7231 - val_loss: 25.2203 - val_mse: 1743.5232\n",
      "Epoch 257/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.4844 - mse: 1640.6049 - val_loss: 25.2498 - val_mse: 1765.2357\n",
      "Epoch 258/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.8488 - mse: 1587.2480 - val_loss: 25.2212 - val_mse: 1742.4203\n",
      "Epoch 259/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.0033 - mse: 1579.3357 - val_loss: 25.2319 - val_mse: 1755.4166\n",
      "Epoch 260/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.9344 - mse: 1620.2953 - val_loss: 25.2249 - val_mse: 1748.9817\n",
      "Epoch 261/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.2603 - mse: 1633.7277 - val_loss: 25.2298 - val_mse: 1719.9036\n",
      "Epoch 262/500\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 24.0532 - mse: 1598.9958 - val_loss: 25.2217 - val_mse: 1731.4352\n",
      "Epoch 263/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.9227 - mse: 1578.1003 - val_loss: 25.2919 - val_mse: 1783.7910\n",
      "Epoch 264/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.5612 - mse: 1665.6931 - val_loss: 25.2708 - val_mse: 1775.6062\n",
      "Epoch 265/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4444 - mse: 1543.0895 - val_loss: 25.2201 - val_mse: 1735.8533\n",
      "Epoch 266/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3695 - mse: 1632.5793 - val_loss: 25.2329 - val_mse: 1717.8187\n",
      "Epoch 267/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.7993 - mse: 1541.2344 - val_loss: 25.2225 - val_mse: 1728.6232\n",
      "Epoch 268/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2224 - mse: 1683.4594 - val_loss: 25.2239 - val_mse: 1747.0001\n",
      "Epoch 269/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0645 - mse: 1573.7064 - val_loss: 25.2194 - val_mse: 1737.2926\n",
      "Epoch 270/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7595 - mse: 1558.0967 - val_loss: 25.2313 - val_mse: 1755.3450\n",
      "Epoch 271/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3883 - mse: 1652.6926 - val_loss: 25.2560 - val_mse: 1769.7294\n",
      "Epoch 272/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9718 - mse: 1571.5811 - val_loss: 25.2213 - val_mse: 1743.5702\n",
      "Epoch 273/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8080 - mse: 1599.1409 - val_loss: 25.2262 - val_mse: 1749.7509\n",
      "Epoch 274/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4421 - mse: 1671.6145 - val_loss: 25.2207 - val_mse: 1732.5929\n",
      "Epoch 275/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1478 - mse: 1614.4152 - val_loss: 25.2381 - val_mse: 1715.0046\n",
      "Epoch 276/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6518 - mse: 1543.8116 - val_loss: 25.3560 - val_mse: 1804.5673\n",
      "Epoch 277/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1002 - mse: 1584.2819 - val_loss: 25.2221 - val_mse: 1728.3770\n",
      "Epoch 278/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0530 - mse: 1598.4985 - val_loss: 25.2408 - val_mse: 1760.7634\n",
      "Epoch 279/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3961 - mse: 1671.2286 - val_loss: 25.2283 - val_mse: 1721.4136\n",
      "Epoch 280/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6549 - mse: 1551.4502 - val_loss: 25.2919 - val_mse: 1784.8971\n",
      "Epoch 281/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3765 - mse: 1658.0452 - val_loss: 25.2189 - val_mse: 1735.3140\n",
      "Epoch 282/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8700 - mse: 1560.8156 - val_loss: 25.2594 - val_mse: 1771.1447\n",
      "Epoch 283/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4504 - mse: 1656.8027 - val_loss: 25.2573 - val_mse: 1706.7241\n",
      "Epoch 284/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6940 - mse: 1548.4836 - val_loss: 25.2229 - val_mse: 1744.7407\n",
      "Epoch 285/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5989 - mse: 1576.9207 - val_loss: 25.2728 - val_mse: 1776.8434\n",
      "Epoch 286/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.1941 - mse: 1634.0000 - val_loss: 25.2205 - val_mse: 1737.9701\n",
      "Epoch 287/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3352 - mse: 1634.1282 - val_loss: 25.2280 - val_mse: 1722.1934\n",
      "Epoch 288/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8624 - mse: 1565.7067 - val_loss: 25.2219 - val_mse: 1730.9167\n",
      "Epoch 289/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0558 - mse: 1591.5106 - val_loss: 25.2891 - val_mse: 1782.2197\n",
      "Epoch 290/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2495 - mse: 1624.4150 - val_loss: 25.3423 - val_mse: 1800.0870\n",
      "Epoch 291/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9084 - mse: 1619.1688 - val_loss: 25.2558 - val_mse: 1769.4177\n",
      "Epoch 292/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9129 - mse: 1596.9801 - val_loss: 25.2297 - val_mse: 1720.6149\n",
      "Epoch 293/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2431 - mse: 1617.4421 - val_loss: 25.2259 - val_mse: 1725.4596\n",
      "Epoch 294/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5825 - mse: 1537.9712 - val_loss: 25.2511 - val_mse: 1767.4392\n",
      "Epoch 295/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4663 - mse: 1639.3103 - val_loss: 25.2311 - val_mse: 1720.0543\n",
      "Epoch 296/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1613 - mse: 1635.2799 - val_loss: 25.2459 - val_mse: 1712.0630\n",
      "Epoch 297/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0672 - mse: 1599.0389 - val_loss: 25.2280 - val_mse: 1723.4144\n",
      "Epoch 298/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5079 - mse: 1546.2614 - val_loss: 25.2218 - val_mse: 1739.8584\n",
      "Epoch 299/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.7595 - mse: 1688.7513 - val_loss: 25.2256 - val_mse: 1727.1963\n",
      "Epoch 300/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4982 - mse: 1541.0028 - val_loss: 25.3288 - val_mse: 1796.4717\n",
      "Epoch 301/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9507 - mse: 1600.5709 - val_loss: 25.2652 - val_mse: 1704.9020\n",
      "Epoch 302/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0450 - mse: 1594.0535 - val_loss: 25.2329 - val_mse: 1755.4775\n",
      "Epoch 303/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.3318 - mse: 1662.3583 - val_loss: 25.2261 - val_mse: 1748.0642\n",
      "Epoch 304/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.9254 - mse: 1557.7292 - val_loss: 25.3155 - val_mse: 1792.0050\n",
      "Epoch 305/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2879 - mse: 1639.9805 - val_loss: 25.2430 - val_mse: 1713.9365\n",
      "Epoch 306/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7375 - mse: 1577.0996 - val_loss: 25.2642 - val_mse: 1773.0118\n",
      "Epoch 307/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8303 - mse: 1556.1322 - val_loss: 25.2314 - val_mse: 1721.2112\n",
      "Epoch 308/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2698 - mse: 1655.1587 - val_loss: 25.2300 - val_mse: 1722.9520\n",
      "Epoch 309/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0000 - mse: 1594.2936 - val_loss: 25.2250 - val_mse: 1727.2583\n",
      "Epoch 310/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0581 - mse: 1618.4877 - val_loss: 25.2962 - val_mse: 1785.7683\n",
      "Epoch 311/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1355 - mse: 1610.3567 - val_loss: 25.2347 - val_mse: 1756.2946\n",
      "Epoch 312/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9215 - mse: 1590.7042 - val_loss: 25.2565 - val_mse: 1769.5172\n",
      "Epoch 313/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1665 - mse: 1601.8188 - val_loss: 25.2425 - val_mse: 1762.0281\n",
      "Epoch 314/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9880 - mse: 1625.1903 - val_loss: 25.2250 - val_mse: 1728.1494\n",
      "Epoch 315/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8573 - mse: 1551.5931 - val_loss: 25.2238 - val_mse: 1744.1011\n",
      "Epoch 316/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2374 - mse: 1635.0437 - val_loss: 25.2440 - val_mse: 1713.8269\n",
      "Epoch 317/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7505 - mse: 1573.1251 - val_loss: 25.2211 - val_mse: 1738.2883\n",
      "Epoch 318/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0055 - mse: 1593.3132 - val_loss: 25.2621 - val_mse: 1771.6476\n",
      "Epoch 319/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2680 - mse: 1635.9541 - val_loss: 25.2383 - val_mse: 1758.5425\n",
      "Epoch 320/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1343 - mse: 1624.8812 - val_loss: 25.2214 - val_mse: 1734.9418\n",
      "Epoch 321/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8191 - mse: 1563.8231 - val_loss: 25.2347 - val_mse: 1755.7120\n",
      "Epoch 322/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2629 - mse: 1644.8256 - val_loss: 25.2309 - val_mse: 1722.5183\n",
      "Epoch 323/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9095 - mse: 1577.9570 - val_loss: 25.3153 - val_mse: 1792.2681\n",
      "Epoch 324/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9204 - mse: 1586.9473 - val_loss: 25.2779 - val_mse: 1778.6702\n",
      "Epoch 325/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0304 - mse: 1602.9512 - val_loss: 25.2326 - val_mse: 1752.3839\n",
      "Epoch 326/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0518 - mse: 1615.0083 - val_loss: 25.2345 - val_mse: 1720.1554\n",
      "Epoch 327/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1125 - mse: 1610.3014 - val_loss: 25.2228 - val_mse: 1732.2844\n",
      "Epoch 328/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9826 - mse: 1616.5835 - val_loss: 25.3464 - val_mse: 1802.2653\n",
      "Epoch 329/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0210 - mse: 1578.5884 - val_loss: 25.2375 - val_mse: 1757.5865\n",
      "Epoch 330/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9758 - mse: 1591.1287 - val_loss: 25.2304 - val_mse: 1722.4468\n",
      "Epoch 331/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8894 - mse: 1600.1927 - val_loss: 25.2235 - val_mse: 1741.0579\n",
      "Epoch 332/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2172 - mse: 1626.6359 - val_loss: 25.2511 - val_mse: 1711.0691\n",
      "Epoch 333/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9830 - mse: 1586.3699 - val_loss: 25.2352 - val_mse: 1719.5225\n",
      "Epoch 334/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0463 - mse: 1614.1744 - val_loss: 25.3922 - val_mse: 1814.6006\n",
      "Epoch 335/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0263 - mse: 1602.4546 - val_loss: 25.3946 - val_mse: 1815.2142\n",
      "Epoch 336/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0479 - mse: 1611.9177 - val_loss: 25.2572 - val_mse: 1708.5503\n",
      "Epoch 337/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0108 - mse: 1602.6714 - val_loss: 25.2344 - val_mse: 1720.1836\n",
      "Epoch 338/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0658 - mse: 1601.2573 - val_loss: 25.2338 - val_mse: 1720.6418\n",
      "Epoch 339/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0190 - mse: 1615.4521 - val_loss: 25.2293 - val_mse: 1750.3826\n",
      "Epoch 340/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0238 - mse: 1601.2279 - val_loss: 25.2448 - val_mse: 1762.9297\n",
      "Epoch 341/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9393 - mse: 1591.7865 - val_loss: 25.2234 - val_mse: 1742.3976\n",
      "Epoch 342/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0712 - mse: 1602.9404 - val_loss: 25.2726 - val_mse: 1776.7936\n",
      "Epoch 343/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9461 - mse: 1596.7303 - val_loss: 25.2219 - val_mse: 1738.0100\n",
      "Epoch 344/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0749 - mse: 1603.9409 - val_loss: 25.2343 - val_mse: 1754.8313\n",
      "Epoch 345/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0711 - mse: 1611.8342 - val_loss: 25.2230 - val_mse: 1740.8417\n",
      "Epoch 346/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9739 - mse: 1595.8336 - val_loss: 25.3289 - val_mse: 1691.0659\n",
      "Epoch 347/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0283 - mse: 1601.1862 - val_loss: 25.2253 - val_mse: 1743.9208\n",
      "Epoch 348/500\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 23.7375 - mse: 1566.0703 - val_loss: 25.2285 - val_mse: 1749.1456\n",
      "Epoch 349/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3847 - mse: 1658.0049 - val_loss: 25.2291 - val_mse: 1749.0365\n",
      "Epoch 350/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0610 - mse: 1604.6549 - val_loss: 25.2224 - val_mse: 1736.4429\n",
      "Epoch 351/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6379 - mse: 1564.8066 - val_loss: 25.3020 - val_mse: 1787.8584\n",
      "Epoch 352/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3812 - mse: 1639.6854 - val_loss: 25.2832 - val_mse: 1780.6620\n",
      "Epoch 353/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8860 - mse: 1593.8048 - val_loss: 25.2323 - val_mse: 1751.7981\n",
      "Epoch 354/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9549 - mse: 1601.2793 - val_loss: 25.2271 - val_mse: 1726.9397\n",
      "Epoch 355/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0742 - mse: 1591.0013 - val_loss: 25.2413 - val_mse: 1759.3975\n",
      "Epoch 356/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8220 - mse: 1568.2767 - val_loss: 25.2462 - val_mse: 1762.9197\n",
      "Epoch 357/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4086 - mse: 1674.3121 - val_loss: 25.2402 - val_mse: 1718.0492\n",
      "Epoch 358/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7006 - mse: 1543.5280 - val_loss: 25.2709 - val_mse: 1775.2699\n",
      "Epoch 359/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0120 - mse: 1609.8718 - val_loss: 25.2775 - val_mse: 1778.6228\n",
      "Epoch 360/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0195 - mse: 1596.4613 - val_loss: 25.2355 - val_mse: 1754.5865\n",
      "Epoch 361/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9723 - mse: 1594.5057 - val_loss: 25.2313 - val_mse: 1750.5538\n",
      "Epoch 362/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8434 - mse: 1588.0125 - val_loss: 25.2249 - val_mse: 1730.2607\n",
      "Epoch 363/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4813 - mse: 1653.3403 - val_loss: 25.3502 - val_mse: 1803.6493\n",
      "Epoch 364/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5667 - mse: 1564.0933 - val_loss: 25.2437 - val_mse: 1761.1349\n",
      "Epoch 365/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2813 - mse: 1625.7932 - val_loss: 25.2229 - val_mse: 1737.8641\n",
      "Epoch 366/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2003 - mse: 1630.9596 - val_loss: 25.2227 - val_mse: 1737.0833\n",
      "Epoch 367/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8877 - mse: 1580.3306 - val_loss: 25.2340 - val_mse: 1722.5284\n",
      "Epoch 368/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2068 - mse: 1626.6344 - val_loss: 25.2385 - val_mse: 1720.4802\n",
      "Epoch 369/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9955 - mse: 1621.0665 - val_loss: 25.2363 - val_mse: 1755.6522\n",
      "Epoch 370/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4051 - mse: 1494.9949 - val_loss: 25.2253 - val_mse: 1743.9585\n",
      "Epoch 371/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.6550 - mse: 1711.0596 - val_loss: 25.2385 - val_mse: 1757.0948\n",
      "Epoch 372/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9520 - mse: 1580.1672 - val_loss: 25.2927 - val_mse: 1784.8466\n",
      "Epoch 373/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.1070 - mse: 1631.3088 - val_loss: 25.2261 - val_mse: 1728.7365\n",
      "Epoch 374/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6902 - mse: 1549.8297 - val_loss: 25.2270 - val_mse: 1728.8086\n",
      "Epoch 375/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.2823 - mse: 1661.7401 - val_loss: 25.2256 - val_mse: 1743.3379\n",
      "Epoch 376/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.6838 - mse: 1553.1392 - val_loss: 25.2251 - val_mse: 1741.6744\n",
      "Epoch 377/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2558 - mse: 1630.5461 - val_loss: 25.3194 - val_mse: 1794.3278\n",
      "Epoch 378/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.8864 - mse: 1565.6704 - val_loss: 25.2369 - val_mse: 1754.8335\n",
      "Epoch 379/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1194 - mse: 1635.5588 - val_loss: 25.2314 - val_mse: 1749.1167\n",
      "Epoch 380/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.1869 - mse: 1608.2738 - val_loss: 25.2284 - val_mse: 1745.5028\n",
      "Epoch 381/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0740 - mse: 1637.2518 - val_loss: 25.2447 - val_mse: 1760.7784\n",
      "Epoch 382/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8476 - mse: 1582.8478 - val_loss: 25.2430 - val_mse: 1717.6298\n",
      "Epoch 383/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9661 - mse: 1576.2834 - val_loss: 25.2608 - val_mse: 1770.1415\n",
      "Epoch 384/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7198 - mse: 1579.4829 - val_loss: 25.2382 - val_mse: 1756.3124\n",
      "Epoch 385/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9513 - mse: 1581.1868 - val_loss: 25.2574 - val_mse: 1769.2301\n",
      "Epoch 386/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.5271 - mse: 1696.5247 - val_loss: 25.2769 - val_mse: 1778.1864\n",
      "Epoch 387/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.7726 - mse: 1550.2189 - val_loss: 25.2680 - val_mse: 1707.8369\n",
      "Epoch 388/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2675 - mse: 1648.4109 - val_loss: 25.2684 - val_mse: 1706.8658\n",
      "Epoch 389/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5937 - mse: 1527.3153 - val_loss: 25.2245 - val_mse: 1739.8448\n",
      "Epoch 390/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.0456 - mse: 1595.8855 - val_loss: 25.2375 - val_mse: 1721.2378\n",
      "Epoch 391/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.9461 - mse: 1600.7523 - val_loss: 25.2842 - val_mse: 1781.1619\n",
      "Epoch 392/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8717 - mse: 1586.7197 - val_loss: 25.2635 - val_mse: 1771.9044\n",
      "Epoch 393/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1821 - mse: 1643.5529 - val_loss: 25.3127 - val_mse: 1791.7848\n",
      "Epoch 394/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3380 - mse: 1634.7452 - val_loss: 25.2258 - val_mse: 1741.2390\n",
      "Epoch 395/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9146 - mse: 1585.6022 - val_loss: 25.2452 - val_mse: 1717.3282\n",
      "Epoch 396/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8571 - mse: 1546.7108 - val_loss: 25.2446 - val_mse: 1761.6904\n",
      "Epoch 397/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.1380 - mse: 1647.5481 - val_loss: 25.2643 - val_mse: 1773.2288\n",
      "Epoch 398/500\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 24.0143 - mse: 1574.8309 - val_loss: 25.2250 - val_mse: 1735.5905\n",
      "Epoch 399/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.9760 - mse: 1642.7356 - val_loss: 25.2294 - val_mse: 1748.2351\n",
      "Epoch 400/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1576 - mse: 1600.7092 - val_loss: 25.2295 - val_mse: 1747.2413\n",
      "Epoch 401/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9977 - mse: 1601.5723 - val_loss: 25.2284 - val_mse: 1745.9260\n",
      "Epoch 402/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8252 - mse: 1581.0946 - val_loss: 25.2401 - val_mse: 1720.1088\n",
      "Epoch 403/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7811 - mse: 1577.0804 - val_loss: 25.2523 - val_mse: 1765.1807\n",
      "Epoch 404/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2794 - mse: 1641.1434 - val_loss: 25.3325 - val_mse: 1798.5360\n",
      "Epoch 405/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9344 - mse: 1567.7830 - val_loss: 25.2255 - val_mse: 1732.1046\n",
      "Epoch 406/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.0905 - mse: 1650.2969 - val_loss: 25.2456 - val_mse: 1762.1737\n",
      "Epoch 407/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.2727 - mse: 1642.4103 - val_loss: 25.2257 - val_mse: 1740.3162\n",
      "Epoch 408/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7875 - mse: 1555.2091 - val_loss: 25.2250 - val_mse: 1736.7224\n",
      "Epoch 409/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7533 - mse: 1554.5122 - val_loss: 25.2807 - val_mse: 1779.9777\n",
      "Epoch 410/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.2000 - mse: 1620.5991 - val_loss: 25.2652 - val_mse: 1709.6377\n",
      "Epoch 411/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1218 - mse: 1642.9877 - val_loss: 25.2649 - val_mse: 1771.4752\n",
      "Epoch 412/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2614 - mse: 1640.0247 - val_loss: 25.2264 - val_mse: 1736.8842\n",
      "Epoch 413/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4863 - mse: 1535.0605 - val_loss: 25.2466 - val_mse: 1761.2832\n",
      "Epoch 414/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0583 - mse: 1579.9575 - val_loss: 25.3514 - val_mse: 1804.0420\n",
      "Epoch 415/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4041 - mse: 1696.4180 - val_loss: 25.2279 - val_mse: 1742.5686\n",
      "Epoch 416/500\n",
      "113/113 [==============================] - 1s 4ms/step - loss: 23.3687 - mse: 1499.6368 - val_loss: 25.2843 - val_mse: 1780.2531\n",
      "Epoch 417/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4828 - mse: 1678.7373 - val_loss: 25.2261 - val_mse: 1733.7196\n",
      "Epoch 418/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1320 - mse: 1604.1960 - val_loss: 25.2267 - val_mse: 1739.2788\n",
      "Epoch 419/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9074 - mse: 1601.3402 - val_loss: 25.2620 - val_mse: 1711.2773\n",
      "Epoch 420/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1647 - mse: 1609.1722 - val_loss: 25.2797 - val_mse: 1779.1527\n",
      "Epoch 421/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6356 - mse: 1565.2351 - val_loss: 25.3331 - val_mse: 1798.5970\n",
      "Epoch 422/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0186 - mse: 1598.5957 - val_loss: 25.2264 - val_mse: 1736.8955\n",
      "Epoch 423/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1677 - mse: 1631.2451 - val_loss: 25.2332 - val_mse: 1749.9572\n",
      "Epoch 424/500\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 24.2319 - mse: 1643.3545 - val_loss: 25.2282 - val_mse: 1735.1184\n",
      "Epoch 425/500\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 23.7493 - mse: 1559.4055 - val_loss: 25.2560 - val_mse: 1767.4043\n",
      "Epoch 426/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0996 - mse: 1613.0121 - val_loss: 25.2308 - val_mse: 1748.2919\n",
      "Epoch 427/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1095 - mse: 1631.9336 - val_loss: 25.2268 - val_mse: 1733.5029\n",
      "Epoch 428/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6956 - mse: 1530.6072 - val_loss: 25.2593 - val_mse: 1769.7432\n",
      "Epoch 429/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1077 - mse: 1618.2219 - val_loss: 25.2291 - val_mse: 1745.4117\n",
      "Epoch 430/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8408 - mse: 1598.7335 - val_loss: 25.2541 - val_mse: 1766.3123\n",
      "Epoch 431/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0733 - mse: 1613.8923 - val_loss: 25.2276 - val_mse: 1733.4955\n",
      "Epoch 432/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2432 - mse: 1621.0474 - val_loss: 25.2695 - val_mse: 1775.0465\n",
      "Epoch 433/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7962 - mse: 1582.9847 - val_loss: 25.2311 - val_mse: 1747.4525\n",
      "Epoch 434/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9408 - mse: 1607.5739 - val_loss: 25.2275 - val_mse: 1743.2184\n",
      "Epoch 435/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1064 - mse: 1597.5581 - val_loss: 25.2275 - val_mse: 1744.0862\n",
      "Epoch 436/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0660 - mse: 1626.3105 - val_loss: 25.2291 - val_mse: 1746.0779\n",
      "Epoch 437/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.0105 - mse: 1591.5798 - val_loss: 25.3174 - val_mse: 1793.9121\n",
      "Epoch 438/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.9611 - mse: 1603.4341 - val_loss: 25.2340 - val_mse: 1751.2792\n",
      "Epoch 439/500\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 24.0238 - mse: 1600.6261 - val_loss: 25.2858 - val_mse: 1782.0624\n",
      "Epoch 440/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9803 - mse: 1605.6796 - val_loss: 25.2844 - val_mse: 1781.3604\n",
      "Epoch 441/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.9662 - mse: 1598.1199 - val_loss: 25.2603 - val_mse: 1769.7961\n",
      "Epoch 442/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0575 - mse: 1603.6982 - val_loss: 25.2310 - val_mse: 1747.3114\n",
      "Epoch 443/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1264 - mse: 1628.3007 - val_loss: 25.2274 - val_mse: 1733.8993\n",
      "Epoch 444/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8614 - mse: 1567.4552 - val_loss: 25.2866 - val_mse: 1782.2963\n",
      "Epoch 445/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.0341 - mse: 1621.0530 - val_loss: 25.2273 - val_mse: 1734.0382\n",
      "Epoch 446/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.0160 - mse: 1588.8143 - val_loss: 25.2322 - val_mse: 1749.6948\n",
      "Epoch 447/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9716 - mse: 1609.4764 - val_loss: 25.2535 - val_mse: 1716.5044\n",
      "Epoch 448/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0314 - mse: 1604.1619 - val_loss: 25.2744 - val_mse: 1777.1638\n",
      "Epoch 449/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0001 - mse: 1602.3734 - val_loss: 25.2308 - val_mse: 1730.2761\n",
      "Epoch 450/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0776 - mse: 1615.9703 - val_loss: 25.2439 - val_mse: 1759.6030\n",
      "Epoch 451/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8711 - mse: 1586.6862 - val_loss: 25.2359 - val_mse: 1752.7166\n",
      "Epoch 452/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.9876 - mse: 1590.8690 - val_loss: 25.2301 - val_mse: 1744.5815\n",
      "Epoch 453/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.0264 - mse: 1609.7971 - val_loss: 25.3296 - val_mse: 1797.5981\n",
      "Epoch 454/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0525 - mse: 1614.6088 - val_loss: 25.2576 - val_mse: 1768.1174\n",
      "Epoch 455/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9760 - mse: 1587.4984 - val_loss: 25.2373 - val_mse: 1753.8124\n",
      "Epoch 456/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.9188 - mse: 1603.9052 - val_loss: 25.2908 - val_mse: 1783.8820\n",
      "Epoch 457/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.2450 - mse: 1639.1420 - val_loss: 25.2830 - val_mse: 1706.1807\n",
      "Epoch 458/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.9323 - mse: 1589.8885 - val_loss: 25.2324 - val_mse: 1747.7899\n",
      "Epoch 459/500\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 23.7777 - mse: 1582.7000 - val_loss: 25.3431 - val_mse: 1801.7246\n",
      "Epoch 460/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.0932 - mse: 1612.9530 - val_loss: 25.2440 - val_mse: 1759.1034\n",
      "Epoch 461/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.0812 - mse: 1607.8921 - val_loss: 25.2311 - val_mse: 1745.5385\n",
      "Epoch 462/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8188 - mse: 1589.0563 - val_loss: 25.2312 - val_mse: 1731.4014\n",
      "Epoch 463/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9247 - mse: 1574.7831 - val_loss: 25.3348 - val_mse: 1799.2170\n",
      "Epoch 464/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2601 - mse: 1651.7783 - val_loss: 25.2329 - val_mse: 1748.3704\n",
      "Epoch 465/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8766 - mse: 1572.7468 - val_loss: 25.2303 - val_mse: 1742.8168\n",
      "Epoch 466/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2653 - mse: 1661.1277 - val_loss: 25.2715 - val_mse: 1774.7946\n",
      "Epoch 467/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8266 - mse: 1573.2231 - val_loss: 25.2337 - val_mse: 1729.7303\n",
      "Epoch 468/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8785 - mse: 1579.4757 - val_loss: 25.2321 - val_mse: 1746.0400\n",
      "Epoch 469/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1749 - mse: 1649.5330 - val_loss: 25.2406 - val_mse: 1755.0830\n",
      "Epoch 470/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8743 - mse: 1542.8297 - val_loss: 25.2576 - val_mse: 1766.8634\n",
      "Epoch 471/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9472 - mse: 1621.6249 - val_loss: 25.3653 - val_mse: 1807.6803\n",
      "Epoch 472/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9232 - mse: 1589.1509 - val_loss: 25.2326 - val_mse: 1743.4323\n",
      "Epoch 473/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2754 - mse: 1651.7683 - val_loss: 25.2734 - val_mse: 1775.0204\n",
      "Epoch 474/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2223 - mse: 1618.4485 - val_loss: 25.2457 - val_mse: 1721.8130\n",
      "Epoch 475/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8605 - mse: 1600.4971 - val_loss: 25.2458 - val_mse: 1721.7734\n",
      "Epoch 476/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7867 - mse: 1569.8049 - val_loss: 25.2381 - val_mse: 1726.7875\n",
      "Epoch 477/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5891 - mse: 1552.9343 - val_loss: 25.2748 - val_mse: 1775.3043\n",
      "Epoch 478/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.6358 - mse: 1686.0391 - val_loss: 25.2479 - val_mse: 1759.6743\n",
      "Epoch 479/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8140 - mse: 1564.5862 - val_loss: 25.2432 - val_mse: 1756.3622\n",
      "Epoch 480/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8461 - mse: 1578.2172 - val_loss: 25.2668 - val_mse: 1771.2327\n",
      "Epoch 481/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2110 - mse: 1623.0712 - val_loss: 25.2361 - val_mse: 1750.0404\n",
      "Epoch 482/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6499 - mse: 1586.7368 - val_loss: 25.3007 - val_mse: 1787.0549\n",
      "Epoch 483/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1886 - mse: 1593.8060 - val_loss: 25.2381 - val_mse: 1752.1282\n",
      "Epoch 484/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2208 - mse: 1656.1472 - val_loss: 25.2692 - val_mse: 1711.8667\n",
      "Epoch 485/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8566 - mse: 1562.5486 - val_loss: 25.2992 - val_mse: 1786.3459\n",
      "Epoch 486/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.9664 - mse: 1587.3636 - val_loss: 25.2476 - val_mse: 1759.6758\n",
      "Epoch 487/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8877 - mse: 1607.9186 - val_loss: 25.2333 - val_mse: 1734.0505\n",
      "Epoch 488/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9602 - mse: 1586.0468 - val_loss: 25.2351 - val_mse: 1747.1401\n",
      "Epoch 489/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2998 - mse: 1678.4130 - val_loss: 25.2342 - val_mse: 1742.2123\n",
      "Epoch 490/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7077 - mse: 1561.1211 - val_loss: 25.2517 - val_mse: 1719.4818\n",
      "Epoch 491/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1118 - mse: 1605.9513 - val_loss: 25.2319 - val_mse: 1740.8857\n",
      "Epoch 492/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9267 - mse: 1576.0358 - val_loss: 25.2402 - val_mse: 1753.1467\n",
      "Epoch 493/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9308 - mse: 1643.8842 - val_loss: 25.2418 - val_mse: 1754.4835\n",
      "Epoch 494/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0015 - mse: 1566.7886 - val_loss: 25.2344 - val_mse: 1733.0665\n",
      "Epoch 495/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1638 - mse: 1656.6555 - val_loss: 25.2626 - val_mse: 1769.0336\n",
      "Epoch 496/500\n",
      " 40/113 [=========>....................] - ETA: 0s - loss: 23.7691 - mse: 1524.0792WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 56500 batches). You may need to use the repeat() function when building your dataset.\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8690 - mse: 1560.6250 - val_loss: 25.2872 - val_mse: 1781.0923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 20:28:12.391057: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_rel_humidity0\n",
      "[40.73, 40.85]\n",
      "Epoch 1/500\n",
      "103/113 [==========================>...] - ETA: 0s - loss: 27.0557 - mse: 1822.6498"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 20:28:13.227335: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 0s 3ms/step - loss: 26.9079 - mse: 1807.6357 - val_loss: 26.5319 - val_mse: 1805.5853\n",
      "Epoch 2/500\n",
      " 39/113 [=========>....................] - ETA: 0s - loss: 26.0807 - mse: 1797.6578"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 20:28:13.467012: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 0s 3ms/step - loss: 25.6025 - mse: 1707.3694 - val_loss: 26.4425 - val_mse: 1840.3473\n",
      "Epoch 3/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.3081 - mse: 1658.8016 - val_loss: 26.4681 - val_mse: 1862.7170\n",
      "Epoch 4/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3957 - mse: 1689.3801 - val_loss: 26.5765 - val_mse: 1756.0048\n",
      "Epoch 5/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1509 - mse: 1659.9707 - val_loss: 26.9646 - val_mse: 1712.2803\n",
      "Epoch 6/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.6381 - mse: 1604.0305 - val_loss: 26.5632 - val_mse: 1907.6150\n",
      "Epoch 7/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.0013 - mse: 1673.9640 - val_loss: 25.8673 - val_mse: 1754.6198\n",
      "Epoch 8/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4124 - mse: 1588.2386 - val_loss: 25.8586 - val_mse: 1815.2125\n",
      "Epoch 9/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.6015 - mse: 1622.2225 - val_loss: 26.2048 - val_mse: 1729.4828\n",
      "Epoch 10/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2251 - mse: 1585.9823 - val_loss: 25.9583 - val_mse: 1709.5940\n",
      "Epoch 11/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.5664 - mse: 1631.2635 - val_loss: 25.8917 - val_mse: 1846.3812\n",
      "Epoch 12/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3152 - mse: 1590.4188 - val_loss: 26.0036 - val_mse: 1841.3489\n",
      "Epoch 13/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3318 - mse: 1599.0597 - val_loss: 25.8022 - val_mse: 1775.4327\n",
      "Epoch 14/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3720 - mse: 1626.0837 - val_loss: 25.8315 - val_mse: 1756.8862\n",
      "Epoch 15/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0853 - mse: 1576.8512 - val_loss: 26.0303 - val_mse: 1872.1886\n",
      "Epoch 16/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4114 - mse: 1613.5496 - val_loss: 25.8124 - val_mse: 1789.9626\n",
      "Epoch 17/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4017 - mse: 1614.2695 - val_loss: 26.0590 - val_mse: 1737.1693\n",
      "Epoch 18/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2191 - mse: 1590.6379 - val_loss: 26.2932 - val_mse: 1699.9844\n",
      "Epoch 19/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0286 - mse: 1583.4128 - val_loss: 25.8104 - val_mse: 1752.8032\n",
      "Epoch 20/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2703 - mse: 1587.8906 - val_loss: 26.0398 - val_mse: 1719.7843\n",
      "Epoch 21/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9655 - mse: 1601.7535 - val_loss: 25.8265 - val_mse: 1794.8484\n",
      "Epoch 22/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1489 - mse: 1594.6326 - val_loss: 25.8132 - val_mse: 1752.7784\n",
      "Epoch 23/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0009 - mse: 1591.1311 - val_loss: 26.2830 - val_mse: 1935.2875\n",
      "Epoch 24/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4570 - mse: 1621.5775 - val_loss: 25.7861 - val_mse: 1813.9502\n",
      "Epoch 25/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8088 - mse: 1576.6721 - val_loss: 25.8443 - val_mse: 1736.6688\n",
      "Epoch 26/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.1351 - mse: 1572.6788 - val_loss: 25.9033 - val_mse: 1765.1271\n",
      "Epoch 27/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7936 - mse: 1582.0391 - val_loss: 25.9118 - val_mse: 1831.2577\n",
      "Epoch 28/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7767 - mse: 1546.2113 - val_loss: 25.7720 - val_mse: 1750.5204\n",
      "Epoch 29/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8427 - mse: 1602.3385 - val_loss: 26.0023 - val_mse: 1746.1381\n",
      "Epoch 30/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9169 - mse: 1602.9639 - val_loss: 25.8798 - val_mse: 1786.6484\n",
      "Epoch 31/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.9203 - mse: 1538.3367 - val_loss: 25.8762 - val_mse: 1811.6993\n",
      "Epoch 32/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6473 - mse: 1545.4440 - val_loss: 26.0116 - val_mse: 1771.8048\n",
      "Epoch 33/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0587 - mse: 1604.5140 - val_loss: 26.0833 - val_mse: 1740.2284\n",
      "Epoch 34/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8686 - mse: 1612.3285 - val_loss: 26.1184 - val_mse: 1738.2740\n",
      "Epoch 35/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6686 - mse: 1551.4702 - val_loss: 25.9700 - val_mse: 1788.6875\n",
      "Epoch 36/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3738 - mse: 1545.7457 - val_loss: 26.0165 - val_mse: 1758.6863\n",
      "Epoch 37/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.4964 - mse: 1564.7679 - val_loss: 26.1896 - val_mse: 1837.1946\n",
      "Epoch 38/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.0550 - mse: 1616.1002 - val_loss: 26.1081 - val_mse: 1836.6180\n",
      "Epoch 39/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2227 - mse: 1529.6914 - val_loss: 26.0651 - val_mse: 1793.8864\n",
      "Epoch 40/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5808 - mse: 1553.4833 - val_loss: 26.1460 - val_mse: 1861.7094\n",
      "Epoch 41/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.3799 - mse: 1520.9639 - val_loss: 26.0490 - val_mse: 1793.8499\n",
      "Epoch 42/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.6961 - mse: 1614.8484 - val_loss: 26.1187 - val_mse: 1806.0338\n",
      "Epoch 43/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3302 - mse: 1536.6881 - val_loss: 26.3217 - val_mse: 1783.6062\n",
      "Epoch 44/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7444 - mse: 1583.4921 - val_loss: 26.2241 - val_mse: 1886.4935\n",
      "Epoch 45/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4351 - mse: 1532.8911 - val_loss: 26.3438 - val_mse: 1876.3583\n",
      "Epoch 46/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.1354 - mse: 1534.3788 - val_loss: 26.2662 - val_mse: 1817.8029\n",
      "Epoch 47/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4537 - mse: 1573.2819 - val_loss: 26.2568 - val_mse: 1813.2529\n",
      "Epoch 48/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.1899 - mse: 1510.2286 - val_loss: 26.4924 - val_mse: 1819.9761\n",
      "Epoch 49/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2724 - mse: 1549.4138 - val_loss: 26.2517 - val_mse: 1861.2374\n",
      "Epoch 50/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3239 - mse: 1568.7787 - val_loss: 26.5182 - val_mse: 1881.5225\n",
      "Epoch 51/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.6134 - mse: 1590.3270 - val_loss: 26.3960 - val_mse: 1877.5048\n",
      "Epoch 52/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2217 - mse: 1488.4819 - val_loss: 26.1429 - val_mse: 1797.7903\n",
      "Epoch 53/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.8252 - mse: 1640.1433 - val_loss: 26.5850 - val_mse: 1756.2863\n",
      "Epoch 54/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.1235 - mse: 1535.9728 - val_loss: 26.2481 - val_mse: 1841.2284\n",
      "Epoch 55/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.7865 - mse: 1516.4753 - val_loss: 26.4766 - val_mse: 1887.1840\n",
      "Epoch 56/500\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 23.2172 - mse: 1515.9540 - val_loss: 26.4551 - val_mse: 1840.4932\n",
      "Epoch 57/500\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 23.5710 - mse: 1602.7931 - val_loss: 26.2944 - val_mse: 1853.3242\n",
      "Epoch 58/500\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 23.0940 - mse: 1539.7683 - val_loss: 26.3987 - val_mse: 1846.2039\n",
      "Epoch 59/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.0212 - mse: 1545.0627 - val_loss: 26.4273 - val_mse: 1775.6733\n",
      "Epoch 60/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.1895 - mse: 1526.9907 - val_loss: 26.4372 - val_mse: 1863.8879\n",
      "Epoch 61/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.2576 - mse: 1569.8711 - val_loss: 26.5135 - val_mse: 1789.2328\n",
      "Epoch 62/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.0540 - mse: 1532.8770 - val_loss: 26.4333 - val_mse: 1839.6608\n",
      "Epoch 63/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.7354 - mse: 1485.3818 - val_loss: 26.4579 - val_mse: 1835.8384\n",
      "Epoch 64/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.2015 - mse: 1554.5972 - val_loss: 27.2361 - val_mse: 1764.8600\n",
      "Epoch 65/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.9297 - mse: 1537.1848 - val_loss: 26.4537 - val_mse: 1832.3348\n",
      "Epoch 66/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.0838 - mse: 1554.4637 - val_loss: 26.4640 - val_mse: 1853.8740\n",
      "Epoch 67/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.0565 - mse: 1515.3563 - val_loss: 26.5264 - val_mse: 1810.9156\n",
      "Epoch 68/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.5826 - mse: 1488.2206 - val_loss: 26.5555 - val_mse: 1856.5094\n",
      "Epoch 69/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.4178 - mse: 1618.3790 - val_loss: 26.6538 - val_mse: 1822.2632\n",
      "Epoch 70/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.5212 - mse: 1471.8866 - val_loss: 26.6356 - val_mse: 1832.2567\n",
      "Epoch 71/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 22.9969 - mse: 1541.3002 - val_loss: 26.7132 - val_mse: 1929.7692\n",
      "Epoch 72/500\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 23.0616 - mse: 1553.8715 - val_loss: 26.9859 - val_mse: 1801.6487\n",
      "Epoch 73/500\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 22.6159 - mse: 1495.5077 - val_loss: 26.6975 - val_mse: 1855.3142\n",
      "Epoch 74/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.8878 - mse: 1524.6201 - val_loss: 26.7651 - val_mse: 1886.8975\n",
      "Epoch 75/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.8715 - mse: 1499.2877 - val_loss: 26.6295 - val_mse: 1819.0410\n",
      "Epoch 76/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.8316 - mse: 1538.2244 - val_loss: 26.6295 - val_mse: 1814.2170\n",
      "Epoch 77/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.6576 - mse: 1506.7140 - val_loss: 27.0296 - val_mse: 1814.2411\n",
      "Epoch 78/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.6319 - mse: 1506.3420 - val_loss: 26.9101 - val_mse: 1874.3469\n",
      "Epoch 79/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.7293 - mse: 1478.7653 - val_loss: 26.8231 - val_mse: 1890.3994\n",
      "Epoch 80/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.9400 - mse: 1560.3817 - val_loss: 26.8272 - val_mse: 1870.8235\n",
      "Epoch 81/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.3797 - mse: 1484.9521 - val_loss: 26.8804 - val_mse: 1800.3812\n",
      "Epoch 82/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.4245 - mse: 1471.6814 - val_loss: 26.9171 - val_mse: 1870.5668\n",
      "Epoch 83/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.8882 - mse: 1565.1710 - val_loss: 26.7576 - val_mse: 1899.6801\n",
      "Epoch 84/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.2167 - mse: 1435.1223 - val_loss: 26.7688 - val_mse: 1838.7577\n",
      "Epoch 85/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.0193 - mse: 1564.6395 - val_loss: 26.6967 - val_mse: 1840.6733\n",
      "Epoch 86/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.5157 - mse: 1505.1514 - val_loss: 27.1161 - val_mse: 1820.2684\n",
      "Epoch 87/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.3001 - mse: 1463.7435 - val_loss: 26.8983 - val_mse: 1874.7521\n",
      "Epoch 88/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 22.7554 - mse: 1529.7213 - val_loss: 26.8315 - val_mse: 1885.1610\n",
      "Epoch 89/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.6186 - mse: 1497.7186 - val_loss: 26.9328 - val_mse: 1858.6550\n",
      "Epoch 90/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.2774 - mse: 1477.4769 - val_loss: 26.9107 - val_mse: 1901.6498\n",
      "Epoch 91/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.5449 - mse: 1518.5328 - val_loss: 27.1127 - val_mse: 1860.3260\n",
      "Epoch 92/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.6018 - mse: 1509.8423 - val_loss: 27.1277 - val_mse: 1845.0542\n",
      "Epoch 93/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.3590 - mse: 1458.3202 - val_loss: 26.8424 - val_mse: 1867.9480\n",
      "Epoch 94/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.4864 - mse: 1515.4939 - val_loss: 27.0787 - val_mse: 1835.6593\n",
      "Epoch 95/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.5921 - mse: 1514.3342 - val_loss: 27.6779 - val_mse: 1766.7411\n",
      "Epoch 96/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.4825 - mse: 1489.4355 - val_loss: 26.9124 - val_mse: 1820.9465\n",
      "Epoch 97/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.3699 - mse: 1499.3401 - val_loss: 27.0989 - val_mse: 1816.4008\n",
      "Epoch 98/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.5987 - mse: 1502.0336 - val_loss: 26.9759 - val_mse: 1879.9332\n",
      "Epoch 99/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.2818 - mse: 1468.7914 - val_loss: 26.9110 - val_mse: 1842.7623\n",
      "Epoch 100/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.3793 - mse: 1500.3760 - val_loss: 27.2123 - val_mse: 1847.5795\n",
      "Epoch 101/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.4422 - mse: 1475.9020 - val_loss: 27.0665 - val_mse: 1842.8313\n",
      "Epoch 102/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.3836 - mse: 1503.2371 - val_loss: 27.0999 - val_mse: 1849.2800\n",
      "Epoch 103/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.5253 - mse: 1508.8181 - val_loss: 26.9697 - val_mse: 1894.5806\n",
      "Epoch 104/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.1716 - mse: 1479.7046 - val_loss: 27.1495 - val_mse: 1856.7238\n",
      "Epoch 105/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 22.3149 - mse: 1479.5485 - val_loss: 27.1438 - val_mse: 1823.6172\n",
      "Epoch 106/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.3212 - mse: 1479.2571 - val_loss: 27.0617 - val_mse: 1920.3315\n",
      "Epoch 107/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 22.3537 - mse: 1470.5474 - val_loss: 27.4659 - val_mse: 1815.9875\n",
      "Epoch 108/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.2819 - mse: 1494.4806 - val_loss: 27.2659 - val_mse: 1888.7666\n",
      "Epoch 109/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.4179 - mse: 1494.0486 - val_loss: 27.2532 - val_mse: 1857.9995\n",
      "Epoch 110/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.3477 - mse: 1479.3981 - val_loss: 27.0889 - val_mse: 1922.8712\n",
      "Epoch 111/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.5793 - mse: 1498.6688 - val_loss: 27.0363 - val_mse: 1944.5232\n",
      "Epoch 112/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.2405 - mse: 1480.8121 - val_loss: 27.1108 - val_mse: 1848.7513\n",
      "Epoch 113/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.3933 - mse: 1491.3279 - val_loss: 26.9909 - val_mse: 1857.5535\n",
      "Epoch 114/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.3752 - mse: 1474.5896 - val_loss: 27.2586 - val_mse: 1840.7008\n",
      "Epoch 115/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.3931 - mse: 1520.4022 - val_loss: 27.2188 - val_mse: 1847.8986\n",
      "Epoch 116/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.1832 - mse: 1451.9954 - val_loss: 27.1774 - val_mse: 1851.0723\n",
      "Epoch 117/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.0359 - mse: 1474.8566 - val_loss: 27.2414 - val_mse: 1886.8236\n",
      "Epoch 118/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.3002 - mse: 1499.1260 - val_loss: 27.2112 - val_mse: 1845.1947\n",
      "Epoch 119/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.2349 - mse: 1475.4608 - val_loss: 27.1677 - val_mse: 1878.9352\n",
      "Epoch 120/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.2612 - mse: 1487.3864 - val_loss: 27.1136 - val_mse: 1860.2759\n",
      "Epoch 121/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8460 - mse: 1436.6847 - val_loss: 27.5049 - val_mse: 1849.3542\n",
      "Epoch 122/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.4844 - mse: 1543.7858 - val_loss: 27.1986 - val_mse: 1874.8726\n",
      "Epoch 123/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.0850 - mse: 1453.0527 - val_loss: 27.3354 - val_mse: 1870.2119\n",
      "Epoch 124/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.2760 - mse: 1502.9761 - val_loss: 27.1568 - val_mse: 1849.2323\n",
      "Epoch 125/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.1167 - mse: 1460.3582 - val_loss: 27.4100 - val_mse: 1913.7225\n",
      "Epoch 126/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.0215 - mse: 1474.6532 - val_loss: 27.3647 - val_mse: 1856.0978\n",
      "Epoch 127/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.1057 - mse: 1475.1696 - val_loss: 27.4812 - val_mse: 1869.7496\n",
      "Epoch 128/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.0918 - mse: 1467.0342 - val_loss: 27.5684 - val_mse: 1840.9685\n",
      "Epoch 129/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.2185 - mse: 1503.8002 - val_loss: 27.2927 - val_mse: 1916.5762\n",
      "Epoch 130/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.0739 - mse: 1500.1067 - val_loss: 27.5157 - val_mse: 1917.3187\n",
      "Epoch 131/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8498 - mse: 1413.6147 - val_loss: 27.7444 - val_mse: 1848.0321\n",
      "Epoch 132/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.1260 - mse: 1497.2368 - val_loss: 27.3364 - val_mse: 1864.7622\n",
      "Epoch 133/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8824 - mse: 1452.1625 - val_loss: 27.5682 - val_mse: 1824.1138\n",
      "Epoch 134/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.3909 - mse: 1511.2231 - val_loss: 27.5186 - val_mse: 1846.6179\n",
      "Epoch 135/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7622 - mse: 1428.0786 - val_loss: 27.2249 - val_mse: 1871.8994\n",
      "Epoch 136/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.9498 - mse: 1483.7626 - val_loss: 27.4017 - val_mse: 1927.8374\n",
      "Epoch 137/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.0253 - mse: 1464.4858 - val_loss: 27.7691 - val_mse: 1896.7446\n",
      "Epoch 138/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.9411 - mse: 1452.7300 - val_loss: 27.4223 - val_mse: 1868.6848\n",
      "Epoch 139/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8547 - mse: 1453.5508 - val_loss: 27.4734 - val_mse: 1862.2490\n",
      "Epoch 140/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.1666 - mse: 1520.4109 - val_loss: 27.4061 - val_mse: 1938.6078\n",
      "Epoch 141/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8925 - mse: 1436.4969 - val_loss: 27.3039 - val_mse: 1865.3940\n",
      "Epoch 142/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.2080 - mse: 1492.3229 - val_loss: 27.3271 - val_mse: 1876.2037\n",
      "Epoch 143/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8540 - mse: 1420.6597 - val_loss: 27.4791 - val_mse: 1859.8979\n",
      "Epoch 144/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.1604 - mse: 1516.6766 - val_loss: 27.4182 - val_mse: 1914.6967\n",
      "Epoch 145/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.9412 - mse: 1475.3776 - val_loss: 27.7806 - val_mse: 1872.5942\n",
      "Epoch 146/500\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 22.1734 - mse: 1486.9993 - val_loss: 27.4112 - val_mse: 1934.6870\n",
      "Epoch 147/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.7829 - mse: 1417.1260 - val_loss: 27.8193 - val_mse: 1856.3423\n",
      "Epoch 148/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.3550 - mse: 1542.9296 - val_loss: 27.6427 - val_mse: 1858.6945\n",
      "Epoch 149/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.9451 - mse: 1445.8613 - val_loss: 27.5065 - val_mse: 1854.9717\n",
      "Epoch 150/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8993 - mse: 1460.4781 - val_loss: 27.7009 - val_mse: 1836.1421\n",
      "Epoch 151/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7166 - mse: 1454.1410 - val_loss: 27.6333 - val_mse: 1866.3843\n",
      "Epoch 152/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.1373 - mse: 1471.8639 - val_loss: 27.3691 - val_mse: 1867.0847\n",
      "Epoch 153/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.0647 - mse: 1506.5978 - val_loss: 27.1379 - val_mse: 1862.2653\n",
      "Epoch 154/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8449 - mse: 1410.0983 - val_loss: 27.3991 - val_mse: 1859.5830\n",
      "Epoch 155/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 22.2159 - mse: 1543.2679 - val_loss: 27.6330 - val_mse: 1822.3627\n",
      "Epoch 156/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7981 - mse: 1442.0491 - val_loss: 27.3508 - val_mse: 1885.4932\n",
      "Epoch 157/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.7894 - mse: 1418.8506 - val_loss: 27.2216 - val_mse: 1892.5312\n",
      "Epoch 158/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8787 - mse: 1463.6833 - val_loss: 27.1945 - val_mse: 1895.2102\n",
      "Epoch 159/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8500 - mse: 1468.7831 - val_loss: 27.5374 - val_mse: 1916.8297\n",
      "Epoch 160/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.8581 - mse: 1474.1312 - val_loss: 27.3619 - val_mse: 1899.7758\n",
      "Epoch 161/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8721 - mse: 1458.6736 - val_loss: 27.4648 - val_mse: 1964.1420\n",
      "Epoch 162/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.0239 - mse: 1477.1646 - val_loss: 27.1619 - val_mse: 1916.1156\n",
      "Epoch 163/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8049 - mse: 1432.1703 - val_loss: 27.3115 - val_mse: 1849.1370\n",
      "Epoch 164/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 22.0855 - mse: 1491.6183 - val_loss: 27.6804 - val_mse: 1845.6086\n",
      "Epoch 165/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5785 - mse: 1433.3212 - val_loss: 27.3530 - val_mse: 1903.0697\n",
      "Epoch 166/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.9275 - mse: 1496.0470 - val_loss: 27.4262 - val_mse: 1885.1726\n",
      "Epoch 167/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8780 - mse: 1467.1771 - val_loss: 27.2577 - val_mse: 1903.3878\n",
      "Epoch 168/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.0770 - mse: 1460.3669 - val_loss: 27.2495 - val_mse: 1916.4797\n",
      "Epoch 169/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.8071 - mse: 1482.5033 - val_loss: 27.6099 - val_mse: 1875.1882\n",
      "Epoch 170/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8016 - mse: 1439.7026 - val_loss: 27.5068 - val_mse: 1879.2889\n",
      "Epoch 171/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8171 - mse: 1465.9241 - val_loss: 27.3672 - val_mse: 1891.2866\n",
      "Epoch 172/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8259 - mse: 1441.0573 - val_loss: 27.3982 - val_mse: 1861.1460\n",
      "Epoch 173/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 22.4133 - mse: 1554.9707 - val_loss: 27.5718 - val_mse: 1879.3925\n",
      "Epoch 174/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.3804 - mse: 1387.7114 - val_loss: 27.2957 - val_mse: 1900.3868\n",
      "Epoch 175/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.9337 - mse: 1475.9406 - val_loss: 27.3569 - val_mse: 1894.9176\n",
      "Epoch 176/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6179 - mse: 1424.8052 - val_loss: 27.5828 - val_mse: 1839.3727\n",
      "Epoch 177/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.8852 - mse: 1469.5836 - val_loss: 27.4697 - val_mse: 1901.8167\n",
      "Epoch 178/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.7323 - mse: 1463.4275 - val_loss: 27.7122 - val_mse: 1866.8422\n",
      "Epoch 179/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.6095 - mse: 1414.1279 - val_loss: 27.6018 - val_mse: 1907.8817\n",
      "Epoch 180/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.1087 - mse: 1503.0835 - val_loss: 27.6157 - val_mse: 1903.3843\n",
      "Epoch 181/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4660 - mse: 1410.0326 - val_loss: 27.4778 - val_mse: 1926.2151\n",
      "Epoch 182/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.2004 - mse: 1514.9991 - val_loss: 27.5960 - val_mse: 1881.2622\n",
      "Epoch 183/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.6056 - mse: 1430.3851 - val_loss: 27.4563 - val_mse: 1913.2924\n",
      "Epoch 184/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.1593 - mse: 1515.5271 - val_loss: 27.4064 - val_mse: 1891.5115\n",
      "Epoch 185/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6845 - mse: 1455.4819 - val_loss: 27.7108 - val_mse: 1888.0443\n",
      "Epoch 186/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7229 - mse: 1422.7458 - val_loss: 27.4342 - val_mse: 1883.5856\n",
      "Epoch 187/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7296 - mse: 1472.8688 - val_loss: 27.4162 - val_mse: 1921.7269\n",
      "Epoch 188/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7870 - mse: 1470.3494 - val_loss: 27.6532 - val_mse: 1868.5356\n",
      "Epoch 189/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7020 - mse: 1445.8253 - val_loss: 27.7763 - val_mse: 1853.6080\n",
      "Epoch 190/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8767 - mse: 1472.5062 - val_loss: 27.5071 - val_mse: 1899.4188\n",
      "Epoch 191/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4255 - mse: 1408.8385 - val_loss: 27.4491 - val_mse: 1891.3439\n",
      "Epoch 192/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8738 - mse: 1434.3726 - val_loss: 27.3246 - val_mse: 1915.7212\n",
      "Epoch 193/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8097 - mse: 1502.3170 - val_loss: 27.4733 - val_mse: 1898.6456\n",
      "Epoch 194/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.7820 - mse: 1420.0833 - val_loss: 27.3057 - val_mse: 1901.9756\n",
      "Epoch 195/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.6960 - mse: 1486.9734 - val_loss: 27.4444 - val_mse: 1900.6462\n",
      "Epoch 196/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7433 - mse: 1435.9835 - val_loss: 27.8736 - val_mse: 1872.6282\n",
      "Epoch 197/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5838 - mse: 1425.3716 - val_loss: 27.7529 - val_mse: 1901.5693\n",
      "Epoch 198/500\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 21.6590 - mse: 1447.1143 - val_loss: 27.5450 - val_mse: 1897.3107\n",
      "Epoch 199/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.9705 - mse: 1509.9827 - val_loss: 27.3823 - val_mse: 1878.0273\n",
      "Epoch 200/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7377 - mse: 1449.6901 - val_loss: 27.6211 - val_mse: 1898.4387\n",
      "Epoch 201/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3071 - mse: 1384.1716 - val_loss: 27.5147 - val_mse: 1867.0664\n",
      "Epoch 202/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.2593 - mse: 1540.1492 - val_loss: 27.7099 - val_mse: 1913.8696\n",
      "Epoch 203/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4640 - mse: 1416.4927 - val_loss: 27.8173 - val_mse: 1851.0503\n",
      "Epoch 204/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.9281 - mse: 1469.7084 - val_loss: 27.5918 - val_mse: 1912.9136\n",
      "Epoch 205/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5501 - mse: 1445.4188 - val_loss: 27.5904 - val_mse: 1883.1493\n",
      "Epoch 206/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7014 - mse: 1430.2723 - val_loss: 27.6027 - val_mse: 1957.8616\n",
      "Epoch 207/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.8475 - mse: 1479.7540 - val_loss: 27.6799 - val_mse: 1867.7688\n",
      "Epoch 208/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3130 - mse: 1413.0210 - val_loss: 27.6653 - val_mse: 1979.1530\n",
      "Epoch 209/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 22.0215 - mse: 1498.3521 - val_loss: 27.5691 - val_mse: 1906.7229\n",
      "Epoch 210/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5890 - mse: 1429.6552 - val_loss: 27.7027 - val_mse: 1855.1753\n",
      "Epoch 211/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8926 - mse: 1480.8137 - val_loss: 27.8732 - val_mse: 1918.0892\n",
      "Epoch 212/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4287 - mse: 1427.3832 - val_loss: 27.4810 - val_mse: 1899.0817\n",
      "Epoch 213/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7009 - mse: 1438.2168 - val_loss: 27.6967 - val_mse: 1939.4695\n",
      "Epoch 214/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.9108 - mse: 1471.7115 - val_loss: 27.6378 - val_mse: 1854.7340\n",
      "Epoch 215/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7872 - mse: 1467.9774 - val_loss: 27.5862 - val_mse: 1868.6892\n",
      "Epoch 216/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4605 - mse: 1432.3105 - val_loss: 27.5346 - val_mse: 1886.1547\n",
      "Epoch 217/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5372 - mse: 1413.4391 - val_loss: 27.6563 - val_mse: 1891.5804\n",
      "Epoch 218/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.1323 - mse: 1489.4507 - val_loss: 27.4290 - val_mse: 1873.0741\n",
      "Epoch 219/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5807 - mse: 1461.6097 - val_loss: 27.4402 - val_mse: 1894.9850\n",
      "Epoch 220/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6014 - mse: 1445.1804 - val_loss: 27.5819 - val_mse: 1904.4156\n",
      "Epoch 221/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7441 - mse: 1449.5802 - val_loss: 27.5848 - val_mse: 1922.2384\n",
      "Epoch 222/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6951 - mse: 1454.7252 - val_loss: 27.7068 - val_mse: 1903.2377\n",
      "Epoch 223/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6487 - mse: 1447.9304 - val_loss: 27.7610 - val_mse: 1922.9897\n",
      "Epoch 224/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5378 - mse: 1443.1133 - val_loss: 27.6884 - val_mse: 1870.8247\n",
      "Epoch 225/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6664 - mse: 1448.9326 - val_loss: 27.6699 - val_mse: 1874.0408\n",
      "Epoch 226/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6600 - mse: 1460.2559 - val_loss: 28.0049 - val_mse: 1876.1863\n",
      "Epoch 227/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5223 - mse: 1425.4619 - val_loss: 27.6962 - val_mse: 1973.8890\n",
      "Epoch 228/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6936 - mse: 1448.8020 - val_loss: 27.5772 - val_mse: 1908.4854\n",
      "Epoch 229/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6618 - mse: 1456.3224 - val_loss: 27.6518 - val_mse: 1919.7090\n",
      "Epoch 230/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7270 - mse: 1458.3687 - val_loss: 27.4754 - val_mse: 1935.5610\n",
      "Epoch 231/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4623 - mse: 1418.8435 - val_loss: 27.6993 - val_mse: 1897.3634\n",
      "Epoch 232/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8017 - mse: 1468.8464 - val_loss: 27.6273 - val_mse: 1907.4567\n",
      "Epoch 233/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3748 - mse: 1414.8071 - val_loss: 27.8890 - val_mse: 1870.8812\n",
      "Epoch 234/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8669 - mse: 1473.5543 - val_loss: 27.6115 - val_mse: 1871.0050\n",
      "Epoch 235/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.5301 - mse: 1437.4432 - val_loss: 27.8551 - val_mse: 1919.5668\n",
      "Epoch 236/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6648 - mse: 1454.8643 - val_loss: 27.7386 - val_mse: 1914.5232\n",
      "Epoch 237/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6805 - mse: 1453.1461 - val_loss: 27.7043 - val_mse: 1904.5682\n",
      "Epoch 238/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7031 - mse: 1470.6252 - val_loss: 27.9584 - val_mse: 1884.2051\n",
      "Epoch 239/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4494 - mse: 1383.5786 - val_loss: 27.7141 - val_mse: 1890.0762\n",
      "Epoch 240/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7217 - mse: 1504.9811 - val_loss: 27.8280 - val_mse: 1896.8242\n",
      "Epoch 241/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5237 - mse: 1408.6998 - val_loss: 27.7720 - val_mse: 1895.2832\n",
      "Epoch 242/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7297 - mse: 1461.0697 - val_loss: 27.8147 - val_mse: 1883.7078\n",
      "Epoch 243/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6997 - mse: 1442.6959 - val_loss: 27.7298 - val_mse: 1897.8881\n",
      "Epoch 244/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4950 - mse: 1427.1260 - val_loss: 27.6683 - val_mse: 1909.0408\n",
      "Epoch 245/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7632 - mse: 1474.5123 - val_loss: 27.7402 - val_mse: 1910.3201\n",
      "Epoch 246/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8794 - mse: 1495.7747 - val_loss: 27.6386 - val_mse: 1902.0837\n",
      "Epoch 247/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3004 - mse: 1393.3468 - val_loss: 27.8819 - val_mse: 1912.6704\n",
      "Epoch 248/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.5083 - mse: 1439.8815 - val_loss: 27.7583 - val_mse: 1951.0536\n",
      "Epoch 249/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5108 - mse: 1426.9224 - val_loss: 27.6760 - val_mse: 1939.3519\n",
      "Epoch 250/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7064 - mse: 1469.9440 - val_loss: 27.6393 - val_mse: 1920.8568\n",
      "Epoch 251/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7617 - mse: 1447.1385 - val_loss: 27.6493 - val_mse: 1926.6472\n",
      "Epoch 252/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3755 - mse: 1412.7561 - val_loss: 27.8351 - val_mse: 1901.0194\n",
      "Epoch 253/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.9468 - mse: 1497.7480 - val_loss: 27.7264 - val_mse: 1872.6827\n",
      "Epoch 254/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.5176 - mse: 1416.9070 - val_loss: 27.5518 - val_mse: 1931.2476\n",
      "Epoch 255/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5201 - mse: 1447.6172 - val_loss: 27.7464 - val_mse: 1878.3204\n",
      "Epoch 256/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.0534 - mse: 1505.7836 - val_loss: 27.7573 - val_mse: 1934.5261\n",
      "Epoch 257/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4144 - mse: 1390.5791 - val_loss: 27.6477 - val_mse: 1892.7904\n",
      "Epoch 258/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2051 - mse: 1420.6968 - val_loss: 27.7524 - val_mse: 1957.8750\n",
      "Epoch 259/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.0274 - mse: 1470.7504 - val_loss: 27.8181 - val_mse: 1890.8961\n",
      "Epoch 260/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.4881 - mse: 1450.3722 - val_loss: 27.7183 - val_mse: 1927.0947\n",
      "Epoch 261/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5120 - mse: 1445.6631 - val_loss: 27.8266 - val_mse: 1904.5977\n",
      "Epoch 262/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8633 - mse: 1461.6163 - val_loss: 27.7088 - val_mse: 1924.7859\n",
      "Epoch 263/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8062 - mse: 1488.0750 - val_loss: 27.7792 - val_mse: 1885.9021\n",
      "Epoch 264/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2368 - mse: 1387.3770 - val_loss: 27.7258 - val_mse: 1933.6387\n",
      "Epoch 265/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.9892 - mse: 1457.5587 - val_loss: 27.7152 - val_mse: 1885.0465\n",
      "Epoch 266/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2194 - mse: 1406.5399 - val_loss: 27.7549 - val_mse: 1943.6603\n",
      "Epoch 267/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7317 - mse: 1487.6851 - val_loss: 27.6119 - val_mse: 1885.0968\n",
      "Epoch 268/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.2404 - mse: 1377.5264 - val_loss: 27.8941 - val_mse: 1876.2830\n",
      "Epoch 269/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 22.2226 - mse: 1523.4823 - val_loss: 27.7964 - val_mse: 1930.9656\n",
      "Epoch 270/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.1608 - mse: 1401.9785 - val_loss: 27.7775 - val_mse: 1920.4167\n",
      "Epoch 271/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.8733 - mse: 1500.4568 - val_loss: 28.0084 - val_mse: 1902.1149\n",
      "Epoch 272/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4678 - mse: 1433.0004 - val_loss: 28.0718 - val_mse: 1922.1417\n",
      "Epoch 273/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.8080 - mse: 1475.3521 - val_loss: 27.8336 - val_mse: 1926.9071\n",
      "Epoch 274/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4074 - mse: 1414.3622 - val_loss: 28.0607 - val_mse: 1850.5405\n",
      "Epoch 275/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.6851 - mse: 1462.9009 - val_loss: 27.8233 - val_mse: 1892.2637\n",
      "Epoch 276/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3040 - mse: 1404.5444 - val_loss: 27.8070 - val_mse: 1913.4615\n",
      "Epoch 277/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6230 - mse: 1471.0227 - val_loss: 27.9315 - val_mse: 1903.1285\n",
      "Epoch 278/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4757 - mse: 1403.8279 - val_loss: 28.0122 - val_mse: 1944.9940\n",
      "Epoch 279/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3387 - mse: 1430.4935 - val_loss: 27.8943 - val_mse: 1933.5266\n",
      "Epoch 280/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7036 - mse: 1473.2841 - val_loss: 27.7316 - val_mse: 1916.6123\n",
      "Epoch 281/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5438 - mse: 1451.4741 - val_loss: 27.9871 - val_mse: 1910.4385\n",
      "Epoch 282/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.9804 - mse: 1512.3069 - val_loss: 27.9024 - val_mse: 1904.3153\n",
      "Epoch 283/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 20.7446 - mse: 1333.8876 - val_loss: 27.8255 - val_mse: 1927.6873\n",
      "Epoch 284/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.0720 - mse: 1495.3608 - val_loss: 27.8056 - val_mse: 1904.2385\n",
      "Epoch 285/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4003 - mse: 1426.3381 - val_loss: 27.8291 - val_mse: 1957.5590\n",
      "Epoch 286/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4359 - mse: 1433.2551 - val_loss: 27.7127 - val_mse: 1944.7460\n",
      "Epoch 287/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5697 - mse: 1459.3622 - val_loss: 28.0234 - val_mse: 1930.9757\n",
      "Epoch 288/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5381 - mse: 1451.5522 - val_loss: 28.0391 - val_mse: 1968.3645\n",
      "Epoch 289/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4963 - mse: 1397.3988 - val_loss: 27.9222 - val_mse: 1953.5496\n",
      "Epoch 290/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5932 - mse: 1471.1436 - val_loss: 28.0553 - val_mse: 1888.7533\n",
      "Epoch 291/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2497 - mse: 1390.9039 - val_loss: 28.2940 - val_mse: 1932.6451\n",
      "Epoch 292/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7521 - mse: 1465.9634 - val_loss: 27.7625 - val_mse: 1909.8844\n",
      "Epoch 293/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5784 - mse: 1466.9462 - val_loss: 27.7565 - val_mse: 1939.6118\n",
      "Epoch 294/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.4510 - mse: 1392.8680 - val_loss: 27.8473 - val_mse: 1921.6553\n",
      "Epoch 295/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7358 - mse: 1501.5115 - val_loss: 28.0873 - val_mse: 1936.8204\n",
      "Epoch 296/500\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 21.4428 - mse: 1427.0802 - val_loss: 28.1057 - val_mse: 1931.2288\n",
      "Epoch 297/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.7266 - mse: 1495.6266 - val_loss: 27.9379 - val_mse: 1882.7836\n",
      "Epoch 298/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.1371 - mse: 1379.2793 - val_loss: 27.9374 - val_mse: 1951.1775\n",
      "Epoch 299/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.6379 - mse: 1481.2446 - val_loss: 28.0379 - val_mse: 1894.4581\n",
      "Epoch 300/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.5849 - mse: 1426.4209 - val_loss: 27.8967 - val_mse: 1997.4789\n",
      "Epoch 301/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3819 - mse: 1416.0608 - val_loss: 28.0334 - val_mse: 1981.0981\n",
      "Epoch 302/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7575 - mse: 1465.7170 - val_loss: 28.0631 - val_mse: 1960.9413\n",
      "Epoch 303/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.4522 - mse: 1441.1539 - val_loss: 27.9604 - val_mse: 1975.8862\n",
      "Epoch 304/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4400 - mse: 1439.9220 - val_loss: 27.9939 - val_mse: 1947.9321\n",
      "Epoch 305/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6807 - mse: 1476.9554 - val_loss: 28.2062 - val_mse: 1958.2238\n",
      "Epoch 306/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2214 - mse: 1402.6512 - val_loss: 28.0406 - val_mse: 1931.5107\n",
      "Epoch 307/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5429 - mse: 1448.6400 - val_loss: 27.7928 - val_mse: 1941.0178\n",
      "Epoch 308/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3486 - mse: 1427.3497 - val_loss: 27.8629 - val_mse: 1948.0314\n",
      "Epoch 309/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3651 - mse: 1421.0701 - val_loss: 27.8982 - val_mse: 1934.9443\n",
      "Epoch 310/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5022 - mse: 1448.2832 - val_loss: 28.1448 - val_mse: 2029.6362\n",
      "Epoch 311/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5516 - mse: 1437.2008 - val_loss: 28.1692 - val_mse: 1952.6917\n",
      "Epoch 312/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8507 - mse: 1462.3451 - val_loss: 28.0657 - val_mse: 1870.0028\n",
      "Epoch 313/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4186 - mse: 1437.5302 - val_loss: 27.8588 - val_mse: 1961.5502\n",
      "Epoch 314/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5110 - mse: 1415.4363 - val_loss: 27.8516 - val_mse: 1902.6600\n",
      "Epoch 315/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6978 - mse: 1508.6339 - val_loss: 27.7018 - val_mse: 1934.2267\n",
      "Epoch 316/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6057 - mse: 1441.0497 - val_loss: 28.4016 - val_mse: 1884.4194\n",
      "Epoch 317/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3173 - mse: 1424.1327 - val_loss: 27.8457 - val_mse: 1929.6820\n",
      "Epoch 318/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5008 - mse: 1419.6243 - val_loss: 27.9804 - val_mse: 1959.7350\n",
      "Epoch 319/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3884 - mse: 1454.2252 - val_loss: 27.9424 - val_mse: 1928.4446\n",
      "Epoch 320/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6331 - mse: 1447.5847 - val_loss: 27.8687 - val_mse: 1947.8068\n",
      "Epoch 321/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3895 - mse: 1413.2324 - val_loss: 27.9826 - val_mse: 1933.9862\n",
      "Epoch 322/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.4268 - mse: 1452.8652 - val_loss: 27.9827 - val_mse: 1960.0532\n",
      "Epoch 323/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.3525 - mse: 1404.2423 - val_loss: 28.0554 - val_mse: 1936.0062\n",
      "Epoch 324/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7764 - mse: 1489.1853 - val_loss: 28.0502 - val_mse: 1934.1687\n",
      "Epoch 325/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4270 - mse: 1429.3721 - val_loss: 27.9389 - val_mse: 1881.2623\n",
      "Epoch 326/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4242 - mse: 1429.9237 - val_loss: 28.2086 - val_mse: 1940.4122\n",
      "Epoch 327/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6319 - mse: 1443.8262 - val_loss: 27.7685 - val_mse: 1901.7588\n",
      "Epoch 328/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4076 - mse: 1445.8092 - val_loss: 28.2401 - val_mse: 1884.3079\n",
      "Epoch 329/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2521 - mse: 1406.3389 - val_loss: 27.8655 - val_mse: 1930.0188\n",
      "Epoch 330/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6399 - mse: 1471.9669 - val_loss: 27.9718 - val_mse: 1945.9551\n",
      "Epoch 331/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6078 - mse: 1457.4457 - val_loss: 28.0870 - val_mse: 1903.6467\n",
      "Epoch 332/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.1935 - mse: 1403.6104 - val_loss: 27.9230 - val_mse: 1911.3907\n",
      "Epoch 333/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6187 - mse: 1461.9594 - val_loss: 28.0939 - val_mse: 1971.2256\n",
      "Epoch 334/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3892 - mse: 1434.3236 - val_loss: 28.1225 - val_mse: 1948.1644\n",
      "Epoch 335/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5487 - mse: 1440.7090 - val_loss: 27.8050 - val_mse: 1916.9585\n",
      "Epoch 336/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5268 - mse: 1443.4523 - val_loss: 27.8757 - val_mse: 1946.3597\n",
      "Epoch 337/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3658 - mse: 1437.0088 - val_loss: 27.8511 - val_mse: 1919.2493\n",
      "Epoch 338/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5083 - mse: 1438.9526 - val_loss: 27.8608 - val_mse: 1927.2904\n",
      "Epoch 339/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4676 - mse: 1437.2188 - val_loss: 27.9472 - val_mse: 1938.7068\n",
      "Epoch 340/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5154 - mse: 1440.3815 - val_loss: 27.7545 - val_mse: 1929.7776\n",
      "Epoch 341/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4279 - mse: 1434.8337 - val_loss: 27.8409 - val_mse: 1918.6208\n",
      "Epoch 342/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5435 - mse: 1439.5809 - val_loss: 27.9329 - val_mse: 1927.6493\n",
      "Epoch 343/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5042 - mse: 1448.7933 - val_loss: 28.2042 - val_mse: 1919.0488\n",
      "Epoch 344/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3719 - mse: 1427.1018 - val_loss: 28.1193 - val_mse: 1912.3646\n",
      "Epoch 345/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3296 - mse: 1429.6792 - val_loss: 27.8265 - val_mse: 1932.8955\n",
      "Epoch 346/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5470 - mse: 1450.9978 - val_loss: 28.4384 - val_mse: 1947.3506\n",
      "Epoch 347/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4115 - mse: 1437.0416 - val_loss: 27.8239 - val_mse: 1939.0569\n",
      "Epoch 348/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6890 - mse: 1463.9302 - val_loss: 27.9303 - val_mse: 1966.0442\n",
      "Epoch 349/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3317 - mse: 1414.3944 - val_loss: 27.9828 - val_mse: 1985.7994\n",
      "Epoch 350/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4126 - mse: 1435.7927 - val_loss: 27.9125 - val_mse: 1956.0236\n",
      "Epoch 351/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4862 - mse: 1441.6604 - val_loss: 28.0231 - val_mse: 1976.4258\n",
      "Epoch 352/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.1757 - mse: 1410.2904 - val_loss: 27.9599 - val_mse: 1978.0527\n",
      "Epoch 353/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4982 - mse: 1445.9243 - val_loss: 28.1396 - val_mse: 1944.1295\n",
      "Epoch 354/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7584 - mse: 1487.6030 - val_loss: 28.3723 - val_mse: 1888.9460\n",
      "Epoch 355/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.1117 - mse: 1390.5433 - val_loss: 28.2157 - val_mse: 1974.3170\n",
      "Epoch 356/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.7872 - mse: 1493.9091 - val_loss: 28.4296 - val_mse: 1922.9703\n",
      "Epoch 357/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2508 - mse: 1412.6093 - val_loss: 27.9143 - val_mse: 1962.9901\n",
      "Epoch 358/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3036 - mse: 1412.8320 - val_loss: 28.1669 - val_mse: 1935.4689\n",
      "Epoch 359/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4345 - mse: 1439.4412 - val_loss: 28.0666 - val_mse: 1958.1621\n",
      "Epoch 360/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2633 - mse: 1414.7570 - val_loss: 28.2107 - val_mse: 1915.4592\n",
      "Epoch 361/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5129 - mse: 1461.5980 - val_loss: 28.2836 - val_mse: 1926.5105\n",
      "Epoch 362/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7369 - mse: 1461.5458 - val_loss: 28.0346 - val_mse: 1972.3608\n",
      "Epoch 363/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.1982 - mse: 1414.2839 - val_loss: 28.6731 - val_mse: 1881.1437\n",
      "Epoch 364/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6724 - mse: 1464.8779 - val_loss: 28.2632 - val_mse: 1921.2302\n",
      "Epoch 365/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 20.9377 - mse: 1390.8632 - val_loss: 28.0818 - val_mse: 1978.4913\n",
      "Epoch 366/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6719 - mse: 1483.7958 - val_loss: 27.8949 - val_mse: 1952.8298\n",
      "Epoch 367/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5512 - mse: 1432.4324 - val_loss: 28.1273 - val_mse: 1966.0514\n",
      "Epoch 368/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.1363 - mse: 1378.9948 - val_loss: 27.9546 - val_mse: 1935.6381\n",
      "Epoch 369/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3965 - mse: 1440.5951 - val_loss: 28.1727 - val_mse: 1963.4478\n",
      "Epoch 370/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3820 - mse: 1440.1012 - val_loss: 27.9063 - val_mse: 1982.9697\n",
      "Epoch 371/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6193 - mse: 1460.4420 - val_loss: 28.0120 - val_mse: 1948.7611\n",
      "Epoch 372/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5299 - mse: 1457.4867 - val_loss: 28.3398 - val_mse: 1947.0381\n",
      "Epoch 373/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.1964 - mse: 1409.2941 - val_loss: 27.8653 - val_mse: 1926.1293\n",
      "Epoch 374/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5534 - mse: 1450.3474 - val_loss: 28.3655 - val_mse: 1932.1744\n",
      "Epoch 375/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.8764 - mse: 1504.0957 - val_loss: 28.1080 - val_mse: 1941.5259\n",
      "Epoch 376/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 20.9669 - mse: 1333.8157 - val_loss: 27.8417 - val_mse: 1946.7267\n",
      "Epoch 377/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.4375 - mse: 1463.2750 - val_loss: 27.9595 - val_mse: 1949.1212\n",
      "Epoch 378/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2771 - mse: 1442.4634 - val_loss: 28.1395 - val_mse: 1932.2302\n",
      "Epoch 379/500\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 21.2427 - mse: 1415.6174 - val_loss: 27.9677 - val_mse: 1964.7737\n",
      "Epoch 380/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.8417 - mse: 1483.8320 - val_loss: 28.2249 - val_mse: 1898.0121\n",
      "Epoch 381/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2294 - mse: 1391.0784 - val_loss: 28.0596 - val_mse: 1949.6154\n",
      "Epoch 382/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6477 - mse: 1469.8300 - val_loss: 27.9645 - val_mse: 1968.2517\n",
      "Epoch 383/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.3288 - mse: 1420.0565 - val_loss: 27.8998 - val_mse: 1943.7018\n",
      "Epoch 384/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 20.9620 - mse: 1364.3400 - val_loss: 27.9618 - val_mse: 1945.7950\n",
      "Epoch 385/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7058 - mse: 1512.9624 - val_loss: 28.1870 - val_mse: 1934.7611\n",
      "Epoch 386/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2331 - mse: 1405.9989 - val_loss: 27.9945 - val_mse: 1943.0992\n",
      "Epoch 387/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5174 - mse: 1448.6659 - val_loss: 28.0100 - val_mse: 1939.7173\n",
      "Epoch 388/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2725 - mse: 1427.5857 - val_loss: 27.9912 - val_mse: 1954.7903\n",
      "Epoch 389/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6747 - mse: 1471.3408 - val_loss: 27.9885 - val_mse: 1985.2063\n",
      "Epoch 390/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2682 - mse: 1397.7626 - val_loss: 28.1754 - val_mse: 1928.3237\n",
      "Epoch 391/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3717 - mse: 1438.9020 - val_loss: 28.0467 - val_mse: 1926.4736\n",
      "Epoch 392/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4186 - mse: 1464.9637 - val_loss: 28.0212 - val_mse: 1983.5387\n",
      "Epoch 393/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5408 - mse: 1448.3859 - val_loss: 27.9592 - val_mse: 1938.2157\n",
      "Epoch 394/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4671 - mse: 1430.1704 - val_loss: 28.0224 - val_mse: 1977.4653\n",
      "Epoch 395/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2118 - mse: 1413.1389 - val_loss: 28.2217 - val_mse: 1919.1770\n",
      "Epoch 396/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4131 - mse: 1461.4128 - val_loss: 28.0499 - val_mse: 1930.4507\n",
      "Epoch 397/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.1959 - mse: 1401.5001 - val_loss: 28.4080 - val_mse: 1939.0253\n",
      "Epoch 398/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3573 - mse: 1425.7434 - val_loss: 28.1849 - val_mse: 1965.0383\n",
      "Epoch 399/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4661 - mse: 1466.6040 - val_loss: 28.1394 - val_mse: 1942.0170\n",
      "Epoch 400/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6235 - mse: 1454.8909 - val_loss: 28.1651 - val_mse: 1946.4141\n",
      "Epoch 401/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.1773 - mse: 1411.5758 - val_loss: 28.2013 - val_mse: 1936.1948\n",
      "Epoch 402/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5797 - mse: 1438.8861 - val_loss: 28.1179 - val_mse: 1948.7885\n",
      "Epoch 403/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.1697 - mse: 1410.2635 - val_loss: 28.2398 - val_mse: 1953.3138\n",
      "Epoch 404/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2897 - mse: 1453.0154 - val_loss: 28.1928 - val_mse: 1952.7209\n",
      "Epoch 405/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2985 - mse: 1408.2103 - val_loss: 28.1572 - val_mse: 1989.7316\n",
      "Epoch 406/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.4639 - mse: 1442.0757 - val_loss: 28.1668 - val_mse: 1969.0062\n",
      "Epoch 407/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2806 - mse: 1419.0566 - val_loss: 28.5049 - val_mse: 1889.9841\n",
      "Epoch 408/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6384 - mse: 1455.9004 - val_loss: 28.1680 - val_mse: 1914.1757\n",
      "Epoch 409/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.2892 - mse: 1444.5001 - val_loss: 28.1967 - val_mse: 1913.1346\n",
      "Epoch 410/500\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 21.5598 - mse: 1464.3993 - val_loss: 27.9942 - val_mse: 1982.4810\n",
      "Epoch 411/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4557 - mse: 1416.5786 - val_loss: 28.2307 - val_mse: 1921.9559\n",
      "Epoch 412/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.0422 - mse: 1399.3602 - val_loss: 28.4600 - val_mse: 1935.9310\n",
      "Epoch 413/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.5760 - mse: 1476.0074 - val_loss: 28.2459 - val_mse: 1920.9437\n",
      "Epoch 414/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.2615 - mse: 1414.1350 - val_loss: 27.9787 - val_mse: 1942.3906\n",
      "Epoch 415/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6153 - mse: 1462.1265 - val_loss: 28.2469 - val_mse: 1920.0912\n",
      "Epoch 416/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2237 - mse: 1403.7257 - val_loss: 28.0287 - val_mse: 1933.0374\n",
      "Epoch 417/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3435 - mse: 1443.9691 - val_loss: 28.2482 - val_mse: 1943.9802\n",
      "Epoch 418/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.1009 - mse: 1381.2891 - val_loss: 28.4742 - val_mse: 1903.1987\n",
      "Epoch 419/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2796 - mse: 1434.9673 - val_loss: 28.3625 - val_mse: 1943.7371\n",
      "Epoch 420/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5918 - mse: 1466.8972 - val_loss: 28.0802 - val_mse: 1957.6757\n",
      "Epoch 421/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3564 - mse: 1411.4395 - val_loss: 28.1975 - val_mse: 1989.1729\n",
      "Epoch 422/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3613 - mse: 1429.3330 - val_loss: 28.1022 - val_mse: 1961.0073\n",
      "Epoch 423/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6026 - mse: 1485.0482 - val_loss: 28.2882 - val_mse: 1916.5577\n",
      "Epoch 424/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2479 - mse: 1398.9130 - val_loss: 28.1201 - val_mse: 1942.8661\n",
      "Epoch 425/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4036 - mse: 1456.1047 - val_loss: 28.0447 - val_mse: 1946.4622\n",
      "Epoch 426/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4220 - mse: 1447.7286 - val_loss: 28.1677 - val_mse: 1928.5710\n",
      "Epoch 427/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4945 - mse: 1441.5925 - val_loss: 28.3704 - val_mse: 1948.6449\n",
      "Epoch 428/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.0956 - mse: 1397.5021 - val_loss: 28.1731 - val_mse: 1984.9113\n",
      "Epoch 429/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5986 - mse: 1446.6549 - val_loss: 28.0011 - val_mse: 1958.4695\n",
      "Epoch 430/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.2324 - mse: 1408.9636 - val_loss: 28.1308 - val_mse: 1951.0449\n",
      "Epoch 431/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3638 - mse: 1452.2507 - val_loss: 28.1445 - val_mse: 1971.8246\n",
      "Epoch 432/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4739 - mse: 1434.8333 - val_loss: 28.3034 - val_mse: 1919.2152\n",
      "Epoch 433/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2055 - mse: 1403.6636 - val_loss: 27.9639 - val_mse: 1975.3695\n",
      "Epoch 434/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6020 - mse: 1482.7919 - val_loss: 27.8946 - val_mse: 1971.4681\n",
      "Epoch 435/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3388 - mse: 1401.8555 - val_loss: 28.1114 - val_mse: 1979.3512\n",
      "Epoch 436/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2698 - mse: 1429.7714 - val_loss: 28.0170 - val_mse: 1914.2196\n",
      "Epoch 437/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4698 - mse: 1462.5145 - val_loss: 28.2276 - val_mse: 1968.5817\n",
      "Epoch 438/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4396 - mse: 1437.8251 - val_loss: 28.2467 - val_mse: 1942.5300\n",
      "Epoch 439/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3189 - mse: 1424.0815 - val_loss: 28.3538 - val_mse: 1918.9686\n",
      "Epoch 440/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4361 - mse: 1443.9015 - val_loss: 28.1303 - val_mse: 1916.6044\n",
      "Epoch 441/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3277 - mse: 1427.6066 - val_loss: 28.2849 - val_mse: 1929.1248\n",
      "Epoch 442/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3565 - mse: 1431.0049 - val_loss: 28.0959 - val_mse: 2012.8752\n",
      "Epoch 443/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3627 - mse: 1417.7220 - val_loss: 27.9572 - val_mse: 1979.7703\n",
      "Epoch 444/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3204 - mse: 1443.2081 - val_loss: 28.2076 - val_mse: 1926.6720\n",
      "Epoch 445/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.3909 - mse: 1444.8215 - val_loss: 28.2162 - val_mse: 1949.9122\n",
      "Epoch 446/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.3137 - mse: 1431.4647 - val_loss: 28.3866 - val_mse: 1916.9165\n",
      "Epoch 447/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3318 - mse: 1431.5834 - val_loss: 28.0509 - val_mse: 1934.3180\n",
      "Epoch 448/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4238 - mse: 1439.9335 - val_loss: 28.2985 - val_mse: 1953.7061\n",
      "Epoch 449/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3572 - mse: 1431.5082 - val_loss: 28.3457 - val_mse: 1907.5903\n",
      "Epoch 450/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4348 - mse: 1432.4324 - val_loss: 27.9255 - val_mse: 1974.9147\n",
      "Epoch 451/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3764 - mse: 1442.4232 - val_loss: 28.1269 - val_mse: 1969.3512\n",
      "Epoch 452/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3979 - mse: 1434.0054 - val_loss: 28.2316 - val_mse: 1938.4880\n",
      "Epoch 453/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3211 - mse: 1431.4688 - val_loss: 28.0251 - val_mse: 1960.7535\n",
      "Epoch 454/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2807 - mse: 1400.1536 - val_loss: 28.2136 - val_mse: 1956.8336\n",
      "Epoch 455/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3939 - mse: 1457.1520 - val_loss: 28.1278 - val_mse: 1956.0651\n",
      "Epoch 456/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2652 - mse: 1412.8781 - val_loss: 28.1911 - val_mse: 1940.9141\n",
      "Epoch 457/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4511 - mse: 1452.5710 - val_loss: 28.4669 - val_mse: 1909.4926\n",
      "Epoch 458/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4168 - mse: 1443.1433 - val_loss: 28.4295 - val_mse: 1925.5071\n",
      "Epoch 459/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3541 - mse: 1411.0942 - val_loss: 28.4095 - val_mse: 1933.7465\n",
      "Epoch 460/500\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 21.3552 - mse: 1454.9702 - val_loss: 28.2722 - val_mse: 1974.1942\n",
      "Epoch 461/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2197 - mse: 1408.8777 - val_loss: 28.0792 - val_mse: 1965.5018\n",
      "Epoch 462/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.3972 - mse: 1436.8973 - val_loss: 28.2183 - val_mse: 1973.6921\n",
      "Epoch 463/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.4398 - mse: 1453.8947 - val_loss: 28.4188 - val_mse: 1969.5878\n",
      "Epoch 464/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3475 - mse: 1434.6947 - val_loss: 28.2013 - val_mse: 1949.2063\n",
      "Epoch 465/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2720 - mse: 1408.4266 - val_loss: 28.0891 - val_mse: 1936.6327\n",
      "Epoch 466/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4079 - mse: 1435.9185 - val_loss: 28.4102 - val_mse: 1977.4713\n",
      "Epoch 467/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4813 - mse: 1440.7664 - val_loss: 28.2373 - val_mse: 2011.4731\n",
      "Epoch 468/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6319 - mse: 1487.9545 - val_loss: 28.3604 - val_mse: 1992.2114\n",
      "Epoch 469/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2172 - mse: 1418.7106 - val_loss: 28.2101 - val_mse: 1983.6298\n",
      "Epoch 470/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.0755 - mse: 1383.2313 - val_loss: 28.2285 - val_mse: 1955.2731\n",
      "Epoch 471/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4154 - mse: 1431.4325 - val_loss: 28.0678 - val_mse: 1939.1193\n",
      "Epoch 472/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.1372 - mse: 1413.9357 - val_loss: 28.1694 - val_mse: 1918.0179\n",
      "Epoch 473/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4190 - mse: 1465.6796 - val_loss: 28.2959 - val_mse: 1972.4908\n",
      "Epoch 474/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.1902 - mse: 1378.6816 - val_loss: 28.0107 - val_mse: 1952.0105\n",
      "Epoch 475/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.1120 - mse: 1419.5885 - val_loss: 28.2694 - val_mse: 1996.5509\n",
      "Epoch 476/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.5294 - mse: 1468.6410 - val_loss: 27.8987 - val_mse: 1939.8755\n",
      "Epoch 477/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7286 - mse: 1473.5808 - val_loss: 28.0882 - val_mse: 1960.6483\n",
      "Epoch 478/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.1689 - mse: 1395.1381 - val_loss: 28.0761 - val_mse: 1960.9023\n",
      "Epoch 479/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.1810 - mse: 1427.7854 - val_loss: 28.3782 - val_mse: 1924.4390\n",
      "Epoch 480/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.2652 - mse: 1398.1121 - val_loss: 28.0139 - val_mse: 1955.6426\n",
      "Epoch 481/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4726 - mse: 1475.1969 - val_loss: 28.3169 - val_mse: 1934.3136\n",
      "Epoch 482/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.1669 - mse: 1404.5668 - val_loss: 28.0688 - val_mse: 1988.5195\n",
      "Epoch 483/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2978 - mse: 1451.5604 - val_loss: 28.1156 - val_mse: 1941.2588\n",
      "Epoch 484/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6909 - mse: 1464.2704 - val_loss: 28.1234 - val_mse: 1968.6245\n",
      "Epoch 485/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.1576 - mse: 1437.7379 - val_loss: 28.3337 - val_mse: 1952.5470\n",
      "Epoch 486/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4306 - mse: 1397.4032 - val_loss: 28.1225 - val_mse: 1955.8771\n",
      "Epoch 487/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2887 - mse: 1424.0298 - val_loss: 28.3386 - val_mse: 1970.6412\n",
      "Epoch 488/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.1497 - mse: 1433.5045 - val_loss: 28.1948 - val_mse: 1965.5938\n",
      "Epoch 489/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3248 - mse: 1390.9697 - val_loss: 28.2118 - val_mse: 1967.7057\n",
      "Epoch 490/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4718 - mse: 1480.5000 - val_loss: 28.1211 - val_mse: 1954.5048\n",
      "Epoch 491/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.4107 - mse: 1440.7635 - val_loss: 28.4559 - val_mse: 1969.4548\n",
      "Epoch 492/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.1731 - mse: 1408.6176 - val_loss: 28.0861 - val_mse: 1931.4658\n",
      "Epoch 493/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.1233 - mse: 1438.7634 - val_loss: 28.1242 - val_mse: 1974.2175\n",
      "Epoch 494/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4813 - mse: 1444.1569 - val_loss: 28.1895 - val_mse: 1944.7303\n",
      "Epoch 495/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4442 - mse: 1447.2838 - val_loss: 28.1173 - val_mse: 1919.5980\n",
      "Epoch 496/500\n",
      " 39/113 [=========>....................] - ETA: 0s - loss: 21.4582 - mse: 1433.2295WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 56500 batches). You may need to use the repeat() function when building your dataset.\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.1721 - mse: 1380.7284 - val_loss: 28.2850 - val_mse: 2009.8553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 20:30:22.906852: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_wind_dir0\n",
      "[39.73, 43.48]\n",
      "Epoch 1/500\n",
      "111/113 [============================>.] - ETA: 0s - loss: 46.9793 - mse: 3918.3401"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 20:30:23.770095: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 0s 3ms/step - loss: 46.8751 - mse: 3909.0056 - val_loss: 35.3602 - val_mse: 3041.5972\n",
      "Epoch 2/500\n",
      " 77/113 [===================>..........] - ETA: 0s - loss: 32.9234 - mse: 2708.3796"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 20:30:23.994091: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 0s 2ms/step - loss: 32.7986 - mse: 2704.5562 - val_loss: 34.1975 - val_mse: 3018.9719\n",
      "Epoch 3/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 32.8390 - mse: 2720.1335 - val_loss: 34.1437 - val_mse: 3016.2324\n",
      "Epoch 4/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 32.5414 - mse: 2690.9805 - val_loss: 33.9502 - val_mse: 2969.7656\n",
      "Epoch 5/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 32.6677 - mse: 2712.9727 - val_loss: 33.9112 - val_mse: 2978.2200\n",
      "Epoch 6/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 32.5732 - mse: 2687.2009 - val_loss: 33.8597 - val_mse: 2974.5603\n",
      "Epoch 7/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 32.3015 - mse: 2663.8105 - val_loss: 33.6513 - val_mse: 2927.3391\n",
      "Epoch 8/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 32.0560 - mse: 2614.2949 - val_loss: 33.5867 - val_mse: 2926.3611\n",
      "Epoch 9/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 32.2658 - mse: 2649.0603 - val_loss: 33.4369 - val_mse: 2895.7595\n",
      "Epoch 10/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 31.9422 - mse: 2626.3643 - val_loss: 33.3190 - val_mse: 2867.6526\n",
      "Epoch 11/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 32.0459 - mse: 2592.9792 - val_loss: 33.1912 - val_mse: 2853.0139\n",
      "Epoch 12/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 31.9022 - mse: 2588.2461 - val_loss: 33.0764 - val_mse: 2840.8469\n",
      "Epoch 13/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 31.5574 - mse: 2553.2383 - val_loss: 32.9473 - val_mse: 2820.4304\n",
      "Epoch 14/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 31.5428 - mse: 2540.0889 - val_loss: 32.8070 - val_mse: 2797.4353\n",
      "Epoch 15/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 31.2900 - mse: 2510.6025 - val_loss: 32.6371 - val_mse: 2767.5469\n",
      "Epoch 16/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 30.8942 - mse: 2435.5500 - val_loss: 32.4539 - val_mse: 2733.1194\n",
      "Epoch 17/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 31.2749 - mse: 2520.2769 - val_loss: 32.2614 - val_mse: 2698.0881\n",
      "Epoch 18/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 30.7681 - mse: 2431.4844 - val_loss: 32.0685 - val_mse: 2662.1162\n",
      "Epoch 19/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 30.5076 - mse: 2372.1384 - val_loss: 31.8542 - val_mse: 2629.0959\n",
      "Epoch 20/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 30.6258 - mse: 2428.0273 - val_loss: 31.6278 - val_mse: 2593.7683\n",
      "Epoch 21/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 29.8161 - mse: 2266.2737 - val_loss: 31.4164 - val_mse: 2560.7913\n",
      "Epoch 22/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 30.1807 - mse: 2355.7019 - val_loss: 31.1294 - val_mse: 2511.4775\n",
      "Epoch 23/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 29.7260 - mse: 2263.4314 - val_loss: 30.9486 - val_mse: 2482.0515\n",
      "Epoch 24/500\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 29.2165 - mse: 2196.6328 - val_loss: 30.5558 - val_mse: 2425.3035\n",
      "Epoch 25/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 29.3297 - mse: 2233.9653 - val_loss: 30.3434 - val_mse: 2383.9351\n",
      "Epoch 26/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 28.6958 - mse: 2154.8669 - val_loss: 29.9135 - val_mse: 2342.5872\n",
      "Epoch 27/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 28.3097 - mse: 2063.9565 - val_loss: 29.5177 - val_mse: 2277.5015\n",
      "Epoch 28/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 28.1033 - mse: 2097.3403 - val_loss: 29.0831 - val_mse: 2231.5923\n",
      "Epoch 29/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 27.8624 - mse: 2060.3501 - val_loss: 28.6792 - val_mse: 2188.0813\n",
      "Epoch 30/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 27.0536 - mse: 1906.5092 - val_loss: 28.2448 - val_mse: 2125.2759\n",
      "Epoch 31/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 26.4710 - mse: 1857.9491 - val_loss: 27.8349 - val_mse: 2106.0076\n",
      "Epoch 32/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 26.3953 - mse: 1919.1283 - val_loss: 27.3487 - val_mse: 2019.8788\n",
      "Epoch 33/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.7730 - mse: 1797.3962 - val_loss: 26.9373 - val_mse: 1979.0906\n",
      "Epoch 34/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.5350 - mse: 1823.4097 - val_loss: 26.5878 - val_mse: 1930.7673\n",
      "Epoch 35/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1978 - mse: 1756.5415 - val_loss: 26.2222 - val_mse: 1900.6102\n",
      "Epoch 36/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.5209 - mse: 1698.4065 - val_loss: 25.9602 - val_mse: 1908.8771\n",
      "Epoch 37/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.7699 - mse: 1716.6174 - val_loss: 25.7438 - val_mse: 1876.3002\n",
      "Epoch 38/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3078 - mse: 1677.4622 - val_loss: 25.5931 - val_mse: 1835.0271\n",
      "Epoch 39/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.3048 - mse: 1680.2062 - val_loss: 25.6999 - val_mse: 1787.7988\n",
      "Epoch 40/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.0438 - mse: 1634.9984 - val_loss: 25.6063 - val_mse: 1768.8073\n",
      "Epoch 41/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.4285 - mse: 1659.1465 - val_loss: 25.3684 - val_mse: 1773.4258\n",
      "Epoch 42/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2203 - mse: 1668.5532 - val_loss: 25.2535 - val_mse: 1827.6714\n",
      "Epoch 43/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.2623 - mse: 1691.5974 - val_loss: 25.1001 - val_mse: 1785.6193\n",
      "Epoch 44/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.5303 - mse: 1575.1294 - val_loss: 25.0906 - val_mse: 1742.1044\n",
      "Epoch 45/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.5990 - mse: 1581.3892 - val_loss: 25.0674 - val_mse: 1818.1129\n",
      "Epoch 46/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7908 - mse: 1628.6006 - val_loss: 24.8631 - val_mse: 1751.8600\n",
      "Epoch 47/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 23.8126 - mse: 1621.1241 - val_loss: 24.9069 - val_mse: 1716.1306\n",
      "Epoch 48/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.6968 - mse: 1589.1787 - val_loss: 24.7398 - val_mse: 1734.5197\n",
      "Epoch 49/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.7439 - mse: 1596.6818 - val_loss: 24.7298 - val_mse: 1709.1711\n",
      "Epoch 50/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.1949 - mse: 1558.0658 - val_loss: 24.7127 - val_mse: 1696.7183\n",
      "Epoch 51/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4075 - mse: 1553.0524 - val_loss: 24.6796 - val_mse: 1685.8356\n",
      "Epoch 52/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4813 - mse: 1567.8905 - val_loss: 24.5389 - val_mse: 1721.1138\n",
      "Epoch 53/500\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 23.6837 - mse: 1618.7168 - val_loss: 24.4945 - val_mse: 1696.5614\n",
      "Epoch 54/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.0456 - mse: 1512.4952 - val_loss: 24.4547 - val_mse: 1715.9614\n",
      "Epoch 55/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3564 - mse: 1540.9987 - val_loss: 24.4170 - val_mse: 1716.2057\n",
      "Epoch 56/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4115 - mse: 1606.7479 - val_loss: 24.3637 - val_mse: 1692.7584\n",
      "Epoch 57/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4752 - mse: 1559.2118 - val_loss: 24.4308 - val_mse: 1649.8483\n",
      "Epoch 58/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.0807 - mse: 1519.6838 - val_loss: 24.3100 - val_mse: 1679.3855\n",
      "Epoch 59/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4961 - mse: 1579.5693 - val_loss: 24.2926 - val_mse: 1687.8243\n",
      "Epoch 60/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.9813 - mse: 1485.4362 - val_loss: 24.2787 - val_mse: 1692.4630\n",
      "Epoch 61/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2280 - mse: 1549.6873 - val_loss: 24.2441 - val_mse: 1678.8271\n",
      "Epoch 62/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3060 - mse: 1547.6821 - val_loss: 24.2510 - val_mse: 1651.9872\n",
      "Epoch 63/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2600 - mse: 1545.4204 - val_loss: 24.2016 - val_mse: 1662.9241\n",
      "Epoch 64/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.9291 - mse: 1492.9053 - val_loss: 24.1888 - val_mse: 1682.0784\n",
      "Epoch 65/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.1713 - mse: 1533.1266 - val_loss: 24.1437 - val_mse: 1667.0995\n",
      "Epoch 66/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.4083 - mse: 1553.3323 - val_loss: 24.1895 - val_mse: 1622.2126\n",
      "Epoch 67/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.1414 - mse: 1529.5680 - val_loss: 24.1583 - val_mse: 1623.5592\n",
      "Epoch 68/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.8290 - mse: 1466.9191 - val_loss: 24.0933 - val_mse: 1648.4171\n",
      "Epoch 69/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2292 - mse: 1541.5555 - val_loss: 24.2261 - val_mse: 1683.6860\n",
      "Epoch 70/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.9057 - mse: 1490.3885 - val_loss: 24.2545 - val_mse: 1712.5585\n",
      "Epoch 71/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.0421 - mse: 1494.7783 - val_loss: 24.0438 - val_mse: 1633.8252\n",
      "Epoch 72/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.9788 - mse: 1527.8135 - val_loss: 24.0419 - val_mse: 1633.7883\n",
      "Epoch 73/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2100 - mse: 1514.5215 - val_loss: 24.0541 - val_mse: 1662.1409\n",
      "Epoch 74/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.9163 - mse: 1491.6620 - val_loss: 24.1385 - val_mse: 1594.3146\n",
      "Epoch 75/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.1935 - mse: 1510.6956 - val_loss: 24.0693 - val_mse: 1605.5338\n",
      "Epoch 76/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.0919 - mse: 1489.4109 - val_loss: 23.9788 - val_mse: 1640.1554\n",
      "Epoch 77/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.8688 - mse: 1484.0066 - val_loss: 23.9890 - val_mse: 1613.1189\n",
      "Epoch 78/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.8752 - mse: 1474.3206 - val_loss: 23.9577 - val_mse: 1637.5753\n",
      "Epoch 79/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.8777 - mse: 1489.2975 - val_loss: 23.9917 - val_mse: 1618.2574\n",
      "Epoch 80/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2811 - mse: 1520.8444 - val_loss: 23.9407 - val_mse: 1618.6766\n",
      "Epoch 81/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.3552 - mse: 1401.8014 - val_loss: 23.9626 - val_mse: 1651.0565\n",
      "Epoch 82/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.2123 - mse: 1532.4576 - val_loss: 23.9203 - val_mse: 1624.1344\n",
      "Epoch 83/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.0811 - mse: 1522.0339 - val_loss: 23.9454 - val_mse: 1602.5898\n",
      "Epoch 84/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.9986 - mse: 1466.2887 - val_loss: 23.9115 - val_mse: 1630.1775\n",
      "Epoch 85/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 22.9825 - mse: 1503.1960 - val_loss: 23.9274 - val_mse: 1590.5044\n",
      "Epoch 86/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.0341 - mse: 1499.3123 - val_loss: 23.9860 - val_mse: 1588.7704\n",
      "Epoch 87/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.6345 - mse: 1440.5089 - val_loss: 23.8826 - val_mse: 1595.9835\n",
      "Epoch 88/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.3129 - mse: 1501.3678 - val_loss: 23.8581 - val_mse: 1618.1090\n",
      "Epoch 89/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.7592 - mse: 1470.4503 - val_loss: 23.8718 - val_mse: 1618.1326\n",
      "Epoch 90/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.1102 - mse: 1504.7709 - val_loss: 23.9625 - val_mse: 1564.9104\n",
      "Epoch 91/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.8182 - mse: 1441.8877 - val_loss: 23.8412 - val_mse: 1603.6594\n",
      "Epoch 92/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.9118 - mse: 1492.1940 - val_loss: 23.8319 - val_mse: 1601.4312\n",
      "Epoch 93/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 22.8275 - mse: 1475.6104 - val_loss: 23.8550 - val_mse: 1589.4534\n",
      "Epoch 94/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.6180 - mse: 1396.0123 - val_loss: 23.8105 - val_mse: 1607.5906\n",
      "Epoch 95/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.1436 - mse: 1544.1896 - val_loss: 23.9472 - val_mse: 1559.0498\n",
      "Epoch 96/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.7899 - mse: 1467.2681 - val_loss: 24.0237 - val_mse: 1547.4460\n",
      "Epoch 97/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.8847 - mse: 1457.0156 - val_loss: 23.8085 - val_mse: 1613.2574\n",
      "Epoch 98/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.9518 - mse: 1485.0642 - val_loss: 23.8109 - val_mse: 1581.2780\n",
      "Epoch 99/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.6728 - mse: 1445.7480 - val_loss: 23.8383 - val_mse: 1564.9294\n",
      "Epoch 100/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.8378 - mse: 1464.2228 - val_loss: 23.8633 - val_mse: 1642.8590\n",
      "Epoch 101/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.0038 - mse: 1485.4962 - val_loss: 23.7811 - val_mse: 1577.4258\n",
      "Epoch 102/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.8863 - mse: 1469.1226 - val_loss: 23.7649 - val_mse: 1585.5028\n",
      "Epoch 103/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.5797 - mse: 1417.6854 - val_loss: 23.7928 - val_mse: 1565.8674\n",
      "Epoch 104/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.6994 - mse: 1451.4667 - val_loss: 23.7737 - val_mse: 1599.6093\n",
      "Epoch 105/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 23.1674 - mse: 1498.2819 - val_loss: 23.7608 - val_mse: 1584.0779\n",
      "Epoch 106/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.6642 - mse: 1447.9886 - val_loss: 23.8178 - val_mse: 1549.8745\n",
      "Epoch 107/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.7685 - mse: 1446.8921 - val_loss: 23.7912 - val_mse: 1554.7024\n",
      "Epoch 108/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.8326 - mse: 1467.0938 - val_loss: 23.7455 - val_mse: 1572.9296\n",
      "Epoch 109/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.7359 - mse: 1450.2272 - val_loss: 23.8701 - val_mse: 1633.9762\n",
      "Epoch 110/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.8127 - mse: 1458.5747 - val_loss: 23.7691 - val_mse: 1552.2538\n",
      "Epoch 111/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.7491 - mse: 1446.9958 - val_loss: 23.7301 - val_mse: 1573.6965\n",
      "Epoch 112/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.7431 - mse: 1445.6691 - val_loss: 23.7548 - val_mse: 1595.2589\n",
      "Epoch 113/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.7380 - mse: 1448.5983 - val_loss: 23.8479 - val_mse: 1621.5398\n",
      "Epoch 114/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.7088 - mse: 1446.4485 - val_loss: 23.7401 - val_mse: 1568.0085\n",
      "Epoch 115/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.8532 - mse: 1477.4321 - val_loss: 23.7251 - val_mse: 1570.6487\n",
      "Epoch 116/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.6818 - mse: 1420.2017 - val_loss: 23.7456 - val_mse: 1554.8093\n",
      "Epoch 117/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.7127 - mse: 1445.0013 - val_loss: 23.7629 - val_mse: 1542.3401\n",
      "Epoch 118/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.7487 - mse: 1454.4318 - val_loss: 23.7056 - val_mse: 1570.5874\n",
      "Epoch 119/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.7252 - mse: 1445.2727 - val_loss: 23.7222 - val_mse: 1563.6051\n",
      "Epoch 120/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.6488 - mse: 1440.8491 - val_loss: 23.7333 - val_mse: 1554.9490\n",
      "Epoch 121/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.8445 - mse: 1461.8450 - val_loss: 23.8375 - val_mse: 1527.1398\n",
      "Epoch 122/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.4764 - mse: 1412.3542 - val_loss: 23.7238 - val_mse: 1535.2979\n",
      "Epoch 123/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.8843 - mse: 1478.3608 - val_loss: 23.7016 - val_mse: 1548.7369\n",
      "Epoch 124/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.5854 - mse: 1412.9332 - val_loss: 23.6566 - val_mse: 1574.3923\n",
      "Epoch 125/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.4954 - mse: 1422.5211 - val_loss: 23.6646 - val_mse: 1574.1565\n",
      "Epoch 126/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.8055 - mse: 1461.4077 - val_loss: 23.7200 - val_mse: 1544.0792\n",
      "Epoch 127/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.7894 - mse: 1458.4087 - val_loss: 23.6930 - val_mse: 1582.2347\n",
      "Epoch 128/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.4564 - mse: 1418.0652 - val_loss: 23.6351 - val_mse: 1553.9036\n",
      "Epoch 129/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.4960 - mse: 1406.0663 - val_loss: 23.6885 - val_mse: 1608.9368\n",
      "Epoch 130/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.4525 - mse: 1435.3505 - val_loss: 23.6347 - val_mse: 1564.9122\n",
      "Epoch 131/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.8284 - mse: 1457.9437 - val_loss: 23.5975 - val_mse: 1567.5905\n",
      "Epoch 132/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.6018 - mse: 1440.0935 - val_loss: 23.6121 - val_mse: 1539.1582\n",
      "Epoch 133/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.3533 - mse: 1398.0486 - val_loss: 23.7297 - val_mse: 1620.1174\n",
      "Epoch 134/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.4815 - mse: 1415.7363 - val_loss: 23.6049 - val_mse: 1555.8447\n",
      "Epoch 135/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.8716 - mse: 1484.8672 - val_loss: 23.5675 - val_mse: 1561.8080\n",
      "Epoch 136/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.2888 - mse: 1406.0502 - val_loss: 23.8340 - val_mse: 1508.6996\n",
      "Epoch 137/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.7187 - mse: 1452.9279 - val_loss: 23.6006 - val_mse: 1538.3959\n",
      "Epoch 138/500\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 22.4576 - mse: 1403.5402 - val_loss: 23.5616 - val_mse: 1556.6375\n",
      "Epoch 139/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.3093 - mse: 1395.3208 - val_loss: 23.5941 - val_mse: 1535.2399\n",
      "Epoch 140/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.5749 - mse: 1407.3257 - val_loss: 23.5735 - val_mse: 1581.9011\n",
      "Epoch 141/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.4155 - mse: 1428.3710 - val_loss: 23.5353 - val_mse: 1551.1559\n",
      "Epoch 142/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.2786 - mse: 1370.8871 - val_loss: 23.5923 - val_mse: 1591.4606\n",
      "Epoch 143/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.7490 - mse: 1493.4066 - val_loss: 23.6322 - val_mse: 1508.4470\n",
      "Epoch 144/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.5000 - mse: 1425.2878 - val_loss: 23.5319 - val_mse: 1549.0017\n",
      "Epoch 145/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.4618 - mse: 1421.5940 - val_loss: 23.5169 - val_mse: 1552.4896\n",
      "Epoch 146/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.5009 - mse: 1413.7131 - val_loss: 23.5328 - val_mse: 1535.7872\n",
      "Epoch 147/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.3453 - mse: 1393.3265 - val_loss: 23.5772 - val_mse: 1501.8519\n",
      "Epoch 148/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.3836 - mse: 1409.2272 - val_loss: 23.5418 - val_mse: 1528.2866\n",
      "Epoch 149/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.6582 - mse: 1441.1122 - val_loss: 23.5198 - val_mse: 1521.5046\n",
      "Epoch 150/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.1763 - mse: 1404.2325 - val_loss: 23.5230 - val_mse: 1545.6897\n",
      "Epoch 151/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.4743 - mse: 1413.9962 - val_loss: 23.5133 - val_mse: 1559.5817\n",
      "Epoch 152/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.4519 - mse: 1413.4965 - val_loss: 23.5140 - val_mse: 1578.4518\n",
      "Epoch 153/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.3629 - mse: 1379.2982 - val_loss: 23.5378 - val_mse: 1510.9335\n",
      "Epoch 154/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.3136 - mse: 1405.8314 - val_loss: 23.4836 - val_mse: 1552.3136\n",
      "Epoch 155/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.2808 - mse: 1387.8516 - val_loss: 23.6006 - val_mse: 1500.8308\n",
      "Epoch 156/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.4191 - mse: 1413.2782 - val_loss: 23.5280 - val_mse: 1505.1102\n",
      "Epoch 157/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.4708 - mse: 1437.7390 - val_loss: 23.4943 - val_mse: 1512.3424\n",
      "Epoch 158/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.3312 - mse: 1378.3490 - val_loss: 23.5354 - val_mse: 1497.6305\n",
      "Epoch 159/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.0655 - mse: 1376.1497 - val_loss: 23.5185 - val_mse: 1504.0861\n",
      "Epoch 160/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 22.7391 - mse: 1448.2625 - val_loss: 23.5099 - val_mse: 1526.7418\n",
      "Epoch 161/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.0678 - mse: 1355.1178 - val_loss: 23.5054 - val_mse: 1523.2334\n",
      "Epoch 162/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.5352 - mse: 1446.9086 - val_loss: 23.4575 - val_mse: 1546.3010\n",
      "Epoch 163/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.2818 - mse: 1385.5049 - val_loss: 23.4465 - val_mse: 1557.2379\n",
      "Epoch 164/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.0329 - mse: 1351.6117 - val_loss: 23.4424 - val_mse: 1540.6021\n",
      "Epoch 165/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.3636 - mse: 1399.0800 - val_loss: 23.4786 - val_mse: 1564.2490\n",
      "Epoch 166/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.3019 - mse: 1406.4231 - val_loss: 23.4603 - val_mse: 1566.8588\n",
      "Epoch 167/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.4555 - mse: 1438.8000 - val_loss: 23.4515 - val_mse: 1558.3658\n",
      "Epoch 168/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.2266 - mse: 1363.5560 - val_loss: 23.5467 - val_mse: 1578.6482\n",
      "Epoch 169/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.2013 - mse: 1398.7882 - val_loss: 23.4825 - val_mse: 1501.3513\n",
      "Epoch 170/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.5191 - mse: 1416.2543 - val_loss: 23.4350 - val_mse: 1538.0382\n",
      "Epoch 171/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.2423 - mse: 1365.4967 - val_loss: 23.4344 - val_mse: 1538.7472\n",
      "Epoch 172/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.9976 - mse: 1394.0430 - val_loss: 23.4411 - val_mse: 1542.0704\n",
      "Epoch 173/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.7724 - mse: 1436.4678 - val_loss: 23.4981 - val_mse: 1579.7693\n",
      "Epoch 174/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7943 - mse: 1340.0192 - val_loss: 23.4203 - val_mse: 1533.8127\n",
      "Epoch 175/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.7652 - mse: 1453.6338 - val_loss: 23.4663 - val_mse: 1561.5370\n",
      "Epoch 176/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.6039 - mse: 1282.2189 - val_loss: 23.4184 - val_mse: 1531.8949\n",
      "Epoch 177/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.5058 - mse: 1446.9551 - val_loss: 23.4290 - val_mse: 1532.7395\n",
      "Epoch 178/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.5588 - mse: 1413.7999 - val_loss: 23.4504 - val_mse: 1515.9929\n",
      "Epoch 179/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.1867 - mse: 1385.0774 - val_loss: 23.5621 - val_mse: 1483.1830\n",
      "Epoch 180/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.3458 - mse: 1399.9924 - val_loss: 23.4368 - val_mse: 1535.0747\n",
      "Epoch 181/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.9581 - mse: 1340.2048 - val_loss: 23.4268 - val_mse: 1508.6519\n",
      "Epoch 182/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.4615 - mse: 1433.7335 - val_loss: 23.4190 - val_mse: 1519.6675\n",
      "Epoch 183/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.0520 - mse: 1358.0416 - val_loss: 23.5282 - val_mse: 1581.7849\n",
      "Epoch 184/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 22.4258 - mse: 1382.8462 - val_loss: 23.5287 - val_mse: 1489.8867\n",
      "Epoch 185/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.9991 - mse: 1367.6199 - val_loss: 23.4179 - val_mse: 1506.2660\n",
      "Epoch 186/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.2965 - mse: 1398.2921 - val_loss: 23.3922 - val_mse: 1542.1533\n",
      "Epoch 187/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.2258 - mse: 1364.2137 - val_loss: 23.3958 - val_mse: 1523.7477\n",
      "Epoch 188/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.2936 - mse: 1395.2085 - val_loss: 23.5793 - val_mse: 1471.2609\n",
      "Epoch 189/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8755 - mse: 1361.2487 - val_loss: 23.4180 - val_mse: 1525.6011\n",
      "Epoch 190/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.2735 - mse: 1381.6365 - val_loss: 23.4074 - val_mse: 1559.1453\n",
      "Epoch 191/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.3245 - mse: 1365.9403 - val_loss: 23.4494 - val_mse: 1496.8582\n",
      "Epoch 192/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.0876 - mse: 1386.8312 - val_loss: 23.4474 - val_mse: 1505.0430\n",
      "Epoch 193/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.4156 - mse: 1428.4541 - val_loss: 23.4232 - val_mse: 1503.1816\n",
      "Epoch 194/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.9727 - mse: 1349.5437 - val_loss: 23.3861 - val_mse: 1526.8566\n",
      "Epoch 195/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.0783 - mse: 1374.0206 - val_loss: 23.3874 - val_mse: 1518.8588\n",
      "Epoch 196/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.2766 - mse: 1350.6975 - val_loss: 23.4023 - val_mse: 1518.4324\n",
      "Epoch 197/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.5110 - mse: 1449.8291 - val_loss: 23.5201 - val_mse: 1478.9897\n",
      "Epoch 198/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.9937 - mse: 1355.0291 - val_loss: 23.4116 - val_mse: 1528.4979\n",
      "Epoch 199/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.1556 - mse: 1377.5616 - val_loss: 23.5350 - val_mse: 1476.4769\n",
      "Epoch 200/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8030 - mse: 1328.5863 - val_loss: 23.4240 - val_mse: 1547.1149\n",
      "Epoch 201/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 22.2099 - mse: 1386.2275 - val_loss: 23.4060 - val_mse: 1516.7299\n",
      "Epoch 202/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.2364 - mse: 1364.1763 - val_loss: 23.4612 - val_mse: 1484.1707\n",
      "Epoch 203/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7542 - mse: 1325.2739 - val_loss: 23.4760 - val_mse: 1487.9878\n",
      "Epoch 204/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.3947 - mse: 1418.7369 - val_loss: 23.4272 - val_mse: 1509.4746\n",
      "Epoch 205/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.9314 - mse: 1341.2972 - val_loss: 23.4190 - val_mse: 1517.8307\n",
      "Epoch 206/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.2763 - mse: 1408.0889 - val_loss: 23.4146 - val_mse: 1512.1737\n",
      "Epoch 207/500\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 22.3722 - mse: 1400.3662 - val_loss: 23.3953 - val_mse: 1509.3389\n",
      "Epoch 208/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.9253 - mse: 1332.0996 - val_loss: 23.4956 - val_mse: 1561.3597\n",
      "Epoch 209/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.1714 - mse: 1378.7487 - val_loss: 23.4225 - val_mse: 1497.5082\n",
      "Epoch 210/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.9311 - mse: 1363.0026 - val_loss: 23.6105 - val_mse: 1473.1171\n",
      "Epoch 211/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.2073 - mse: 1369.2124 - val_loss: 23.4369 - val_mse: 1488.4219\n",
      "Epoch 212/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.1456 - mse: 1385.8497 - val_loss: 23.6243 - val_mse: 1460.8435\n",
      "Epoch 213/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8456 - mse: 1338.7926 - val_loss: 23.4157 - val_mse: 1546.9464\n",
      "Epoch 214/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.2448 - mse: 1387.0514 - val_loss: 23.4022 - val_mse: 1537.7577\n",
      "Epoch 215/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.0918 - mse: 1369.9685 - val_loss: 23.4439 - val_mse: 1491.1188\n",
      "Epoch 216/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.0434 - mse: 1363.8737 - val_loss: 23.5704 - val_mse: 1463.4821\n",
      "Epoch 217/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.1011 - mse: 1363.9829 - val_loss: 23.4800 - val_mse: 1478.9231\n",
      "Epoch 218/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.9654 - mse: 1350.8721 - val_loss: 23.3933 - val_mse: 1509.3987\n",
      "Epoch 219/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.0864 - mse: 1364.3331 - val_loss: 23.4341 - val_mse: 1485.4137\n",
      "Epoch 220/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8888 - mse: 1348.3806 - val_loss: 23.4075 - val_mse: 1492.0544\n",
      "Epoch 221/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.0407 - mse: 1367.0192 - val_loss: 23.3853 - val_mse: 1534.4600\n",
      "Epoch 222/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.1521 - mse: 1375.7444 - val_loss: 23.3798 - val_mse: 1506.2041\n",
      "Epoch 223/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.9777 - mse: 1351.2964 - val_loss: 23.4302 - val_mse: 1489.6376\n",
      "Epoch 224/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.0299 - mse: 1360.0891 - val_loss: 23.3975 - val_mse: 1508.2964\n",
      "Epoch 225/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 22.0408 - mse: 1361.9866 - val_loss: 23.4263 - val_mse: 1551.6814\n",
      "Epoch 226/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.9839 - mse: 1353.8882 - val_loss: 23.4163 - val_mse: 1498.2942\n",
      "Epoch 227/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.0749 - mse: 1359.3357 - val_loss: 23.4211 - val_mse: 1543.0636\n",
      "Epoch 228/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 22.0113 - mse: 1363.0005 - val_loss: 23.4996 - val_mse: 1495.1722\n",
      "Epoch 229/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.0806 - mse: 1368.0293 - val_loss: 23.4727 - val_mse: 1481.3428\n",
      "Epoch 230/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8044 - mse: 1323.5144 - val_loss: 23.3942 - val_mse: 1496.7926\n",
      "Epoch 231/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.1099 - mse: 1381.1198 - val_loss: 23.4024 - val_mse: 1491.2273\n",
      "Epoch 232/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.0280 - mse: 1369.5137 - val_loss: 23.4187 - val_mse: 1501.3306\n",
      "Epoch 233/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7797 - mse: 1323.9683 - val_loss: 23.3867 - val_mse: 1510.1063\n",
      "Epoch 234/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.1283 - mse: 1385.5201 - val_loss: 23.4466 - val_mse: 1474.4911\n",
      "Epoch 235/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.9378 - mse: 1338.8458 - val_loss: 23.4301 - val_mse: 1484.9186\n",
      "Epoch 236/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.0044 - mse: 1356.3500 - val_loss: 23.5185 - val_mse: 1463.9122\n",
      "Epoch 237/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.9237 - mse: 1325.5737 - val_loss: 23.3579 - val_mse: 1523.8624\n",
      "Epoch 238/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.0515 - mse: 1380.1102 - val_loss: 23.3682 - val_mse: 1516.6483\n",
      "Epoch 239/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8084 - mse: 1329.9729 - val_loss: 23.3550 - val_mse: 1509.2310\n",
      "Epoch 240/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.1217 - mse: 1375.6124 - val_loss: 23.3811 - val_mse: 1503.9600\n",
      "Epoch 241/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7790 - mse: 1310.2017 - val_loss: 23.3908 - val_mse: 1504.6168\n",
      "Epoch 242/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.9732 - mse: 1363.3890 - val_loss: 23.3640 - val_mse: 1518.6309\n",
      "Epoch 243/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 22.1876 - mse: 1379.6486 - val_loss: 23.7124 - val_mse: 1444.1493\n",
      "Epoch 244/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7765 - mse: 1343.0886 - val_loss: 23.3755 - val_mse: 1517.5558\n",
      "Epoch 245/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.9146 - mse: 1343.0035 - val_loss: 23.3593 - val_mse: 1516.3427\n",
      "Epoch 246/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.9091 - mse: 1354.7069 - val_loss: 23.3678 - val_mse: 1510.0851\n",
      "Epoch 247/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.0242 - mse: 1338.5717 - val_loss: 23.3804 - val_mse: 1497.6113\n",
      "Epoch 248/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.0336 - mse: 1385.0074 - val_loss: 23.4030 - val_mse: 1493.8314\n",
      "Epoch 249/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8339 - mse: 1313.9155 - val_loss: 23.3860 - val_mse: 1536.3002\n",
      "Epoch 250/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.0551 - mse: 1361.5293 - val_loss: 23.3791 - val_mse: 1492.4139\n",
      "Epoch 251/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8890 - mse: 1347.2469 - val_loss: 23.4135 - val_mse: 1499.2987\n",
      "Epoch 252/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.9757 - mse: 1336.1362 - val_loss: 23.4426 - val_mse: 1478.4888\n",
      "Epoch 253/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7560 - mse: 1334.7455 - val_loss: 23.3782 - val_mse: 1508.0723\n",
      "Epoch 254/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.9453 - mse: 1335.1584 - val_loss: 23.4068 - val_mse: 1532.7115\n",
      "Epoch 255/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6126 - mse: 1317.8716 - val_loss: 23.3480 - val_mse: 1502.9655\n",
      "Epoch 256/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.2327 - mse: 1377.3357 - val_loss: 23.3824 - val_mse: 1501.1647\n",
      "Epoch 257/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.9201 - mse: 1378.3412 - val_loss: 23.4551 - val_mse: 1476.3110\n",
      "Epoch 258/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8104 - mse: 1314.2032 - val_loss: 23.3705 - val_mse: 1512.9225\n",
      "Epoch 259/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.9116 - mse: 1359.0753 - val_loss: 23.3688 - val_mse: 1533.9504\n",
      "Epoch 260/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.9467 - mse: 1339.5212 - val_loss: 23.4222 - val_mse: 1469.0354\n",
      "Epoch 261/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.9405 - mse: 1350.5525 - val_loss: 23.4593 - val_mse: 1474.5587\n",
      "Epoch 262/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.9907 - mse: 1349.6062 - val_loss: 23.3540 - val_mse: 1499.8794\n",
      "Epoch 263/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5987 - mse: 1290.5582 - val_loss: 23.3647 - val_mse: 1528.6881\n",
      "Epoch 264/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8224 - mse: 1350.9449 - val_loss: 23.4367 - val_mse: 1481.4896\n",
      "Epoch 265/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.0204 - mse: 1343.0712 - val_loss: 23.4045 - val_mse: 1529.5483\n",
      "Epoch 266/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7123 - mse: 1334.5927 - val_loss: 23.4196 - val_mse: 1547.3911\n",
      "Epoch 267/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.1130 - mse: 1369.2693 - val_loss: 23.6465 - val_mse: 1443.0924\n",
      "Epoch 268/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.0049 - mse: 1370.5981 - val_loss: 23.7843 - val_mse: 1435.3174\n",
      "Epoch 269/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.3034 - mse: 1279.1350 - val_loss: 23.3702 - val_mse: 1504.6577\n",
      "Epoch 270/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.9726 - mse: 1331.7275 - val_loss: 23.3564 - val_mse: 1516.5898\n",
      "Epoch 271/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.0250 - mse: 1365.9189 - val_loss: 23.3587 - val_mse: 1490.0447\n",
      "Epoch 272/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8043 - mse: 1347.0970 - val_loss: 23.3507 - val_mse: 1496.6632\n",
      "Epoch 273/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6727 - mse: 1298.2712 - val_loss: 23.3919 - val_mse: 1511.2252\n",
      "Epoch 274/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.1390 - mse: 1375.0256 - val_loss: 23.5184 - val_mse: 1457.7378\n",
      "Epoch 275/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5186 - mse: 1293.6223 - val_loss: 23.4327 - val_mse: 1489.4236\n",
      "Epoch 276/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.9060 - mse: 1365.8033 - val_loss: 23.4326 - val_mse: 1479.3101\n",
      "Epoch 277/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8447 - mse: 1307.9930 - val_loss: 23.4543 - val_mse: 1559.7819\n",
      "Epoch 278/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5424 - mse: 1310.6586 - val_loss: 23.4278 - val_mse: 1460.4076\n",
      "Epoch 279/500\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 22.2512 - mse: 1378.7421 - val_loss: 23.3814 - val_mse: 1483.3796\n",
      "Epoch 280/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7293 - mse: 1317.7621 - val_loss: 23.3514 - val_mse: 1506.4080\n",
      "Epoch 281/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8212 - mse: 1352.1534 - val_loss: 23.3677 - val_mse: 1481.0778\n",
      "Epoch 282/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6918 - mse: 1315.2946 - val_loss: 23.4904 - val_mse: 1458.4205\n",
      "Epoch 283/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.8620 - mse: 1348.2953 - val_loss: 23.4308 - val_mse: 1469.4357\n",
      "Epoch 284/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 22.0691 - mse: 1352.5225 - val_loss: 23.3768 - val_mse: 1487.4972\n",
      "Epoch 285/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7388 - mse: 1330.8433 - val_loss: 23.3453 - val_mse: 1510.5375\n",
      "Epoch 286/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4530 - mse: 1274.8971 - val_loss: 23.4178 - val_mse: 1546.9703\n",
      "Epoch 287/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7942 - mse: 1343.9203 - val_loss: 23.3457 - val_mse: 1496.5692\n",
      "Epoch 288/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7880 - mse: 1331.9919 - val_loss: 23.3578 - val_mse: 1497.7104\n",
      "Epoch 289/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.9733 - mse: 1364.8031 - val_loss: 23.3942 - val_mse: 1470.7162\n",
      "Epoch 290/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3850 - mse: 1275.3706 - val_loss: 23.3955 - val_mse: 1524.5902\n",
      "Epoch 291/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.9640 - mse: 1356.2242 - val_loss: 23.3970 - val_mse: 1469.7961\n",
      "Epoch 292/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.9692 - mse: 1326.7438 - val_loss: 23.3338 - val_mse: 1502.0133\n",
      "Epoch 293/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7603 - mse: 1354.1239 - val_loss: 23.3539 - val_mse: 1488.3047\n",
      "Epoch 294/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5570 - mse: 1277.3716 - val_loss: 23.3559 - val_mse: 1489.6531\n",
      "Epoch 295/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8366 - mse: 1360.8931 - val_loss: 23.3160 - val_mse: 1499.0267\n",
      "Epoch 296/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.0769 - mse: 1372.0188 - val_loss: 23.3839 - val_mse: 1473.7552\n",
      "Epoch 297/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4343 - mse: 1278.9861 - val_loss: 23.4072 - val_mse: 1464.5286\n",
      "Epoch 298/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.9636 - mse: 1346.7444 - val_loss: 23.3446 - val_mse: 1485.2812\n",
      "Epoch 299/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5981 - mse: 1302.4910 - val_loss: 23.4749 - val_mse: 1446.8383\n",
      "Epoch 300/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6385 - mse: 1320.3699 - val_loss: 23.3207 - val_mse: 1496.7363\n",
      "Epoch 301/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.0318 - mse: 1368.4413 - val_loss: 23.6257 - val_mse: 1585.9430\n",
      "Epoch 302/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6715 - mse: 1316.0398 - val_loss: 23.3418 - val_mse: 1480.3422\n",
      "Epoch 303/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4081 - mse: 1256.1637 - val_loss: 23.3628 - val_mse: 1482.4062\n",
      "Epoch 304/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8987 - mse: 1360.9160 - val_loss: 23.3961 - val_mse: 1472.8992\n",
      "Epoch 305/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6205 - mse: 1323.5039 - val_loss: 23.3879 - val_mse: 1470.5361\n",
      "Epoch 306/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.7666 - mse: 1319.9248 - val_loss: 23.3460 - val_mse: 1505.4733\n",
      "Epoch 307/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5799 - mse: 1320.8115 - val_loss: 23.3221 - val_mse: 1495.3759\n",
      "Epoch 308/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5594 - mse: 1275.8820 - val_loss: 23.3373 - val_mse: 1497.5768\n",
      "Epoch 309/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 22.0946 - mse: 1378.3206 - val_loss: 23.3545 - val_mse: 1521.0992\n",
      "Epoch 310/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8132 - mse: 1344.1176 - val_loss: 23.3411 - val_mse: 1504.0352\n",
      "Epoch 311/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.5351 - mse: 1270.6981 - val_loss: 23.3808 - val_mse: 1469.7125\n",
      "Epoch 312/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.9338 - mse: 1354.0623 - val_loss: 23.3418 - val_mse: 1471.3733\n",
      "Epoch 313/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5963 - mse: 1335.1572 - val_loss: 23.3612 - val_mse: 1482.9946\n",
      "Epoch 314/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8045 - mse: 1330.4839 - val_loss: 23.3660 - val_mse: 1477.5326\n",
      "Epoch 315/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5632 - mse: 1300.1066 - val_loss: 23.4672 - val_mse: 1445.3711\n",
      "Epoch 316/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7179 - mse: 1323.8608 - val_loss: 23.3555 - val_mse: 1519.1523\n",
      "Epoch 317/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6389 - mse: 1318.5217 - val_loss: 23.3597 - val_mse: 1502.9923\n",
      "Epoch 318/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6676 - mse: 1317.9768 - val_loss: 23.3942 - val_mse: 1467.0303\n",
      "Epoch 319/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8752 - mse: 1349.9292 - val_loss: 23.4384 - val_mse: 1453.9626\n",
      "Epoch 320/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5621 - mse: 1301.6187 - val_loss: 23.3988 - val_mse: 1460.9557\n",
      "Epoch 321/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6602 - mse: 1316.7061 - val_loss: 23.3951 - val_mse: 1516.0990\n",
      "Epoch 322/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.5745 - mse: 1294.4131 - val_loss: 23.3708 - val_mse: 1485.1113\n",
      "Epoch 323/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.9362 - mse: 1347.0354 - val_loss: 23.3513 - val_mse: 1483.9852\n",
      "Epoch 324/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4896 - mse: 1287.4382 - val_loss: 23.3269 - val_mse: 1503.3474\n",
      "Epoch 325/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4847 - mse: 1321.5190 - val_loss: 23.3431 - val_mse: 1488.3859\n",
      "Epoch 326/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7158 - mse: 1307.9872 - val_loss: 23.3790 - val_mse: 1478.1946\n",
      "Epoch 327/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6748 - mse: 1328.1682 - val_loss: 23.3658 - val_mse: 1463.9708\n",
      "Epoch 328/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5410 - mse: 1292.8484 - val_loss: 23.3706 - val_mse: 1468.2156\n",
      "Epoch 329/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8145 - mse: 1351.7196 - val_loss: 23.4505 - val_mse: 1451.0409\n",
      "Epoch 330/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7409 - mse: 1313.7841 - val_loss: 23.3467 - val_mse: 1476.3877\n",
      "Epoch 331/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5317 - mse: 1303.8258 - val_loss: 23.3446 - val_mse: 1476.7148\n",
      "Epoch 332/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5331 - mse: 1304.2682 - val_loss: 23.3501 - val_mse: 1490.7360\n",
      "Epoch 333/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7287 - mse: 1327.0508 - val_loss: 23.3482 - val_mse: 1494.5023\n",
      "Epoch 334/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5728 - mse: 1305.1628 - val_loss: 23.3512 - val_mse: 1487.1342\n",
      "Epoch 335/500\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 21.6245 - mse: 1310.6433 - val_loss: 23.3567 - val_mse: 1486.2162\n",
      "Epoch 336/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6315 - mse: 1312.3021 - val_loss: 23.4079 - val_mse: 1460.2013\n",
      "Epoch 337/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7286 - mse: 1324.1635 - val_loss: 23.3979 - val_mse: 1460.3762\n",
      "Epoch 338/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5564 - mse: 1301.8501 - val_loss: 23.3468 - val_mse: 1485.0286\n",
      "Epoch 339/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4567 - mse: 1287.9458 - val_loss: 23.3800 - val_mse: 1474.4509\n",
      "Epoch 340/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5825 - mse: 1314.3022 - val_loss: 23.3690 - val_mse: 1479.5488\n",
      "Epoch 341/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7561 - mse: 1331.4150 - val_loss: 23.3461 - val_mse: 1507.9270\n",
      "Epoch 342/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4754 - mse: 1294.2676 - val_loss: 23.3672 - val_mse: 1474.4260\n",
      "Epoch 343/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7907 - mse: 1328.5087 - val_loss: 23.3832 - val_mse: 1476.8942\n",
      "Epoch 344/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5459 - mse: 1308.8806 - val_loss: 23.3699 - val_mse: 1479.7281\n",
      "Epoch 345/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4182 - mse: 1279.4722 - val_loss: 23.3910 - val_mse: 1519.0581\n",
      "Epoch 346/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6049 - mse: 1313.6786 - val_loss: 23.4716 - val_mse: 1448.9557\n",
      "Epoch 347/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6346 - mse: 1303.9343 - val_loss: 23.4923 - val_mse: 1563.4691\n",
      "Epoch 348/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6606 - mse: 1319.3267 - val_loss: 23.4867 - val_mse: 1449.8860\n",
      "Epoch 349/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4878 - mse: 1293.8201 - val_loss: 23.3838 - val_mse: 1474.7935\n",
      "Epoch 350/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7362 - mse: 1334.1125 - val_loss: 23.3667 - val_mse: 1516.1263\n",
      "Epoch 351/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5213 - mse: 1300.4973 - val_loss: 23.4247 - val_mse: 1458.0089\n",
      "Epoch 352/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5757 - mse: 1315.3824 - val_loss: 23.4513 - val_mse: 1455.5673\n",
      "Epoch 353/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6992 - mse: 1317.4231 - val_loss: 23.4234 - val_mse: 1461.4385\n",
      "Epoch 354/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3579 - mse: 1283.0504 - val_loss: 23.3743 - val_mse: 1471.0812\n",
      "Epoch 355/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6775 - mse: 1314.9896 - val_loss: 23.4948 - val_mse: 1443.7911\n",
      "Epoch 356/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6167 - mse: 1312.6703 - val_loss: 23.4106 - val_mse: 1505.7209\n",
      "Epoch 357/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2112 - mse: 1254.5765 - val_loss: 23.4806 - val_mse: 1442.3577\n",
      "Epoch 358/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7547 - mse: 1327.3517 - val_loss: 23.5698 - val_mse: 1436.3992\n",
      "Epoch 359/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.5148 - mse: 1315.5431 - val_loss: 23.4014 - val_mse: 1468.5327\n",
      "Epoch 360/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2731 - mse: 1260.7498 - val_loss: 23.3764 - val_mse: 1502.8877\n",
      "Epoch 361/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8817 - mse: 1363.1390 - val_loss: 23.4782 - val_mse: 1444.5452\n",
      "Epoch 362/500\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 21.3869 - mse: 1229.5436 - val_loss: 23.4090 - val_mse: 1464.8862\n",
      "Epoch 363/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4558 - mse: 1311.6985 - val_loss: 23.3829 - val_mse: 1484.7578\n",
      "Epoch 364/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7069 - mse: 1356.7126 - val_loss: 23.3955 - val_mse: 1469.4385\n",
      "Epoch 365/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6440 - mse: 1305.0521 - val_loss: 23.4591 - val_mse: 1452.4003\n",
      "Epoch 366/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5445 - mse: 1307.4344 - val_loss: 23.3784 - val_mse: 1482.3674\n",
      "Epoch 367/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2843 - mse: 1261.6451 - val_loss: 23.3931 - val_mse: 1479.4374\n",
      "Epoch 368/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6606 - mse: 1314.0209 - val_loss: 23.4143 - val_mse: 1473.0112\n",
      "Epoch 369/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4776 - mse: 1309.1578 - val_loss: 23.3963 - val_mse: 1482.1770\n",
      "Epoch 370/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3790 - mse: 1282.7169 - val_loss: 23.3995 - val_mse: 1510.3105\n",
      "Epoch 371/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8110 - mse: 1343.7307 - val_loss: 23.4927 - val_mse: 1446.4976\n",
      "Epoch 372/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.1453 - mse: 1239.9185 - val_loss: 23.4103 - val_mse: 1469.3956\n",
      "Epoch 373/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6970 - mse: 1334.5190 - val_loss: 23.5058 - val_mse: 1453.5752\n",
      "Epoch 374/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4137 - mse: 1281.3320 - val_loss: 23.3948 - val_mse: 1486.4126\n",
      "Epoch 375/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.5201 - mse: 1309.4647 - val_loss: 23.5547 - val_mse: 1441.3933\n",
      "Epoch 376/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5506 - mse: 1294.4197 - val_loss: 23.3988 - val_mse: 1503.5483\n",
      "Epoch 377/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5255 - mse: 1293.6577 - val_loss: 23.4451 - val_mse: 1509.3077\n",
      "Epoch 378/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3479 - mse: 1298.3101 - val_loss: 23.3969 - val_mse: 1486.8718\n",
      "Epoch 379/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.3259 - mse: 1278.4401 - val_loss: 23.4087 - val_mse: 1484.8529\n",
      "Epoch 380/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7388 - mse: 1307.9481 - val_loss: 23.4820 - val_mse: 1449.0797\n",
      "Epoch 381/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2963 - mse: 1286.3895 - val_loss: 23.4555 - val_mse: 1547.5367\n",
      "Epoch 382/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.7173 - mse: 1349.3087 - val_loss: 23.3978 - val_mse: 1477.7039\n",
      "Epoch 383/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4074 - mse: 1268.1614 - val_loss: 23.5086 - val_mse: 1453.7322\n",
      "Epoch 384/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7281 - mse: 1335.2902 - val_loss: 23.4865 - val_mse: 1457.1095\n",
      "Epoch 385/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.0972 - mse: 1254.1672 - val_loss: 23.3864 - val_mse: 1489.9866\n",
      "Epoch 386/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5478 - mse: 1294.3103 - val_loss: 23.3973 - val_mse: 1497.2626\n",
      "Epoch 387/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3299 - mse: 1280.1693 - val_loss: 23.3818 - val_mse: 1499.2207\n",
      "Epoch 388/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8791 - mse: 1344.4065 - val_loss: 23.4179 - val_mse: 1486.1238\n",
      "Epoch 389/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.0130 - mse: 1226.9375 - val_loss: 23.3973 - val_mse: 1484.4680\n",
      "Epoch 390/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6957 - mse: 1358.2616 - val_loss: 23.3746 - val_mse: 1487.4202\n",
      "Epoch 391/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4264 - mse: 1302.9808 - val_loss: 23.4202 - val_mse: 1478.1151\n",
      "Epoch 392/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.4820 - mse: 1298.6134 - val_loss: 23.4564 - val_mse: 1459.0089\n",
      "Epoch 393/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2918 - mse: 1245.0980 - val_loss: 23.5426 - val_mse: 1443.5590\n",
      "Epoch 394/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5690 - mse: 1301.3928 - val_loss: 23.4469 - val_mse: 1462.2502\n",
      "Epoch 395/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2132 - mse: 1300.7216 - val_loss: 23.4799 - val_mse: 1457.1071\n",
      "Epoch 396/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5395 - mse: 1300.5117 - val_loss: 23.5149 - val_mse: 1449.6079\n",
      "Epoch 397/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7022 - mse: 1316.3470 - val_loss: 23.6005 - val_mse: 1444.2303\n",
      "Epoch 398/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.1439 - mse: 1273.7062 - val_loss: 23.4035 - val_mse: 1480.8521\n",
      "Epoch 399/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2726 - mse: 1249.4751 - val_loss: 23.5519 - val_mse: 1440.7162\n",
      "Epoch 400/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6667 - mse: 1350.2687 - val_loss: 23.4281 - val_mse: 1477.5115\n",
      "Epoch 401/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2227 - mse: 1244.8892 - val_loss: 23.4945 - val_mse: 1459.4612\n",
      "Epoch 402/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3020 - mse: 1272.7461 - val_loss: 23.3957 - val_mse: 1480.0077\n",
      "Epoch 403/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3462 - mse: 1294.3367 - val_loss: 23.4786 - val_mse: 1460.8231\n",
      "Epoch 404/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.7907 - mse: 1333.4271 - val_loss: 23.4691 - val_mse: 1459.7220\n",
      "Epoch 405/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2341 - mse: 1262.2413 - val_loss: 23.4087 - val_mse: 1476.8120\n",
      "Epoch 406/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5746 - mse: 1328.9027 - val_loss: 23.7668 - val_mse: 1422.8921\n",
      "Epoch 407/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3765 - mse: 1272.1797 - val_loss: 23.5142 - val_mse: 1449.6531\n",
      "Epoch 408/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5713 - mse: 1332.3889 - val_loss: 23.5457 - val_mse: 1443.7690\n",
      "Epoch 409/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2306 - mse: 1269.6884 - val_loss: 23.4585 - val_mse: 1462.0911\n",
      "Epoch 410/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.8339 - mse: 1318.9089 - val_loss: 23.4712 - val_mse: 1454.0784\n",
      "Epoch 411/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.1111 - mse: 1253.0618 - val_loss: 23.5139 - val_mse: 1451.0747\n",
      "Epoch 412/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3513 - mse: 1291.0216 - val_loss: 23.4769 - val_mse: 1476.1208\n",
      "Epoch 413/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4697 - mse: 1309.3878 - val_loss: 23.4540 - val_mse: 1472.7889\n",
      "Epoch 414/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4950 - mse: 1285.1071 - val_loss: 23.4395 - val_mse: 1518.7971\n",
      "Epoch 415/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3194 - mse: 1274.1769 - val_loss: 23.4258 - val_mse: 1476.2437\n",
      "Epoch 416/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3820 - mse: 1301.2792 - val_loss: 23.5145 - val_mse: 1447.5452\n",
      "Epoch 417/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.3644 - mse: 1294.2242 - val_loss: 23.4508 - val_mse: 1497.7532\n",
      "Epoch 418/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.3218 - mse: 1256.9514 - val_loss: 23.4340 - val_mse: 1471.0747\n",
      "Epoch 419/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4010 - mse: 1297.9344 - val_loss: 23.6811 - val_mse: 1430.1846\n",
      "Epoch 420/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.2835 - mse: 1270.3110 - val_loss: 23.4017 - val_mse: 1479.3577\n",
      "Epoch 421/500\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 21.2637 - mse: 1276.6208 - val_loss: 23.4860 - val_mse: 1456.5098\n",
      "Epoch 422/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.5684 - mse: 1311.7767 - val_loss: 23.5056 - val_mse: 1465.1384\n",
      "Epoch 423/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.2571 - mse: 1265.6699 - val_loss: 23.4568 - val_mse: 1463.7354\n",
      "Epoch 424/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.3443 - mse: 1286.0303 - val_loss: 23.5397 - val_mse: 1442.7988\n",
      "Epoch 425/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.3980 - mse: 1275.4868 - val_loss: 23.5696 - val_mse: 1439.7588\n",
      "Epoch 426/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.3920 - mse: 1307.6616 - val_loss: 23.5105 - val_mse: 1451.6482\n",
      "Epoch 427/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4836 - mse: 1303.6569 - val_loss: 23.4183 - val_mse: 1479.4674\n",
      "Epoch 428/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4068 - mse: 1271.3682 - val_loss: 23.4304 - val_mse: 1491.8997\n",
      "Epoch 429/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.2533 - mse: 1289.7211 - val_loss: 23.4346 - val_mse: 1508.0211\n",
      "Epoch 430/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.6799 - mse: 1314.8973 - val_loss: 23.4656 - val_mse: 1470.8314\n",
      "Epoch 431/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.1520 - mse: 1253.2111 - val_loss: 23.5704 - val_mse: 1438.5742\n",
      "Epoch 432/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.2778 - mse: 1270.4796 - val_loss: 23.5139 - val_mse: 1444.5477\n",
      "Epoch 433/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3425 - mse: 1310.4536 - val_loss: 23.4190 - val_mse: 1480.8190\n",
      "Epoch 434/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4914 - mse: 1264.8154 - val_loss: 23.4284 - val_mse: 1484.8796\n",
      "Epoch 435/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2798 - mse: 1277.5349 - val_loss: 23.4845 - val_mse: 1462.4967\n",
      "Epoch 436/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.1461 - mse: 1263.7655 - val_loss: 23.5549 - val_mse: 1442.7446\n",
      "Epoch 437/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4825 - mse: 1304.5825 - val_loss: 23.4550 - val_mse: 1470.8016\n",
      "Epoch 438/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4825 - mse: 1304.3394 - val_loss: 23.4831 - val_mse: 1455.2548\n",
      "Epoch 439/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3056 - mse: 1280.4358 - val_loss: 23.5181 - val_mse: 1451.6295\n",
      "Epoch 440/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3994 - mse: 1282.3000 - val_loss: 23.5072 - val_mse: 1463.9163\n",
      "Epoch 441/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2454 - mse: 1281.1248 - val_loss: 23.4423 - val_mse: 1463.1836\n",
      "Epoch 442/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2556 - mse: 1258.5532 - val_loss: 23.4608 - val_mse: 1465.5898\n",
      "Epoch 443/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3588 - mse: 1288.9944 - val_loss: 23.4365 - val_mse: 1488.8060\n",
      "Epoch 444/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3201 - mse: 1282.7587 - val_loss: 23.4639 - val_mse: 1464.5056\n",
      "Epoch 445/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3082 - mse: 1271.7031 - val_loss: 23.4326 - val_mse: 1507.9540\n",
      "Epoch 446/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3255 - mse: 1286.7422 - val_loss: 23.4640 - val_mse: 1478.4575\n",
      "Epoch 447/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4207 - mse: 1287.6483 - val_loss: 23.4567 - val_mse: 1496.8547\n",
      "Epoch 448/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3138 - mse: 1275.8445 - val_loss: 23.4670 - val_mse: 1460.9946\n",
      "Epoch 449/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3069 - mse: 1283.4198 - val_loss: 23.4430 - val_mse: 1474.9812\n",
      "Epoch 450/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3155 - mse: 1280.9982 - val_loss: 23.4637 - val_mse: 1485.4839\n",
      "Epoch 451/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2717 - mse: 1277.3446 - val_loss: 23.4446 - val_mse: 1470.2144\n",
      "Epoch 452/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3869 - mse: 1295.5688 - val_loss: 23.5061 - val_mse: 1458.3815\n",
      "Epoch 453/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2486 - mse: 1267.6531 - val_loss: 23.5378 - val_mse: 1450.0826\n",
      "Epoch 454/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4349 - mse: 1290.7660 - val_loss: 23.6544 - val_mse: 1432.2109\n",
      "Epoch 455/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.0505 - mse: 1255.4282 - val_loss: 23.4761 - val_mse: 1480.4861\n",
      "Epoch 456/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4665 - mse: 1292.1183 - val_loss: 23.4669 - val_mse: 1462.6427\n",
      "Epoch 457/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3998 - mse: 1307.0447 - val_loss: 23.5410 - val_mse: 1454.3776\n",
      "Epoch 458/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2346 - mse: 1261.9594 - val_loss: 23.4558 - val_mse: 1474.9691\n",
      "Epoch 459/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.1781 - mse: 1259.9139 - val_loss: 23.4628 - val_mse: 1463.5316\n",
      "Epoch 460/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.1807 - mse: 1280.5427 - val_loss: 23.4940 - val_mse: 1471.6664\n",
      "Epoch 461/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.4094 - mse: 1276.4556 - val_loss: 23.4947 - val_mse: 1459.3457\n",
      "Epoch 462/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.1141 - mse: 1253.3304 - val_loss: 23.5148 - val_mse: 1466.3500\n",
      "Epoch 463/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5012 - mse: 1306.7340 - val_loss: 23.4521 - val_mse: 1516.9700\n",
      "Epoch 464/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.1427 - mse: 1261.1531 - val_loss: 23.4781 - val_mse: 1480.0393\n",
      "Epoch 465/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.6831 - mse: 1319.1559 - val_loss: 23.4967 - val_mse: 1453.3191\n",
      "Epoch 466/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.1185 - mse: 1267.6559 - val_loss: 23.4625 - val_mse: 1466.1888\n",
      "Epoch 467/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2461 - mse: 1263.4515 - val_loss: 23.5918 - val_mse: 1440.3621\n",
      "Epoch 468/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.0462 - mse: 1239.5807 - val_loss: 23.4692 - val_mse: 1507.3979\n",
      "Epoch 469/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3307 - mse: 1322.0485 - val_loss: 23.5058 - val_mse: 1463.3735\n",
      "Epoch 470/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.3896 - mse: 1270.8781 - val_loss: 23.5294 - val_mse: 1462.6755\n",
      "Epoch 471/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.1644 - mse: 1260.6810 - val_loss: 23.7051 - val_mse: 1427.8126\n",
      "Epoch 472/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4941 - mse: 1306.1669 - val_loss: 23.5096 - val_mse: 1462.2201\n",
      "Epoch 473/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.1819 - mse: 1265.4136 - val_loss: 23.5647 - val_mse: 1441.4120\n",
      "Epoch 474/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.2422 - mse: 1270.5433 - val_loss: 23.5874 - val_mse: 1442.0559\n",
      "Epoch 475/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.3935 - mse: 1296.6699 - val_loss: 23.4898 - val_mse: 1469.0635\n",
      "Epoch 476/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.1038 - mse: 1245.3715 - val_loss: 23.4942 - val_mse: 1466.8022\n",
      "Epoch 477/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4951 - mse: 1310.2294 - val_loss: 23.4409 - val_mse: 1492.3569\n",
      "Epoch 478/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 20.9371 - mse: 1222.5459 - val_loss: 23.4642 - val_mse: 1473.6923\n",
      "Epoch 479/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4140 - mse: 1331.8971 - val_loss: 23.5201 - val_mse: 1453.6554\n",
      "Epoch 480/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.1669 - mse: 1252.8019 - val_loss: 23.4712 - val_mse: 1490.8456\n",
      "Epoch 481/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.2835 - mse: 1266.7214 - val_loss: 23.4840 - val_mse: 1465.4948\n",
      "Epoch 482/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.1646 - mse: 1265.1355 - val_loss: 23.4366 - val_mse: 1480.8835\n",
      "Epoch 483/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.1661 - mse: 1278.9791 - val_loss: 23.5394 - val_mse: 1521.6981\n",
      "Epoch 484/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4437 - mse: 1277.9020 - val_loss: 23.4838 - val_mse: 1466.6735\n",
      "Epoch 485/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 20.9154 - mse: 1216.8579 - val_loss: 23.4888 - val_mse: 1476.8477\n",
      "Epoch 486/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5639 - mse: 1339.1660 - val_loss: 23.5831 - val_mse: 1442.8750\n",
      "Epoch 487/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.0731 - mse: 1248.5560 - val_loss: 23.4243 - val_mse: 1488.5106\n",
      "Epoch 488/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.3855 - mse: 1305.7172 - val_loss: 23.4578 - val_mse: 1473.9110\n",
      "Epoch 489/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.1897 - mse: 1257.2855 - val_loss: 23.4533 - val_mse: 1483.5698\n",
      "Epoch 490/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.1222 - mse: 1270.5483 - val_loss: 23.6203 - val_mse: 1435.0214\n",
      "Epoch 491/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.4224 - mse: 1281.9684 - val_loss: 23.4627 - val_mse: 1468.0596\n",
      "Epoch 492/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.0472 - mse: 1255.9921 - val_loss: 23.5442 - val_mse: 1449.9958\n",
      "Epoch 493/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.3321 - mse: 1290.9457 - val_loss: 23.4882 - val_mse: 1461.2893\n",
      "Epoch 494/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.1259 - mse: 1262.5717 - val_loss: 23.5487 - val_mse: 1451.7886\n",
      "Epoch 495/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 21.0796 - mse: 1257.4867 - val_loss: 23.4514 - val_mse: 1488.1395\n",
      "Epoch 496/500\n",
      " 64/113 [===============>..............] - ETA: 0s - loss: 21.5078 - mse: 1279.3014WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 56500 batches). You may need to use the repeat() function when building your dataset.\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 21.5049 - mse: 1282.7234 - val_loss: 23.4401 - val_mse: 1501.6382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 20:32:29.214782: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_wind_spd0\n",
      "[36.72, 38.16]\n",
      "Epoch 1/500\n",
      "112/113 [============================>.] - ETA: 0s - loss: 27.6197 - mse: 1978.9338"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 20:32:30.029815: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 0s 3ms/step - loss: 27.6956 - mse: 1990.2714 - val_loss: 26.2818 - val_mse: 1920.4232\n",
      "Epoch 2/500\n",
      " 78/113 [===================>..........] - ETA: 0s - loss: 24.9577 - mse: 1709.9390"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 20:32:30.254220: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2068 - mse: 1748.1859 - val_loss: 26.3402 - val_mse: 1884.0491\n",
      "Epoch 3/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.2972 - mse: 1772.7081 - val_loss: 26.3167 - val_mse: 1892.7053\n",
      "Epoch 4/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.5644 - mse: 1801.4393 - val_loss: 26.3851 - val_mse: 1871.2325\n",
      "Epoch 5/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3663 - mse: 1771.8755 - val_loss: 26.2863 - val_mse: 1911.9117\n",
      "Epoch 6/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2351 - mse: 1745.2167 - val_loss: 26.2878 - val_mse: 1938.4310\n",
      "Epoch 7/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3231 - mse: 1772.0320 - val_loss: 26.2831 - val_mse: 1932.7946\n",
      "Epoch 8/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3036 - mse: 1756.6021 - val_loss: 26.3682 - val_mse: 1875.3279\n",
      "Epoch 9/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3681 - mse: 1780.5936 - val_loss: 26.2867 - val_mse: 1937.5002\n",
      "Epoch 10/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.5406 - mse: 1807.5035 - val_loss: 26.3819 - val_mse: 1871.8401\n",
      "Epoch 11/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2202 - mse: 1742.8182 - val_loss: 26.5209 - val_mse: 2023.1857\n",
      "Epoch 12/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2799 - mse: 1749.7056 - val_loss: 26.2855 - val_mse: 1912.3438\n",
      "Epoch 13/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.5720 - mse: 1823.5874 - val_loss: 26.3075 - val_mse: 1954.7522\n",
      "Epoch 14/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2415 - mse: 1755.8193 - val_loss: 26.2903 - val_mse: 1941.3530\n",
      "Epoch 15/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3388 - mse: 1772.2778 - val_loss: 26.3043 - val_mse: 1952.6946\n",
      "Epoch 16/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3546 - mse: 1768.6748 - val_loss: 26.2812 - val_mse: 1919.2598\n",
      "Epoch 17/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4746 - mse: 1777.5439 - val_loss: 26.2846 - val_mse: 1913.0149\n",
      "Epoch 18/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.9157 - mse: 1707.0983 - val_loss: 26.3219 - val_mse: 1962.8948\n",
      "Epoch 19/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.5728 - mse: 1806.5830 - val_loss: 26.3001 - val_mse: 1899.7495\n",
      "Epoch 20/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.5218 - mse: 1808.7719 - val_loss: 26.2915 - val_mse: 1905.5343\n",
      "Epoch 21/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.0598 - mse: 1717.6765 - val_loss: 26.3839 - val_mse: 1871.1284\n",
      "Epoch 22/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.7825 - mse: 1845.9751 - val_loss: 26.2835 - val_mse: 1914.1382\n",
      "Epoch 23/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.0714 - mse: 1723.3092 - val_loss: 26.3076 - val_mse: 1955.1206\n",
      "Epoch 24/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.6232 - mse: 1818.0759 - val_loss: 26.3687 - val_mse: 1874.8484\n",
      "Epoch 25/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2948 - mse: 1748.4110 - val_loss: 26.2906 - val_mse: 1906.0796\n",
      "Epoch 26/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4402 - mse: 1779.8420 - val_loss: 26.4235 - val_mse: 1997.6021\n",
      "Epoch 27/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2876 - mse: 1760.8531 - val_loss: 26.3001 - val_mse: 1899.4563\n",
      "Epoch 28/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2568 - mse: 1763.6298 - val_loss: 26.4071 - val_mse: 1992.9780\n",
      "Epoch 29/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1629 - mse: 1734.3453 - val_loss: 26.2793 - val_mse: 1923.6298\n",
      "Epoch 30/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.5855 - mse: 1812.4100 - val_loss: 26.3442 - val_mse: 1972.5463\n",
      "Epoch 31/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2191 - mse: 1775.3445 - val_loss: 26.5696 - val_mse: 2034.6157\n",
      "Epoch 32/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2969 - mse: 1738.7166 - val_loss: 26.4382 - val_mse: 1859.8118\n",
      "Epoch 33/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3414 - mse: 1759.7866 - val_loss: 26.4673 - val_mse: 1854.9319\n",
      "Epoch 34/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.7152 - mse: 1861.1792 - val_loss: 26.2791 - val_mse: 1926.6305\n",
      "Epoch 35/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1560 - mse: 1716.1970 - val_loss: 26.3728 - val_mse: 1873.5829\n",
      "Epoch 36/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1982 - mse: 1745.3102 - val_loss: 26.2943 - val_mse: 1902.6029\n",
      "Epoch 37/500\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 25.1913 - mse: 1774.9777 - val_loss: 26.5840 - val_mse: 2037.7679\n",
      "Epoch 38/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.6512 - mse: 1809.3029 - val_loss: 26.2799 - val_mse: 1929.6433\n",
      "Epoch 39/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3887 - mse: 1763.3427 - val_loss: 26.3085 - val_mse: 1956.1431\n",
      "Epoch 40/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4944 - mse: 1811.7656 - val_loss: 26.5188 - val_mse: 1847.5723\n",
      "Epoch 41/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.0057 - mse: 1702.1676 - val_loss: 26.3554 - val_mse: 1976.7175\n",
      "Epoch 42/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2369 - mse: 1744.0470 - val_loss: 26.4060 - val_mse: 1992.8040\n",
      "Epoch 43/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.5096 - mse: 1807.1982 - val_loss: 26.3898 - val_mse: 1869.4149\n",
      "Epoch 44/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.5114 - mse: 1787.7982 - val_loss: 26.2792 - val_mse: 1921.0573\n",
      "Epoch 45/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.0952 - mse: 1741.0781 - val_loss: 26.2797 - val_mse: 1929.8475\n",
      "Epoch 46/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.7472 - mse: 1818.2599 - val_loss: 26.3204 - val_mse: 1962.6749\n",
      "Epoch 47/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.7426 - mse: 1689.9824 - val_loss: 26.2794 - val_mse: 1929.1757\n",
      "Epoch 48/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.7746 - mse: 1838.9700 - val_loss: 26.3233 - val_mse: 1889.0874\n",
      "Epoch 49/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3012 - mse: 1751.2511 - val_loss: 26.5335 - val_mse: 2026.5107\n",
      "Epoch 50/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4922 - mse: 1816.6178 - val_loss: 26.2910 - val_mse: 1904.8641\n",
      "Epoch 51/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2280 - mse: 1742.3363 - val_loss: 26.2887 - val_mse: 1941.2084\n",
      "Epoch 52/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.0706 - mse: 1738.3558 - val_loss: 26.2922 - val_mse: 1903.8704\n",
      "Epoch 53/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.7677 - mse: 1817.4204 - val_loss: 26.3495 - val_mse: 1974.6823\n",
      "Epoch 54/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2228 - mse: 1763.8861 - val_loss: 26.2790 - val_mse: 1928.1627\n",
      "Epoch 55/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2040 - mse: 1753.6179 - val_loss: 26.2878 - val_mse: 1940.3407\n",
      "Epoch 56/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.5412 - mse: 1810.8123 - val_loss: 26.3201 - val_mse: 1890.2998\n",
      "Epoch 57/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1889 - mse: 1728.9120 - val_loss: 26.3122 - val_mse: 1893.4142\n",
      "Epoch 58/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.5289 - mse: 1810.9518 - val_loss: 26.2855 - val_mse: 1910.0814\n",
      "Epoch 59/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.9876 - mse: 1683.0891 - val_loss: 26.2791 - val_mse: 1920.9645\n",
      "Epoch 60/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3064 - mse: 1774.6615 - val_loss: 26.3815 - val_mse: 1985.3875\n",
      "Epoch 61/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.8550 - mse: 1878.3280 - val_loss: 26.3683 - val_mse: 1981.0951\n",
      "Epoch 62/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1621 - mse: 1763.7644 - val_loss: 26.2854 - val_mse: 1910.2676\n",
      "Epoch 63/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.0148 - mse: 1719.3043 - val_loss: 26.4175 - val_mse: 1996.0569\n",
      "Epoch 64/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.5122 - mse: 1793.4594 - val_loss: 26.2977 - val_mse: 1900.3091\n",
      "Epoch 65/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.0209 - mse: 1702.4385 - val_loss: 26.2861 - val_mse: 1909.4784\n",
      "Epoch 66/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3044 - mse: 1758.0504 - val_loss: 26.3223 - val_mse: 1889.4967\n",
      "Epoch 67/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.6504 - mse: 1866.5149 - val_loss: 26.2890 - val_mse: 1906.6147\n",
      "Epoch 68/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1533 - mse: 1721.6969 - val_loss: 26.2981 - val_mse: 1900.1058\n",
      "Epoch 69/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.6482 - mse: 1819.5745 - val_loss: 26.3755 - val_mse: 1983.4823\n",
      "Epoch 70/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.9658 - mse: 1689.4851 - val_loss: 26.2799 - val_mse: 1930.1818\n",
      "Epoch 71/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.8073 - mse: 1862.0785 - val_loss: 26.3290 - val_mse: 1886.9951\n",
      "Epoch 72/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.0693 - mse: 1744.5104 - val_loss: 26.3640 - val_mse: 1875.7666\n",
      "Epoch 73/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.9015 - mse: 1680.9269 - val_loss: 26.5074 - val_mse: 2020.1008\n",
      "Epoch 74/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.7193 - mse: 1860.0159 - val_loss: 26.2820 - val_mse: 1934.0071\n",
      "Epoch 75/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.0915 - mse: 1724.5811 - val_loss: 26.2929 - val_mse: 1944.9517\n",
      "Epoch 76/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3265 - mse: 1772.6271 - val_loss: 26.2787 - val_mse: 1927.0679\n",
      "Epoch 77/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.7561 - mse: 1838.9803 - val_loss: 26.3052 - val_mse: 1954.1869\n",
      "Epoch 78/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2105 - mse: 1737.3207 - val_loss: 26.4308 - val_mse: 1999.8373\n",
      "Epoch 79/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.0208 - mse: 1742.9135 - val_loss: 26.2794 - val_mse: 1919.9209\n",
      "Epoch 80/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.5513 - mse: 1791.9894 - val_loss: 26.3393 - val_mse: 1883.2833\n",
      "Epoch 81/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.0200 - mse: 1738.6348 - val_loss: 26.4278 - val_mse: 1861.5463\n",
      "Epoch 82/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4189 - mse: 1775.6351 - val_loss: 26.2827 - val_mse: 1913.4023\n",
      "Epoch 83/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3989 - mse: 1767.2894 - val_loss: 26.2786 - val_mse: 1922.5968\n",
      "Epoch 84/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2781 - mse: 1784.1650 - val_loss: 26.2786 - val_mse: 1922.6194\n",
      "Epoch 85/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.9222 - mse: 1709.1971 - val_loss: 26.4128 - val_mse: 1864.4315\n",
      "Epoch 86/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.7340 - mse: 1834.6140 - val_loss: 26.3349 - val_mse: 1969.1710\n",
      "Epoch 87/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2361 - mse: 1790.9995 - val_loss: 26.2836 - val_mse: 1936.3099\n",
      "Epoch 88/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1622 - mse: 1733.5220 - val_loss: 26.2907 - val_mse: 1943.3070\n",
      "Epoch 89/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.5201 - mse: 1798.9602 - val_loss: 26.3282 - val_mse: 1966.3530\n",
      "Epoch 90/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2422 - mse: 1745.1256 - val_loss: 26.3143 - val_mse: 1892.3486\n",
      "Epoch 91/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3627 - mse: 1793.3584 - val_loss: 26.2822 - val_mse: 1913.6742\n",
      "Epoch 92/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4084 - mse: 1779.2947 - val_loss: 26.2785 - val_mse: 1927.9634\n",
      "Epoch 93/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2335 - mse: 1782.9194 - val_loss: 26.3099 - val_mse: 1957.3580\n",
      "Epoch 94/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2005 - mse: 1729.4159 - val_loss: 26.2904 - val_mse: 1943.2114\n",
      "Epoch 95/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3347 - mse: 1760.3903 - val_loss: 26.2946 - val_mse: 1946.7084\n",
      "Epoch 96/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1698 - mse: 1752.5066 - val_loss: 26.3165 - val_mse: 1891.4207\n",
      "Epoch 97/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4522 - mse: 1786.8478 - val_loss: 26.2845 - val_mse: 1910.3730\n",
      "Epoch 98/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2930 - mse: 1780.8591 - val_loss: 26.3870 - val_mse: 1987.2666\n",
      "Epoch 99/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4090 - mse: 1783.3657 - val_loss: 26.3762 - val_mse: 1872.3839\n",
      "Epoch 100/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4981 - mse: 1784.1295 - val_loss: 26.3453 - val_mse: 1973.3282\n",
      "Epoch 101/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2487 - mse: 1733.2416 - val_loss: 26.4106 - val_mse: 1994.3230\n",
      "Epoch 102/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1537 - mse: 1780.2065 - val_loss: 26.3577 - val_mse: 1877.2283\n",
      "Epoch 103/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3154 - mse: 1761.2471 - val_loss: 26.2929 - val_mse: 1902.6819\n",
      "Epoch 104/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.4311 - mse: 1771.0447 - val_loss: 26.2793 - val_mse: 1930.7173\n",
      "Epoch 105/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4381 - mse: 1776.3744 - val_loss: 26.2992 - val_mse: 1950.4888\n",
      "Epoch 106/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.0906 - mse: 1762.2932 - val_loss: 26.3118 - val_mse: 1893.0861\n",
      "Epoch 107/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.5748 - mse: 1792.5975 - val_loss: 26.3470 - val_mse: 1974.0287\n",
      "Epoch 108/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1792 - mse: 1755.8448 - val_loss: 26.2875 - val_mse: 1906.9963\n",
      "Epoch 109/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2993 - mse: 1764.3082 - val_loss: 26.3528 - val_mse: 1976.1139\n",
      "Epoch 110/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3367 - mse: 1776.5006 - val_loss: 26.2828 - val_mse: 1911.9529\n",
      "Epoch 111/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3601 - mse: 1776.8151 - val_loss: 26.2775 - val_mse: 1925.7661\n",
      "Epoch 112/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2545 - mse: 1763.6241 - val_loss: 26.3113 - val_mse: 1958.4464\n",
      "Epoch 113/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2776 - mse: 1761.9448 - val_loss: 26.2926 - val_mse: 1945.5360\n",
      "Epoch 114/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2963 - mse: 1771.8947 - val_loss: 26.2789 - val_mse: 1930.5862\n",
      "Epoch 115/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3534 - mse: 1769.5836 - val_loss: 26.2958 - val_mse: 1948.0930\n",
      "Epoch 116/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2852 - mse: 1760.7181 - val_loss: 26.2774 - val_mse: 1924.0151\n",
      "Epoch 117/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2547 - mse: 1767.0070 - val_loss: 26.3523 - val_mse: 1878.6622\n",
      "Epoch 118/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4751 - mse: 1785.2598 - val_loss: 26.2939 - val_mse: 1901.6331\n",
      "Epoch 119/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2467 - mse: 1764.2772 - val_loss: 26.3295 - val_mse: 1967.2336\n",
      "Epoch 120/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1566 - mse: 1748.8486 - val_loss: 26.3580 - val_mse: 1978.0541\n",
      "Epoch 121/500\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 25.3850 - mse: 1753.5396 - val_loss: 26.2833 - val_mse: 1937.0681\n",
      "Epoch 122/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.2825 - mse: 1799.9807 - val_loss: 26.2809 - val_mse: 1934.3441\n",
      "Epoch 123/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.2374 - mse: 1746.4420 - val_loss: 26.3463 - val_mse: 1973.9136\n",
      "Epoch 124/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3603 - mse: 1799.8334 - val_loss: 26.3383 - val_mse: 1970.9113\n",
      "Epoch 125/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4574 - mse: 1767.5559 - val_loss: 26.3714 - val_mse: 1873.2942\n",
      "Epoch 126/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2527 - mse: 1750.8218 - val_loss: 26.3234 - val_mse: 1964.5867\n",
      "Epoch 127/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3092 - mse: 1777.9183 - val_loss: 26.2925 - val_mse: 1902.4088\n",
      "Epoch 128/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.5357 - mse: 1807.0851 - val_loss: 26.3088 - val_mse: 1957.2579\n",
      "Epoch 129/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3302 - mse: 1760.4462 - val_loss: 26.3724 - val_mse: 1872.9989\n",
      "Epoch 130/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.0528 - mse: 1722.7386 - val_loss: 26.3178 - val_mse: 1962.0271\n",
      "Epoch 131/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.5285 - mse: 1808.7751 - val_loss: 26.3790 - val_mse: 1871.3806\n",
      "Epoch 132/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2629 - mse: 1759.3107 - val_loss: 26.2909 - val_mse: 1903.4017\n",
      "Epoch 133/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4364 - mse: 1824.6681 - val_loss: 26.2968 - val_mse: 1899.6251\n",
      "Epoch 134/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3025 - mse: 1735.3594 - val_loss: 26.4840 - val_mse: 2014.4067\n",
      "Epoch 135/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.0482 - mse: 1717.9338 - val_loss: 26.3823 - val_mse: 1870.5568\n",
      "Epoch 136/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.4483 - mse: 1818.6669 - val_loss: 26.2887 - val_mse: 1905.0848\n",
      "Epoch 137/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2669 - mse: 1712.7294 - val_loss: 26.2843 - val_mse: 1909.0326\n",
      "Epoch 138/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2110 - mse: 1805.4895 - val_loss: 26.2949 - val_mse: 1900.5690\n",
      "Epoch 139/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3370 - mse: 1740.2902 - val_loss: 26.3292 - val_mse: 1886.0266\n",
      "Epoch 140/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2505 - mse: 1778.5497 - val_loss: 26.3880 - val_mse: 1987.8794\n",
      "Epoch 141/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2949 - mse: 1767.1526 - val_loss: 26.3847 - val_mse: 1869.9241\n",
      "Epoch 142/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2539 - mse: 1766.1233 - val_loss: 26.2821 - val_mse: 1936.4851\n",
      "Epoch 143/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.5218 - mse: 1786.8766 - val_loss: 26.3956 - val_mse: 1990.1951\n",
      "Epoch 144/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4175 - mse: 1780.0350 - val_loss: 26.3633 - val_mse: 1875.2013\n",
      "Epoch 145/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.0304 - mse: 1737.4532 - val_loss: 26.3683 - val_mse: 1873.8771\n",
      "Epoch 146/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3847 - mse: 1766.5052 - val_loss: 26.2984 - val_mse: 1950.7574\n",
      "Epoch 147/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.7927 - mse: 1843.3490 - val_loss: 26.2765 - val_mse: 1926.4689\n",
      "Epoch 148/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2728 - mse: 1749.8051 - val_loss: 26.3580 - val_mse: 1876.6012\n",
      "Epoch 149/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.9067 - mse: 1725.6071 - val_loss: 26.3148 - val_mse: 1960.7781\n",
      "Epoch 150/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.5491 - mse: 1819.6882 - val_loss: 26.4685 - val_mse: 1854.0858\n",
      "Epoch 151/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3728 - mse: 1738.3035 - val_loss: 26.3413 - val_mse: 1881.6626\n",
      "Epoch 152/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1170 - mse: 1765.8116 - val_loss: 26.2807 - val_mse: 1935.1676\n",
      "Epoch 153/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2283 - mse: 1746.8322 - val_loss: 26.3292 - val_mse: 1967.4370\n",
      "Epoch 154/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1992 - mse: 1737.5803 - val_loss: 26.2946 - val_mse: 1900.4381\n",
      "Epoch 155/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.7877 - mse: 1840.6504 - val_loss: 26.2955 - val_mse: 1899.9319\n",
      "Epoch 156/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.9570 - mse: 1732.0496 - val_loss: 26.3051 - val_mse: 1955.4113\n",
      "Epoch 157/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.6499 - mse: 1812.3965 - val_loss: 26.3019 - val_mse: 1896.5850\n",
      "Epoch 158/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.9533 - mse: 1720.3870 - val_loss: 26.2895 - val_mse: 1903.7743\n",
      "Epoch 159/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3899 - mse: 1781.7976 - val_loss: 26.2844 - val_mse: 1939.3319\n",
      "Epoch 160/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.5864 - mse: 1840.5726 - val_loss: 26.2759 - val_mse: 1924.7767\n",
      "Epoch 161/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4444 - mse: 1742.7361 - val_loss: 26.3522 - val_mse: 1878.1440\n",
      "Epoch 162/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.9572 - mse: 1723.5690 - val_loss: 26.3611 - val_mse: 1875.5742\n",
      "Epoch 163/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2709 - mse: 1773.7072 - val_loss: 26.3501 - val_mse: 1878.7423\n",
      "Epoch 164/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.5476 - mse: 1804.6582 - val_loss: 26.2791 - val_mse: 1933.6648\n",
      "Epoch 165/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3437 - mse: 1767.1234 - val_loss: 26.2763 - val_mse: 1927.9634\n",
      "Epoch 166/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1168 - mse: 1760.2380 - val_loss: 26.2796 - val_mse: 1934.3060\n",
      "Epoch 167/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.0467 - mse: 1732.7489 - val_loss: 26.2771 - val_mse: 1930.0308\n",
      "Epoch 168/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.6036 - mse: 1829.7123 - val_loss: 26.2781 - val_mse: 1932.1250\n",
      "Epoch 169/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3011 - mse: 1744.9946 - val_loss: 26.2816 - val_mse: 1910.8824\n",
      "Epoch 170/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2475 - mse: 1732.9667 - val_loss: 26.3452 - val_mse: 1880.1702\n",
      "Epoch 171/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.6319 - mse: 1809.4371 - val_loss: 26.3193 - val_mse: 1889.1891\n",
      "Epoch 172/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1802 - mse: 1767.6658 - val_loss: 26.3086 - val_mse: 1957.7622\n",
      "Epoch 173/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.0896 - mse: 1741.2018 - val_loss: 26.2789 - val_mse: 1933.6504\n",
      "Epoch 174/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.5537 - mse: 1810.2671 - val_loss: 26.2830 - val_mse: 1909.0966\n",
      "Epoch 175/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.0412 - mse: 1704.1245 - val_loss: 26.2785 - val_mse: 1915.0267\n",
      "Epoch 176/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.6089 - mse: 1840.0851 - val_loss: 26.2775 - val_mse: 1916.8821\n",
      "Epoch 177/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.7784 - mse: 1664.7808 - val_loss: 26.2921 - val_mse: 1946.5665\n",
      "Epoch 178/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.7652 - mse: 1869.1772 - val_loss: 26.5234 - val_mse: 1846.2576\n",
      "Epoch 179/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3061 - mse: 1758.6198 - val_loss: 26.3434 - val_mse: 1973.3436\n",
      "Epoch 180/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1104 - mse: 1712.1306 - val_loss: 26.3210 - val_mse: 1888.4408\n",
      "Epoch 181/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1682 - mse: 1751.6772 - val_loss: 26.3016 - val_mse: 1953.5864\n",
      "Epoch 182/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.7219 - mse: 1813.4523 - val_loss: 26.3118 - val_mse: 1959.6326\n",
      "Epoch 183/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3922 - mse: 1807.6895 - val_loss: 26.3120 - val_mse: 1891.8672\n",
      "Epoch 184/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.0787 - mse: 1739.8579 - val_loss: 26.2759 - val_mse: 1928.5901\n",
      "Epoch 185/500\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 25.4079 - mse: 1768.7997 - val_loss: 26.2800 - val_mse: 1935.5256\n",
      "Epoch 186/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1895 - mse: 1761.2140 - val_loss: 26.2783 - val_mse: 1914.7927\n",
      "Epoch 187/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3459 - mse: 1769.2583 - val_loss: 26.3037 - val_mse: 1955.0580\n",
      "Epoch 188/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2068 - mse: 1745.0441 - val_loss: 26.2982 - val_mse: 1951.4390\n",
      "Epoch 189/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3610 - mse: 1782.9426 - val_loss: 26.4682 - val_mse: 1853.8385\n",
      "Epoch 190/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.9983 - mse: 1727.3094 - val_loss: 26.2752 - val_mse: 1922.1049\n",
      "Epoch 191/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.9353 - mse: 1847.7841 - val_loss: 26.4672 - val_mse: 2010.3462\n",
      "Epoch 192/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3927 - mse: 1764.8986 - val_loss: 26.2920 - val_mse: 1901.1064\n",
      "Epoch 193/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.0844 - mse: 1766.0051 - val_loss: 26.2867 - val_mse: 1942.4106\n",
      "Epoch 194/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.8727 - mse: 1718.2854 - val_loss: 26.2759 - val_mse: 1918.9390\n",
      "Epoch 195/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4601 - mse: 1790.9020 - val_loss: 26.2748 - val_mse: 1924.4966\n",
      "Epoch 196/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.5410 - mse: 1763.8586 - val_loss: 26.2838 - val_mse: 1939.7831\n",
      "Epoch 197/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3033 - mse: 1765.3519 - val_loss: 26.2845 - val_mse: 1940.4711\n",
      "Epoch 198/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3076 - mse: 1779.8374 - val_loss: 26.3006 - val_mse: 1953.2976\n",
      "Epoch 199/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1317 - mse: 1749.5183 - val_loss: 26.3004 - val_mse: 1953.1682\n",
      "Epoch 200/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4503 - mse: 1783.6373 - val_loss: 26.2890 - val_mse: 1902.9596\n",
      "Epoch 201/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3493 - mse: 1772.5609 - val_loss: 26.3110 - val_mse: 1959.5029\n",
      "Epoch 202/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1910 - mse: 1765.4095 - val_loss: 26.2803 - val_mse: 1936.3911\n",
      "Epoch 203/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3956 - mse: 1779.1849 - val_loss: 26.4440 - val_mse: 1857.5956\n",
      "Epoch 204/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4218 - mse: 1783.8732 - val_loss: 26.4479 - val_mse: 1856.9214\n",
      "Epoch 205/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4156 - mse: 1786.9374 - val_loss: 26.2989 - val_mse: 1952.2684\n",
      "Epoch 206/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.9736 - mse: 1706.1677 - val_loss: 26.3296 - val_mse: 1968.1945\n",
      "Epoch 207/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1895 - mse: 1780.7461 - val_loss: 26.2760 - val_mse: 1930.5468\n",
      "Epoch 208/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.5120 - mse: 1800.7045 - val_loss: 26.3484 - val_mse: 1975.3914\n",
      "Epoch 209/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.2354 - mse: 1742.7856 - val_loss: 26.3833 - val_mse: 1986.9089\n",
      "Epoch 210/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.5675 - mse: 1805.5292 - val_loss: 26.2752 - val_mse: 1928.7230\n",
      "Epoch 211/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.3076 - mse: 1772.8596 - val_loss: 26.3981 - val_mse: 1991.4237\n",
      "Epoch 212/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.0056 - mse: 1714.7433 - val_loss: 26.2987 - val_mse: 1952.2778\n",
      "Epoch 213/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.6216 - mse: 1829.0211 - val_loss: 26.3786 - val_mse: 1985.4908\n",
      "Epoch 214/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.0701 - mse: 1711.3458 - val_loss: 26.2816 - val_mse: 1938.1490\n",
      "Epoch 215/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3510 - mse: 1788.0852 - val_loss: 26.2832 - val_mse: 1939.7332\n",
      "Epoch 216/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3209 - mse: 1774.5427 - val_loss: 26.2855 - val_mse: 1905.2549\n",
      "Epoch 217/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.3031 - mse: 1767.7821 - val_loss: 26.3135 - val_mse: 1961.0287\n",
      "Epoch 218/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2944 - mse: 1756.1180 - val_loss: 26.3025 - val_mse: 1954.9115\n",
      "Epoch 219/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3221 - mse: 1783.6780 - val_loss: 26.2846 - val_mse: 1905.9834\n",
      "Epoch 220/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3214 - mse: 1759.5349 - val_loss: 26.3171 - val_mse: 1962.7751\n",
      "Epoch 221/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3800 - mse: 1780.9287 - val_loss: 26.3052 - val_mse: 1893.9507\n",
      "Epoch 222/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.2554 - mse: 1762.2159 - val_loss: 26.3513 - val_mse: 1976.5551\n",
      "Epoch 223/500\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 25.3480 - mse: 1776.2990 - val_loss: 26.3614 - val_mse: 1874.7704\n",
      "Epoch 224/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.2439 - mse: 1760.0353 - val_loss: 26.2828 - val_mse: 1907.4250\n",
      "Epoch 225/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2842 - mse: 1771.2122 - val_loss: 26.3796 - val_mse: 1870.2291\n",
      "Epoch 226/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3229 - mse: 1762.4299 - val_loss: 26.2742 - val_mse: 1921.1757\n",
      "Epoch 227/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.3715 - mse: 1780.2042 - val_loss: 26.2762 - val_mse: 1915.8763\n",
      "Epoch 228/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3297 - mse: 1774.7202 - val_loss: 26.2808 - val_mse: 1937.7361\n",
      "Epoch 229/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.2857 - mse: 1757.4882 - val_loss: 26.3294 - val_mse: 1884.6145\n",
      "Epoch 230/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3541 - mse: 1777.0067 - val_loss: 26.3216 - val_mse: 1887.4092\n",
      "Epoch 231/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.2272 - mse: 1747.6543 - val_loss: 26.3092 - val_mse: 1892.1702\n",
      "Epoch 232/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.3604 - mse: 1792.0562 - val_loss: 26.3093 - val_mse: 1959.0259\n",
      "Epoch 233/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2377 - mse: 1743.6627 - val_loss: 26.3120 - val_mse: 1960.4880\n",
      "Epoch 234/500\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 25.2819 - mse: 1773.0547 - val_loss: 26.2751 - val_mse: 1930.4528\n",
      "Epoch 235/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3100 - mse: 1745.7216 - val_loss: 26.2736 - val_mse: 1922.4973\n",
      "Epoch 236/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2710 - mse: 1778.4044 - val_loss: 26.2776 - val_mse: 1934.5342\n",
      "Epoch 237/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.5509 - mse: 1822.7158 - val_loss: 26.2763 - val_mse: 1932.9188\n",
      "Epoch 238/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2041 - mse: 1731.5112 - val_loss: 26.2886 - val_mse: 1902.1786\n",
      "Epoch 239/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2492 - mse: 1755.1377 - val_loss: 26.2920 - val_mse: 1947.9237\n",
      "Epoch 240/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1042 - mse: 1740.3998 - val_loss: 26.3364 - val_mse: 1882.0122\n",
      "Epoch 241/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.5408 - mse: 1788.3311 - val_loss: 26.2756 - val_mse: 1915.9392\n",
      "Epoch 242/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.5120 - mse: 1798.0538 - val_loss: 26.2878 - val_mse: 1902.6382\n",
      "Epoch 243/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.3564 - mse: 1812.3136 - val_loss: 26.3112 - val_mse: 1960.2152\n",
      "Epoch 244/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2609 - mse: 1757.7810 - val_loss: 26.2734 - val_mse: 1921.9045\n",
      "Epoch 245/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1840 - mse: 1732.3481 - val_loss: 26.2743 - val_mse: 1918.4858\n",
      "Epoch 246/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.1544 - mse: 1741.4176 - val_loss: 26.2902 - val_mse: 1946.6378\n",
      "Epoch 247/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.6666 - mse: 1823.0505 - val_loss: 26.3590 - val_mse: 1875.1295\n",
      "Epoch 248/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.9271 - mse: 1715.9432 - val_loss: 26.3821 - val_mse: 1986.8506\n",
      "Epoch 249/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4046 - mse: 1776.9000 - val_loss: 26.2929 - val_mse: 1948.8337\n",
      "Epoch 250/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2981 - mse: 1769.1826 - val_loss: 26.2881 - val_mse: 1945.0115\n",
      "Epoch 251/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.2846 - mse: 1784.5072 - val_loss: 26.3858 - val_mse: 1988.0011\n",
      "Epoch 252/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.7250 - mse: 1782.6088 - val_loss: 26.3943 - val_mse: 1990.5963\n",
      "Epoch 253/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2078 - mse: 1766.2128 - val_loss: 26.2757 - val_mse: 1914.9548\n",
      "Epoch 254/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3518 - mse: 1771.0596 - val_loss: 26.2758 - val_mse: 1914.7595\n",
      "Epoch 255/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.8006 - mse: 1728.2050 - val_loss: 26.2728 - val_mse: 1924.1128\n",
      "Epoch 256/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.5064 - mse: 1771.3844 - val_loss: 26.5719 - val_mse: 2035.9663\n",
      "Epoch 257/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1832 - mse: 1760.8086 - val_loss: 26.3144 - val_mse: 1961.9734\n",
      "Epoch 258/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.5387 - mse: 1802.1161 - val_loss: 26.3116 - val_mse: 1960.5927\n",
      "Epoch 259/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3333 - mse: 1777.8602 - val_loss: 26.2734 - val_mse: 1919.4529\n",
      "Epoch 260/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4113 - mse: 1777.6445 - val_loss: 26.3103 - val_mse: 1891.2660\n",
      "Epoch 261/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1974 - mse: 1752.3448 - val_loss: 26.2754 - val_mse: 1915.0580\n",
      "Epoch 262/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.9276 - mse: 1727.7734 - val_loss: 26.2870 - val_mse: 1944.4193\n",
      "Epoch 263/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.7221 - mse: 1834.2622 - val_loss: 26.3034 - val_mse: 1956.2473\n",
      "Epoch 264/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3049 - mse: 1763.6108 - val_loss: 26.2733 - val_mse: 1928.5912\n",
      "Epoch 265/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2296 - mse: 1734.9456 - val_loss: 26.4508 - val_mse: 2006.4656\n",
      "Epoch 266/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4558 - mse: 1803.7324 - val_loss: 26.2755 - val_mse: 1914.5903\n",
      "Epoch 267/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2612 - mse: 1760.6350 - val_loss: 26.2980 - val_mse: 1896.3793\n",
      "Epoch 268/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3738 - mse: 1777.4127 - val_loss: 26.2736 - val_mse: 1918.1196\n",
      "Epoch 269/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3318 - mse: 1776.9952 - val_loss: 26.3083 - val_mse: 1959.0277\n",
      "Epoch 270/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1211 - mse: 1732.3030 - val_loss: 26.3956 - val_mse: 1991.1082\n",
      "Epoch 271/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2263 - mse: 1725.0155 - val_loss: 26.3085 - val_mse: 1891.7612\n",
      "Epoch 272/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4522 - mse: 1823.1860 - val_loss: 26.3502 - val_mse: 1877.2388\n",
      "Epoch 273/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4304 - mse: 1770.8646 - val_loss: 26.2721 - val_mse: 1924.6772\n",
      "Epoch 274/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.0768 - mse: 1745.0814 - val_loss: 26.2793 - val_mse: 1909.0624\n",
      "Epoch 275/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2012 - mse: 1754.6150 - val_loss: 26.2976 - val_mse: 1896.3965\n",
      "Epoch 276/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1649 - mse: 1744.8083 - val_loss: 26.3139 - val_mse: 1889.5985\n",
      "Epoch 277/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.7617 - mse: 1875.6724 - val_loss: 26.3169 - val_mse: 1963.3940\n",
      "Epoch 278/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.9709 - mse: 1685.7538 - val_loss: 26.3026 - val_mse: 1955.9844\n",
      "Epoch 279/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.6928 - mse: 1785.2218 - val_loss: 26.2996 - val_mse: 1895.3574\n",
      "Epoch 280/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2018 - mse: 1793.1277 - val_loss: 26.2744 - val_mse: 1915.5345\n",
      "Epoch 281/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2851 - mse: 1766.4182 - val_loss: 26.3378 - val_mse: 1880.9543\n",
      "Epoch 282/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1180 - mse: 1739.8910 - val_loss: 26.3390 - val_mse: 1880.5402\n",
      "Epoch 283/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4192 - mse: 1752.6024 - val_loss: 26.2834 - val_mse: 1941.8922\n",
      "Epoch 284/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1728 - mse: 1774.9059 - val_loss: 26.3052 - val_mse: 1957.6332\n",
      "Epoch 285/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.7279 - mse: 1842.5382 - val_loss: 26.2743 - val_mse: 1915.3512\n",
      "Epoch 286/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1933 - mse: 1733.9951 - val_loss: 26.2988 - val_mse: 1953.8217\n",
      "Epoch 287/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.7973 - mse: 1699.1904 - val_loss: 26.4130 - val_mse: 1862.4706\n",
      "Epoch 288/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.7251 - mse: 1778.0881 - val_loss: 26.2803 - val_mse: 1939.1381\n",
      "Epoch 289/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.6859 - mse: 1904.5349 - val_loss: 26.2801 - val_mse: 1938.9663\n",
      "Epoch 290/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.6984 - mse: 1623.3123 - val_loss: 26.2913 - val_mse: 1899.3062\n",
      "Epoch 291/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.7429 - mse: 1825.2717 - val_loss: 26.3231 - val_mse: 1885.8519\n",
      "Epoch 292/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.0083 - mse: 1733.8870 - val_loss: 26.2752 - val_mse: 1933.8929\n",
      "Epoch 293/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.0168 - mse: 1726.0658 - val_loss: 26.3063 - val_mse: 1892.2465\n",
      "Epoch 294/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.7354 - mse: 1835.3136 - val_loss: 26.2988 - val_mse: 1895.4155\n",
      "Epoch 295/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1994 - mse: 1760.8635 - val_loss: 26.2809 - val_mse: 1939.9622\n",
      "Epoch 296/500\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 25.3745 - mse: 1784.6772 - val_loss: 26.3222 - val_mse: 1886.1002\n",
      "Epoch 297/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.3013 - mse: 1789.2212 - val_loss: 26.2740 - val_mse: 1915.0309\n",
      "Epoch 298/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.2641 - mse: 1757.5243 - val_loss: 26.3525 - val_mse: 1977.6552\n",
      "Epoch 299/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.0805 - mse: 1731.8195 - val_loss: 26.3413 - val_mse: 1973.7026\n",
      "Epoch 300/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.7201 - mse: 1824.0466 - val_loss: 26.3808 - val_mse: 1986.8574\n",
      "Epoch 301/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.9952 - mse: 1695.4565 - val_loss: 26.3260 - val_mse: 1884.6370\n",
      "Epoch 302/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4537 - mse: 1810.0419 - val_loss: 26.2714 - val_mse: 1927.0452\n",
      "Epoch 303/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.1185 - mse: 1754.9935 - val_loss: 26.2880 - val_mse: 1900.7852\n",
      "Epoch 304/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.5927 - mse: 1811.2776 - val_loss: 26.2997 - val_mse: 1954.7574\n",
      "Epoch 305/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.4163 - mse: 1772.1609 - val_loss: 26.3469 - val_mse: 1877.7434\n",
      "Epoch 306/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1481 - mse: 1723.7435 - val_loss: 26.2768 - val_mse: 1936.2593\n",
      "Epoch 307/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.0998 - mse: 1759.9701 - val_loss: 26.3068 - val_mse: 1958.8489\n",
      "Epoch 308/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4963 - mse: 1787.4138 - val_loss: 26.3019 - val_mse: 1956.1458\n",
      "Epoch 309/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.9163 - mse: 1724.4302 - val_loss: 26.3392 - val_mse: 1973.0376\n",
      "Epoch 310/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.5891 - mse: 1818.5803 - val_loss: 26.3117 - val_mse: 1889.8149\n",
      "Epoch 311/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.5993 - mse: 1809.5734 - val_loss: 26.3029 - val_mse: 1893.2828\n",
      "Epoch 312/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1326 - mse: 1722.7959 - val_loss: 26.2746 - val_mse: 1913.0172\n",
      "Epoch 313/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3044 - mse: 1775.1107 - val_loss: 26.2774 - val_mse: 1937.1013\n",
      "Epoch 314/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1478 - mse: 1752.9924 - val_loss: 26.2723 - val_mse: 1917.0248\n",
      "Epoch 315/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.5730 - mse: 1801.8990 - val_loss: 26.2922 - val_mse: 1898.2262\n",
      "Epoch 316/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1027 - mse: 1728.0060 - val_loss: 26.2705 - val_mse: 1924.7449\n",
      "Epoch 317/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4478 - mse: 1788.8732 - val_loss: 26.2942 - val_mse: 1897.1705\n",
      "Epoch 318/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.0528 - mse: 1751.2942 - val_loss: 26.3326 - val_mse: 1970.6575\n",
      "Epoch 319/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3022 - mse: 1756.1345 - val_loss: 26.3140 - val_mse: 1962.6514\n",
      "Epoch 320/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.6153 - mse: 1840.1211 - val_loss: 26.2757 - val_mse: 1935.5651\n",
      "Epoch 321/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2484 - mse: 1729.5084 - val_loss: 26.2855 - val_mse: 1944.8212\n",
      "Epoch 322/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2619 - mse: 1796.2831 - val_loss: 26.2703 - val_mse: 1925.2812\n",
      "Epoch 323/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4170 - mse: 1778.0212 - val_loss: 26.2742 - val_mse: 1934.0762\n",
      "Epoch 324/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1966 - mse: 1749.8043 - val_loss: 26.2927 - val_mse: 1897.7388\n",
      "Epoch 325/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2571 - mse: 1757.5302 - val_loss: 26.3127 - val_mse: 1962.1110\n",
      "Epoch 326/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1803 - mse: 1755.8804 - val_loss: 26.2701 - val_mse: 1924.8859\n",
      "Epoch 327/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4405 - mse: 1775.5442 - val_loss: 26.3160 - val_mse: 1963.6682\n",
      "Epoch 328/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3385 - mse: 1782.5452 - val_loss: 26.3071 - val_mse: 1891.2903\n",
      "Epoch 329/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3734 - mse: 1790.0804 - val_loss: 26.3900 - val_mse: 1989.9368\n",
      "Epoch 330/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3489 - mse: 1779.6423 - val_loss: 26.4477 - val_mse: 1855.8037\n",
      "Epoch 331/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3609 - mse: 1764.3571 - val_loss: 26.2725 - val_mse: 1915.2649\n",
      "Epoch 332/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.0801 - mse: 1735.5287 - val_loss: 26.2711 - val_mse: 1929.2284\n",
      "Epoch 333/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4242 - mse: 1787.1552 - val_loss: 26.2782 - val_mse: 1938.5353\n",
      "Epoch 334/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.2326 - mse: 1753.2957 - val_loss: 26.2964 - val_mse: 1953.2754\n",
      "Epoch 335/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3692 - mse: 1775.1002 - val_loss: 26.3879 - val_mse: 1989.3530\n",
      "Epoch 336/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.2361 - mse: 1763.2473 - val_loss: 26.3065 - val_mse: 1959.1893\n",
      "Epoch 337/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.2707 - mse: 1761.5468 - val_loss: 26.2719 - val_mse: 1931.4449\n",
      "Epoch 338/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.2490 - mse: 1760.1956 - val_loss: 26.3091 - val_mse: 1890.2902\n",
      "Epoch 339/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3813 - mse: 1773.1154 - val_loss: 26.2834 - val_mse: 1902.8746\n",
      "Epoch 340/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3168 - mse: 1781.4481 - val_loss: 26.2706 - val_mse: 1918.2986\n",
      "Epoch 341/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2591 - mse: 1759.2987 - val_loss: 26.3065 - val_mse: 1959.2987\n",
      "Epoch 342/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.6235 - mse: 1801.0586 - val_loss: 26.2719 - val_mse: 1915.3844\n",
      "Epoch 343/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.0606 - mse: 1750.9696 - val_loss: 26.2810 - val_mse: 1941.5266\n",
      "Epoch 344/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3594 - mse: 1759.3124 - val_loss: 26.2731 - val_mse: 1933.5365\n",
      "Epoch 345/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2980 - mse: 1751.9225 - val_loss: 26.2831 - val_mse: 1902.8966\n",
      "Epoch 346/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3927 - mse: 1794.0072 - val_loss: 26.2842 - val_mse: 1944.4799\n",
      "Epoch 347/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2245 - mse: 1775.0021 - val_loss: 26.2696 - val_mse: 1921.2927\n",
      "Epoch 348/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.2519 - mse: 1737.9601 - val_loss: 26.2761 - val_mse: 1937.0101\n",
      "Epoch 349/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4426 - mse: 1804.6763 - val_loss: 26.2700 - val_mse: 1928.0575\n",
      "Epoch 350/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2450 - mse: 1737.2375 - val_loss: 26.2718 - val_mse: 1915.1289\n",
      "Epoch 351/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.3735 - mse: 1799.7288 - val_loss: 26.3751 - val_mse: 1985.5472\n",
      "Epoch 352/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2132 - mse: 1738.4846 - val_loss: 26.2873 - val_mse: 1899.9763\n",
      "Epoch 353/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3848 - mse: 1787.8253 - val_loss: 26.2726 - val_mse: 1913.5128\n",
      "Epoch 354/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2787 - mse: 1758.1416 - val_loss: 26.2770 - val_mse: 1908.0056\n",
      "Epoch 355/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2391 - mse: 1762.8328 - val_loss: 26.2718 - val_mse: 1914.6217\n",
      "Epoch 356/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4937 - mse: 1807.6287 - val_loss: 26.2868 - val_mse: 1900.1647\n",
      "Epoch 357/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2943 - mse: 1746.0074 - val_loss: 26.2690 - val_mse: 1924.6543\n",
      "Epoch 358/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2314 - mse: 1773.9535 - val_loss: 26.2755 - val_mse: 1909.4392\n",
      "Epoch 359/500\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 25.2061 - mse: 1742.3557 - val_loss: 26.3114 - val_mse: 1961.9938\n",
      "Epoch 360/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.5496 - mse: 1846.5247 - val_loss: 26.2706 - val_mse: 1930.4329\n",
      "Epoch 361/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.0162 - mse: 1714.9652 - val_loss: 26.3094 - val_mse: 1961.0480\n",
      "Epoch 362/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3885 - mse: 1769.1638 - val_loss: 26.3460 - val_mse: 1976.0499\n",
      "Epoch 363/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.2296 - mse: 1735.5549 - val_loss: 26.2784 - val_mse: 1939.8026\n",
      "Epoch 364/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.3321 - mse: 1796.7592 - val_loss: 26.2698 - val_mse: 1917.7435\n",
      "Epoch 365/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.5178 - mse: 1782.9072 - val_loss: 26.3352 - val_mse: 1880.4332\n",
      "Epoch 366/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3716 - mse: 1772.1632 - val_loss: 26.2695 - val_mse: 1918.4061\n",
      "Epoch 367/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.0716 - mse: 1763.3574 - val_loss: 26.2996 - val_mse: 1893.5592\n",
      "Epoch 368/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3305 - mse: 1742.7008 - val_loss: 26.2915 - val_mse: 1897.3036\n",
      "Epoch 369/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1810 - mse: 1744.1780 - val_loss: 26.2808 - val_mse: 1903.8280\n",
      "Epoch 370/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2205 - mse: 1771.5011 - val_loss: 26.3301 - val_mse: 1882.0157\n",
      "Epoch 371/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3852 - mse: 1791.0011 - val_loss: 26.2888 - val_mse: 1948.8225\n",
      "Epoch 372/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3703 - mse: 1747.2625 - val_loss: 26.2746 - val_mse: 1936.4478\n",
      "Epoch 373/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3765 - mse: 1799.0150 - val_loss: 26.2692 - val_mse: 1928.3959\n",
      "Epoch 374/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.5372 - mse: 1785.7821 - val_loss: 26.2695 - val_mse: 1929.2880\n",
      "Epoch 375/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.9607 - mse: 1757.7399 - val_loss: 26.2682 - val_mse: 1924.4948\n",
      "Epoch 376/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.5215 - mse: 1783.1735 - val_loss: 26.2761 - val_mse: 1938.0094\n",
      "Epoch 377/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2138 - mse: 1739.5692 - val_loss: 26.3349 - val_mse: 1972.1976\n",
      "Epoch 378/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.8978 - mse: 1716.3221 - val_loss: 26.3767 - val_mse: 1986.2742\n",
      "Epoch 379/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.5916 - mse: 1818.6960 - val_loss: 26.2773 - val_mse: 1906.5773\n",
      "Epoch 380/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3813 - mse: 1768.2596 - val_loss: 26.3233 - val_mse: 1884.2220\n",
      "Epoch 381/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.6781 - mse: 1841.4514 - val_loss: 26.4093 - val_mse: 1862.1479\n",
      "Epoch 382/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.6901 - mse: 1664.7296 - val_loss: 26.3289 - val_mse: 1969.9489\n",
      "Epoch 383/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.8968 - mse: 1878.8938 - val_loss: 26.2748 - val_mse: 1909.0332\n",
      "Epoch 384/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.8533 - mse: 1692.7792 - val_loss: 26.3013 - val_mse: 1957.2253\n",
      "Epoch 385/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3358 - mse: 1754.3141 - val_loss: 26.5139 - val_mse: 2023.5066\n",
      "Epoch 386/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4313 - mse: 1801.3489 - val_loss: 26.2702 - val_mse: 1931.7172\n",
      "Epoch 387/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2902 - mse: 1744.7758 - val_loss: 26.2737 - val_mse: 1910.0647\n",
      "Epoch 388/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.5747 - mse: 1843.7662 - val_loss: 26.2950 - val_mse: 1953.5034\n",
      "Epoch 389/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2116 - mse: 1722.2388 - val_loss: 26.2838 - val_mse: 1945.3553\n",
      "Epoch 390/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2720 - mse: 1799.8181 - val_loss: 26.3002 - val_mse: 1892.8044\n",
      "Epoch 391/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1806 - mse: 1725.1161 - val_loss: 26.3842 - val_mse: 1867.1807\n",
      "Epoch 392/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.5787 - mse: 1814.8306 - val_loss: 26.3341 - val_mse: 1972.0974\n",
      "Epoch 393/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.9828 - mse: 1717.6219 - val_loss: 26.2851 - val_mse: 1946.5394\n",
      "Epoch 394/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3844 - mse: 1762.6328 - val_loss: 26.2872 - val_mse: 1948.2070\n",
      "Epoch 395/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.5063 - mse: 1796.9468 - val_loss: 26.2690 - val_mse: 1929.9482\n",
      "Epoch 396/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1874 - mse: 1777.9446 - val_loss: 26.2863 - val_mse: 1899.3575\n",
      "Epoch 397/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2413 - mse: 1737.9342 - val_loss: 26.3143 - val_mse: 1887.1852\n",
      "Epoch 398/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4939 - mse: 1768.2648 - val_loss: 26.2790 - val_mse: 1941.4888\n",
      "Epoch 399/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.9936 - mse: 1734.8273 - val_loss: 26.2865 - val_mse: 1947.7716\n",
      "Epoch 400/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4572 - mse: 1836.3741 - val_loss: 26.4232 - val_mse: 2000.0028\n",
      "Epoch 401/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.5440 - mse: 1805.1487 - val_loss: 26.3805 - val_mse: 1987.6664\n",
      "Epoch 402/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.9935 - mse: 1645.8761 - val_loss: 26.3030 - val_mse: 1958.4457\n",
      "Epoch 403/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.5445 - mse: 1865.0345 - val_loss: 26.2682 - val_mse: 1917.7424\n",
      "Epoch 404/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.7456 - mse: 1671.2194 - val_loss: 26.2728 - val_mse: 1935.7700\n",
      "Epoch 405/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.7946 - mse: 1864.0474 - val_loss: 26.3237 - val_mse: 1968.1251\n",
      "Epoch 406/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.0337 - mse: 1689.8420 - val_loss: 26.4937 - val_mse: 2018.6444\n",
      "Epoch 407/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.6122 - mse: 1834.0927 - val_loss: 26.3101 - val_mse: 1962.0892\n",
      "Epoch 408/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2859 - mse: 1753.9562 - val_loss: 26.3096 - val_mse: 1961.8556\n",
      "Epoch 409/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.5047 - mse: 1806.8949 - val_loss: 26.2939 - val_mse: 1895.1757\n",
      "Epoch 410/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.9254 - mse: 1699.6797 - val_loss: 26.2761 - val_mse: 1939.2006\n",
      "Epoch 411/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3799 - mse: 1794.7468 - val_loss: 26.2887 - val_mse: 1897.6987\n",
      "Epoch 412/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2392 - mse: 1764.3749 - val_loss: 26.2686 - val_mse: 1915.9205\n",
      "Epoch 413/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.5231 - mse: 1777.7782 - val_loss: 26.3345 - val_mse: 1972.4517\n",
      "Epoch 414/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.3932 - mse: 1795.0769 - val_loss: 26.3146 - val_mse: 1964.2634\n",
      "Epoch 415/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.2328 - mse: 1734.4681 - val_loss: 26.2782 - val_mse: 1941.3475\n",
      "Epoch 416/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2569 - mse: 1768.5197 - val_loss: 26.2685 - val_mse: 1915.7755\n",
      "Epoch 417/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1792 - mse: 1769.9465 - val_loss: 26.2695 - val_mse: 1914.0153\n",
      "Epoch 418/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2645 - mse: 1733.4769 - val_loss: 26.2824 - val_mse: 1945.0601\n",
      "Epoch 419/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.5354 - mse: 1804.0123 - val_loss: 26.4147 - val_mse: 1860.6804\n",
      "Epoch 420/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.1333 - mse: 1771.4602 - val_loss: 26.2733 - val_mse: 1908.8397\n",
      "Epoch 421/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.5369 - mse: 1788.3633 - val_loss: 26.3271 - val_mse: 1882.1531\n",
      "Epoch 422/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.0658 - mse: 1721.8308 - val_loss: 26.2908 - val_mse: 1951.5167\n",
      "Epoch 423/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.6078 - mse: 1813.5594 - val_loss: 26.3280 - val_mse: 1970.0898\n",
      "Epoch 424/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.9447 - mse: 1706.0629 - val_loss: 26.3607 - val_mse: 1981.6364\n",
      "Epoch 425/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4938 - mse: 1814.7317 - val_loss: 26.2910 - val_mse: 1951.6995\n",
      "Epoch 426/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2177 - mse: 1745.3367 - val_loss: 26.2700 - val_mse: 1933.5264\n",
      "Epoch 427/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4695 - mse: 1797.4176 - val_loss: 26.3219 - val_mse: 1967.6919\n",
      "Epoch 428/500\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 25.2145 - mse: 1752.7134 - val_loss: 26.3231 - val_mse: 1883.4010\n",
      "Epoch 429/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3996 - mse: 1790.0184 - val_loss: 26.3449 - val_mse: 1876.4138\n",
      "Epoch 430/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.0878 - mse: 1717.3945 - val_loss: 26.3167 - val_mse: 1885.6744\n",
      "Epoch 431/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2992 - mse: 1752.4481 - val_loss: 26.3249 - val_mse: 1882.7433\n",
      "Epoch 432/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.6419 - mse: 1853.1127 - val_loss: 26.2732 - val_mse: 1908.3185\n",
      "Epoch 433/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1773 - mse: 1740.8524 - val_loss: 26.2660 - val_mse: 1921.1117\n",
      "Epoch 434/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3053 - mse: 1766.1504 - val_loss: 26.4184 - val_mse: 1998.9431\n",
      "Epoch 435/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1619 - mse: 1736.6721 - val_loss: 26.2835 - val_mse: 1899.8878\n",
      "Epoch 436/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.0216 - mse: 1715.1541 - val_loss: 26.2785 - val_mse: 1942.3301\n",
      "Epoch 437/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4934 - mse: 1817.8795 - val_loss: 26.2822 - val_mse: 1945.4253\n",
      "Epoch 438/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4062 - mse: 1785.4181 - val_loss: 26.2663 - val_mse: 1927.2946\n",
      "Epoch 439/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3902 - mse: 1766.4027 - val_loss: 26.3102 - val_mse: 1962.6172\n",
      "Epoch 440/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4629 - mse: 1800.5432 - val_loss: 26.3548 - val_mse: 1979.8350\n",
      "Epoch 441/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1269 - mse: 1739.9458 - val_loss: 26.3745 - val_mse: 1986.1733\n",
      "Epoch 442/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4196 - mse: 1788.1370 - val_loss: 26.2776 - val_mse: 1941.6486\n",
      "Epoch 443/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2791 - mse: 1759.2988 - val_loss: 26.3073 - val_mse: 1889.0010\n",
      "Epoch 444/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2313 - mse: 1758.2441 - val_loss: 26.2767 - val_mse: 1904.3662\n",
      "Epoch 445/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3760 - mse: 1782.1602 - val_loss: 26.2859 - val_mse: 1948.5917\n",
      "Epoch 446/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1958 - mse: 1755.6149 - val_loss: 26.3920 - val_mse: 1991.5410\n",
      "Epoch 447/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3713 - mse: 1769.2595 - val_loss: 26.2751 - val_mse: 1939.5924\n",
      "Epoch 448/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3218 - mse: 1776.7729 - val_loss: 26.3192 - val_mse: 1966.8225\n",
      "Epoch 449/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2590 - mse: 1761.6970 - val_loss: 26.2669 - val_mse: 1930.0518\n",
      "Epoch 450/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.3653 - mse: 1768.3888 - val_loss: 26.4752 - val_mse: 2014.1981\n",
      "Epoch 451/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3049 - mse: 1778.2002 - val_loss: 26.2981 - val_mse: 1956.6470\n",
      "Epoch 452/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.2896 - mse: 1766.4436 - val_loss: 26.2760 - val_mse: 1940.5629\n",
      "Epoch 453/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1903 - mse: 1735.7498 - val_loss: 26.4789 - val_mse: 2015.1747\n",
      "Epoch 454/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.3437 - mse: 1776.8303 - val_loss: 26.3334 - val_mse: 1972.5170\n",
      "Epoch 455/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.4432 - mse: 1799.2802 - val_loss: 26.2750 - val_mse: 1939.7522\n",
      "Epoch 456/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1860 - mse: 1744.6475 - val_loss: 26.3176 - val_mse: 1966.2172\n",
      "Epoch 457/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.5285 - mse: 1806.4220 - val_loss: 26.2711 - val_mse: 1909.5743\n",
      "Epoch 458/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1651 - mse: 1748.0802 - val_loss: 26.2799 - val_mse: 1944.2434\n",
      "Epoch 459/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1150 - mse: 1745.6632 - val_loss: 26.2965 - val_mse: 1955.9290\n",
      "Epoch 460/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4743 - mse: 1811.3982 - val_loss: 26.2655 - val_mse: 1927.7559\n",
      "Epoch 461/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3796 - mse: 1758.3977 - val_loss: 26.2684 - val_mse: 1912.7969\n",
      "Epoch 462/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3126 - mse: 1759.6802 - val_loss: 26.3238 - val_mse: 1882.5721\n",
      "Epoch 463/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1532 - mse: 1756.8339 - val_loss: 26.2645 - val_mse: 1922.4508\n",
      "Epoch 464/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1432 - mse: 1753.4142 - val_loss: 26.3708 - val_mse: 1985.2570\n",
      "Epoch 465/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3429 - mse: 1755.9850 - val_loss: 26.3278 - val_mse: 1881.1423\n",
      "Epoch 466/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.6103 - mse: 1817.3851 - val_loss: 26.2769 - val_mse: 1941.8402\n",
      "Epoch 467/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1488 - mse: 1744.3224 - val_loss: 26.3816 - val_mse: 1988.6154\n",
      "Epoch 468/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.3613 - mse: 1763.8606 - val_loss: 26.2782 - val_mse: 1943.0940\n",
      "Epoch 469/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4631 - mse: 1803.4371 - val_loss: 26.2646 - val_mse: 1920.3601\n",
      "Epoch 470/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1125 - mse: 1743.1589 - val_loss: 26.2720 - val_mse: 1907.7207\n",
      "Epoch 471/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4444 - mse: 1787.4890 - val_loss: 26.2675 - val_mse: 1932.6658\n",
      "Epoch 472/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.3084 - mse: 1768.6664 - val_loss: 26.2912 - val_mse: 1895.0195\n",
      "Epoch 473/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2592 - mse: 1761.0349 - val_loss: 26.2766 - val_mse: 1903.3429\n",
      "Epoch 474/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.0888 - mse: 1761.4294 - val_loss: 26.2860 - val_mse: 1949.4465\n",
      "Epoch 475/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4839 - mse: 1760.1262 - val_loss: 26.2651 - val_mse: 1928.4707\n",
      "Epoch 476/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.4927 - mse: 1810.4373 - val_loss: 26.2668 - val_mse: 1914.0056\n",
      "Epoch 477/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.2544 - mse: 1740.7406 - val_loss: 26.3332 - val_mse: 1879.1445\n",
      "Epoch 478/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3293 - mse: 1811.9209 - val_loss: 26.2925 - val_mse: 1894.2621\n",
      "Epoch 479/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 24.8745 - mse: 1679.1829 - val_loss: 26.3353 - val_mse: 1973.5068\n",
      "Epoch 480/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.1153 - mse: 1752.4688 - val_loss: 26.2719 - val_mse: 1907.3586\n",
      "Epoch 481/500\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 25.7026 - mse: 1819.3113 - val_loss: 26.2744 - val_mse: 1904.8966\n",
      "Epoch 482/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.2375 - mse: 1772.8590 - val_loss: 26.2756 - val_mse: 1941.2233\n",
      "Epoch 483/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3974 - mse: 1793.8535 - val_loss: 26.2827 - val_mse: 1947.2723\n",
      "Epoch 484/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3241 - mse: 1766.3953 - val_loss: 26.2647 - val_mse: 1928.3893\n",
      "Epoch 485/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1846 - mse: 1735.5764 - val_loss: 26.3236 - val_mse: 1882.1750\n",
      "Epoch 486/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3684 - mse: 1764.0040 - val_loss: 26.2797 - val_mse: 1944.9454\n",
      "Epoch 487/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.3088 - mse: 1792.2700 - val_loss: 26.2657 - val_mse: 1914.9218\n",
      "Epoch 488/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.7464 - mse: 1837.1471 - val_loss: 26.2642 - val_mse: 1918.3992\n",
      "Epoch 489/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 24.6323 - mse: 1654.6912 - val_loss: 26.3137 - val_mse: 1965.0021\n",
      "Epoch 490/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.5354 - mse: 1772.3063 - val_loss: 26.2679 - val_mse: 1934.0433\n",
      "Epoch 491/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1113 - mse: 1779.6544 - val_loss: 26.2762 - val_mse: 1942.1447\n",
      "Epoch 492/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4083 - mse: 1776.1982 - val_loss: 26.2636 - val_mse: 1919.8898\n",
      "Epoch 493/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4485 - mse: 1769.4653 - val_loss: 26.2937 - val_mse: 1893.3229\n",
      "Epoch 494/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1352 - mse: 1741.0653 - val_loss: 26.2814 - val_mse: 1946.6000\n",
      "Epoch 495/500\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.4985 - mse: 1812.9745 - val_loss: 26.2631 - val_mse: 1923.4138\n",
      "Epoch 496/500\n",
      " 38/113 [=========>....................] - ETA: 0s - loss: 24.5730 - mse: 1657.3313WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 56500 batches). You may need to use the repeat() function when building your dataset.\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 25.1100 - mse: 1724.3895 - val_loss: 26.2634 - val_mse: 1926.0671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 20:34:37.400418: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_atmos_press0\n",
      "[42.40, 42.64]\n"
     ]
    }
   ],
   "source": [
    "for dframe in df_names:\n",
    "    df = locals()[dframe]\n",
    "    name = df.columns[0]\n",
    "\n",
    "    model_compile_and_fit(df, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LinearRegression()_new_temp0</th>\n",
       "      <td>38.044771</td>\n",
       "      <td>38.503181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearRegression()_new_precip0</th>\n",
       "      <td>40.441800</td>\n",
       "      <td>41.090318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearRegression()_new_rel_humidity0</th>\n",
       "      <td>38.566880</td>\n",
       "      <td>39.104745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearRegression()_new_wind_dir0</th>\n",
       "      <td>39.627375</td>\n",
       "      <td>40.188481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearRegression()_new_wind_spd0</th>\n",
       "      <td>38.951910</td>\n",
       "      <td>39.394749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearRegression()_new_atmos_press0</th>\n",
       "      <td>38.578760</td>\n",
       "      <td>38.918730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;keras.engine.sequential.Sequential object at 0x2c410d760&gt;_new_temp0</th>\n",
       "      <td>40.255653</td>\n",
       "      <td>40.223103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;keras.engine.sequential.Sequential object at 0x2c7427220&gt;_new_precip0</th>\n",
       "      <td>40.850029</td>\n",
       "      <td>42.932675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;keras.engine.sequential.Sequential object at 0x2c5362d00&gt;_new_rel_humidity0</th>\n",
       "      <td>40.731582</td>\n",
       "      <td>40.849574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;keras.engine.sequential.Sequential object at 0x2c5474820&gt;_new_wind_dir0</th>\n",
       "      <td>39.727026</td>\n",
       "      <td>43.476105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;keras.engine.sequential.Sequential object at 0x2c7114f40&gt;_new_wind_spd0</th>\n",
       "      <td>36.718471</td>\n",
       "      <td>38.158110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;keras.engine.sequential.Sequential object at 0x2c71a0e20&gt;_new_atmos_press0</th>\n",
       "      <td>42.399458</td>\n",
       "      <td>42.641924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            0          1\n",
       "LinearRegression()_new_temp0                        38.044771  38.503181\n",
       "LinearRegression()_new_precip0                      40.441800  41.090318\n",
       "LinearRegression()_new_rel_humidity0                38.566880  39.104745\n",
       "LinearRegression()_new_wind_dir0                    39.627375  40.188481\n",
       "LinearRegression()_new_wind_spd0                    38.951910  39.394749\n",
       "LinearRegression()_new_atmos_press0                 38.578760  38.918730\n",
       "<keras.engine.sequential.Sequential object at 0...  40.255653  40.223103\n",
       "<keras.engine.sequential.Sequential object at 0...  40.850029  42.932675\n",
       "<keras.engine.sequential.Sequential object at 0...  40.731582  40.849574\n",
       "<keras.engine.sequential.Sequential object at 0...  39.727026  43.476105\n",
       "<keras.engine.sequential.Sequential object at 0...  36.718471  38.158110\n",
       "<keras.engine.sequential.Sequential object at 0...  42.399458  42.641924"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(test_results).T"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d8f548114482b487876add26679caae41a2a9f4541ddf007921bd7bf3c60f478"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
